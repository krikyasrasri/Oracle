{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fswiss\fcharset0 Helvetica-Bold;\f2\fnil\fcharset0 HelveticaNeue-Light;
\f3\fnil\fcharset0 HelveticaNeue;\f4\fswiss\fcharset0 Arial-BoldMT;\f5\fswiss\fcharset0 ArialMT;
\f6\fnil\fcharset0 LucidaGrande;\f7\fnil\fcharset0 HelveticaNeue-Bold;\f8\fnil\fcharset0 Monaco;
\f9\fnil\fcharset0 HelveticaNeue-Medium;\f10\fswiss\fcharset0 Helvetica-Oblique;\f11\fnil\fcharset0 Verdana;
\f12\fmodern\fcharset0 Courier;\f13\fmodern\fcharset0 Courier-Oblique;\f14\fmodern\fcharset0 Courier-Bold;
\f15\fnil\fcharset0 Menlo-Regular;\f16\fnil\fcharset0 Chalkboard-Bold;\f17\fnil\fcharset0 Chalkboard;
}
{\colortbl;\red255\green255\blue255;\red128\green0\blue0;\red0\green60\blue82;\red0\green0\blue255;
\red3\green53\blue197;\red252\green252\blue252;\red25\green25\blue25;\red255\green255\blue255;\red134\green19\blue62;
\red153\green102\blue51;\red31\green36\blue45;\red52\green52\blue52;\red239\green239\blue239;\red233\green233\blue233;
\red201\green113\blue6;\red51\green51\blue51;\red241\green244\blue246;\red191\green191\blue191;\red247\green247\blue247;
\red37\green37\blue37;\red18\green19\blue24;\red127\green127\blue127;\red38\green38\blue38;\red242\green242\blue242;
\red0\green0\blue0;\red67\green73\blue81;\red153\green102\blue51;\red247\green247\blue247;\red178\green113\blue6;
\red16\green60\blue192;\red128\green0\blue128;\red255\green255\blue255;\red24\green25\blue27;\red101\green125\blue138;
\red255\green255\blue255;\red63\green105\blue30;\red127\green0\blue128;\red0\green0\blue255;\red41\green0\blue130;
\red32\green0\blue99;\red122\green117\blue3;\red7\green60\blue82;\red110\green5\blue2;\red0\green0\blue0;
\red255\green255\blue255;\red246\green246\blue246;\red250\green250\blue250;\red27\green35\blue47;\red26\green26\blue26;
\red29\green48\blue13;\red51\green102\blue255;}
{\*\expandedcolortbl;;\csgenericrgb\c50196\c0\c0;\csgenericrgb\c0\c23529\c32157;\csgenericrgb\c0\c0\c100000;
\cssrgb\c0\c30588\c81569;\cssrgb\c99216\c99216\c99216;\cssrgb\c12941\c12941\c12941;\cssrgb\c100000\c100000\c100000;\cssrgb\c60282\c14051\c30968;
\cssrgb\c66800\c47512\c25860;\cssrgb\c16078\c18824\c23137;\cssrgb\c26667\c26667\c26667;\cssrgb\c94902\c94902\c94902;\cssrgb\c92941\c92941\c92941;
\cssrgb\c83336\c51866\c0;\csgenericrgb\c20000\c20000\c20000;\csgenericrgb\c94510\c95686\c96471;\csgray\c79525;\csgenericrgb\c96863\c96863\c96863;
\cssrgb\c19216\c19216\c19216;\cssrgb\c8627\c9804\c12157;\cssrgb\c57046\c57047\c57046;\cssrgb\c20000\c20000\c20000;\cssrgb\c96078\c96078\c96078;
\cssrgb\c0\c0\c0;\cssrgb\c32941\c35686\c39216;\csgenericrgb\c60000\c40000\c20000;\cssrgb\c97647\c97647\c97647;\cssrgb\c75686\c51765\c392;
\cssrgb\c6667\c33333\c80000;\csgenericrgb\c50000\c0\c50000;\csgenericrgb\c100000\c100000\c100000;\cssrgb\c12549\c12941\c14118;\cssrgb\c47059\c56471\c61176;
\cssrgb\c100000\c100000\c100000\c95294;\cssrgb\c30831\c47797\c15540;\cssrgb\c57919\c12801\c57269;\cssrgb\c1680\c19835\c100000;\cssrgb\c21681\c10251\c58230;
\cssrgb\c17067\c7357\c46453;\cssrgb\c55206\c52547\c0;\cssrgb\c0\c30219\c39596;\cssrgb\c51239\c6511\c0;\csgray\c0;
\csgray\c100000;\cssrgb\c97255\c97255\c97255;\cssrgb\c98431\c98431\c98431;\cssrgb\c13725\c18431\c24314;\cssrgb\c13333\c13333\c13333;
\csgenericrgb\c11373\c18824\c5098;\csgenericrgb\c20000\c40000\c100000;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid2}
{\list\listtemplateid3\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid201\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid3}
{\list\listtemplateid4\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid301\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid4}
{\list\listtemplateid5\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid5}
{\list\listtemplateid6\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid6}
{\list\listtemplateid7\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid601\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid7}
{\list\listtemplateid8\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{check\}}{\leveltext\leveltemplateid701\'01\uc0\u10003 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid8}
{\list\listtemplateid9\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid801\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid9}
{\list\listtemplateid10\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid10}
{\list\listtemplateid11\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid11}
{\list\listtemplateid12\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid12}
{\list\listtemplateid13\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{check\}}{\leveltext\leveltemplateid1201\'01\uc0\u10003 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid13}
{\list\listtemplateid14\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{hyphen\}}{\leveltext\leveltemplateid1301\'01\uc0\u8259 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid14}
{\list\listtemplateid15\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1401\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid15}
{\list\listtemplateid16\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1501\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid16}
{\list\listtemplateid17\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid17}
{\list\listtemplateid18\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid18}
{\list\listtemplateid19\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1801\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid19}
{\list\listtemplateid20\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1901\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid20}
{\list\listtemplateid21\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid21}
{\list\listtemplateid22\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid22}
{\list\listtemplateid23\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid2201\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid23}
{\list\listtemplateid24\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{check\}}{\leveltext\leveltemplateid2301\'01\uc0\u10003 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid24}
{\list\listtemplateid25\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{check\}}{\leveltext\leveltemplateid2401\'01\uc0\u10003 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid25}
{\list\listtemplateid26\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2501\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid26}
{\list\listtemplateid27\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2601\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid27}
{\list\listtemplateid28\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid2701\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid28}
{\list\listtemplateid29\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{square\}}{\leveltext\leveltemplateid2801\'01\uc0\u9642 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid29}
{\list\listtemplateid30\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{none\}.}{\leveltext\leveltemplateid2901\'01.;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid30}
{\list\listtemplateid31\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid3001\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid31}
{\list\listtemplateid32\listhybrid{\listlevel\levelnfc0\levelnfcn0\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{decimal\}.}{\leveltext\leveltemplateid3101\'02\'00.;}{\levelnumbers\'01;}\fi-360\li720\lin720 }{\listname ;}\listid32}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}{\listoverride\listid3\listoverridecount0\ls3}{\listoverride\listid4\listoverridecount0\ls4}{\listoverride\listid5\listoverridecount0\ls5}{\listoverride\listid6\listoverridecount0\ls6}{\listoverride\listid7\listoverridecount0\ls7}{\listoverride\listid8\listoverridecount0\ls8}{\listoverride\listid9\listoverridecount0\ls9}{\listoverride\listid10\listoverridecount0\ls10}{\listoverride\listid11\listoverridecount0\ls11}{\listoverride\listid12\listoverridecount0\ls12}{\listoverride\listid13\listoverridecount0\ls13}{\listoverride\listid14\listoverridecount0\ls14}{\listoverride\listid15\listoverridecount0\ls15}{\listoverride\listid16\listoverridecount0\ls16}{\listoverride\listid17\listoverridecount0\ls17}{\listoverride\listid18\listoverridecount0\ls18}{\listoverride\listid19\listoverridecount0\ls19}{\listoverride\listid20\listoverridecount0\ls20}{\listoverride\listid21\listoverridecount0\ls21}{\listoverride\listid22\listoverridecount0\ls22}{\listoverride\listid23\listoverridecount0\ls23}{\listoverride\listid24\listoverridecount0\ls24}{\listoverride\listid25\listoverridecount0\ls25}{\listoverride\listid26\listoverridecount0\ls26}{\listoverride\listid27\listoverridecount0\ls27}{\listoverride\listid28\listoverridecount0\ls28}{\listoverride\listid29\listoverridecount0\ls29}{\listoverride\listid30\listoverridecount0\ls30}{\listoverride\listid31\listoverridecount0\ls31}{\listoverride\listid32\listoverridecount0\ls32}}
\vieww34820\viewh19380\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f1\b\fs48 \cf2 Cloud						\cf3 ({\field{\*\fldinst{HYPERLINK "http://satya-hadoop.blogspot.com"}}{\fldrslt \cf4 \ul \ulc4 http://satya-hadoop.blogspot.com}})
\fs32 \cf2 \
\pard\pardeftab720\partightenfactor0

\fs36 \cf2 ==================================================================
\fs32 \
\pard\pardeftab720\partightenfactor0

\f0\b0\fs34 \cf0 \
Advantages of Cloud computing\
1. Trade capital expense for variable expense\
2. Benefit from massive economies of scale\
3. Stop guessing about capacity\
4. Increase speed of agility\
5. Stop spending money running/maintaining data centers\
6. Go global in minutes\
\
Benefits of Cloud computing - 
\fs32 Cost, Speed, Scalability, Productivity, Reliability, Performance\
\
Types of Cloud services:\
IaaS    	- Infrastructure as as service	- Servers, VMs, storage, network, OS\
PaaS	- Platform as as service		- IaaS, middleware, Dev Tools, BI\
SaaS	- Software as as service		- IaaS, PaaS, other softwares, IoT, ERP\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs48 \cf5 \cb6 \expnd0\expndtw0\kerning0
AWS 
\f2\b0 \cf7 \cb8 Amazon Web Services
\fs24 \
\pard\pardeftab720\partightenfactor0

\f0\fs34 \cf5 \cb6 AWS components:\
	Compute\
	Storage\
	Database\
	Network\
\
AWS Regions are geographic locations that contain multiple Availability Zones.\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 \cb1 \kerning1\expnd0\expndtw0 AWS regions (total 19 regions):
\f0\b0 \
us-east-1  		-> Northern Virginia\
us-east-2  		-> Ohio\
us-west-1  		-> Northern California\
us-west-2  		-> Oregon\
sa-east-1			-> Sao Paulo\
ca-central-1		-> Canada\
ap-south-1 		-> Mumbai\
ap-southeast-1 	-> Singapore\
ap-southeast-2 	-> Sydney\
ap-northeast-1 	-> Tokyo\
ap-northeast-2	-> Seoul\
ap-northeast-3	-> Osaka\
eu-west-1 		-> Ireland\
eu-west-2 		-> London\
eu-west-3 		-> Paris\
eu-central-1 		-> Frankfurt\
eu-north-1		-> Stockholm\
cn-north-1		-> Beijing\
me-south-1    		-> Behrain\
\
Availability zone - cluster of data centers. Each region has, at least two, availability zones. AZ names end with alphabet (a, b, c etc.).\
Edge locations - will have content copy locally, to improve performance.\
Edge cache - central point for edge locations.\
\pard\pardeftab720\partightenfactor0

\fs32 \cf5 \cb6 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf5 EC2 (Elastic Cloud Compute)
\f0\b0\fs32 \
EC2 allows us to deploy virtual servers within AWS environment.\
EC2 elements:\
	AMI		=> templates of preconfigured EC2 instances\
	Instance Types 	=> defines the size of the instance based CPU, memory, storage and network\
	Instance family types 	=> General purpose, Compute Optimized (c*), GPU, Memory/RAM Optimized (r*), Storage/Disk Optimized (d*)\
	Instance Purchasing Options	=> On-demand instances, reserved instances, scheduled instances, spot instance, dedicated instances, dedicated hosts\
	Tenancy			=> shared, dedicated instances, dedicated hosts\
	User Data		=> perform functions upon boot, download latest OS updates\
	Storage Options	=> Persistent storage, ephemeral storage\
	Security	=> security groups => source & destination restrictions, inbound & outbound rules, ports & protocols usage\
			=> key pairs => public key & private key - Key Pairs are unique to the region\
\
Instance store volumes provide ephemeral/temporary storage for EC2 instance. temporary block-level storage. Useful for swap space and HDFS.\cf9 \
\pard\pardeftab720\partightenfactor0
\cf5 AMIs, Amazon Machine Images, are templates of a computer\'92s root volumes for EC2 instances.\
AMI and are either public (the owner grants launch permissions to all AWS accounts), explicit (the owner grants launch permissions to specific AWS accounts), or implicit (the owner has implicit launch permissions for an AMI).\
AMIs can be copied to other regions and made available to other AWS accounts in those regions when they create new EC2 Instances.\
\
Instance Types -\
t - typical			-- websites and web applications, development environments, build servers, code repositories, micro services, test and staging environments, and line of business applications\
m - multipurpose	-- small and mid-size databases, data processing tasks that require additional memory, caching fleets, and for running backend servers\
r - ram (memory)	-- high-performance databases, data mining and analysis, in-memory databases, distributed web scale in-memory caches, applications performing real-time processing of unstructured big data, Hadoop/Spark clusters, and other enterprise applications.\
x - eXtreme RAM	-- in-memory databases (e.g. SAP HANA), big data processing engines (e.g. Apache Spark or Presto), and high performance computing (HPC).\
c - compute	--  high-performance web servers, scientific modelling, batch processing, distributed analytics, high-performance computing (HPC), machine/deep learning inference, ad serving, highly scalable multiplayer gaming, and video encoding.\
i - iops	--  NoSQL databases (e.g. Cassandra, MongoDB, Redis), in-memory databases (e.g. Aerospike), scale-out transactional databases, data warehousing, Elasticsearch, and analytics workloads.\
d - disk	--  Massively Parallel Processing (MPP) data warehousing, MapReduce and Hadoop distributed computing, distributed file systems, network file systems, log or data-processing applications.\
h - high/hard disk throughput -- Amazon EMR-based workloads, distributed file systems such as HDFS and MapR-FS, network file systems, log or data processing applications such as Apache Kafka, and big data workload clusters.\
g - gpu, graphics	-- 3D visualizations, graphics-intensive remote workstation, 3D rendering, application streaming, video encoding, and other server-side graphics workloads.\
f - fpga, field programmable gate arrays -- genomics research, financial analytics, real-time video processing, big data search and analysis, and security.\
p - performance	-- GPUs, machine learning, deep learning, high-performance computing, computational fluid dynamics, computational finance, seismic analysis, speech recognition, autonomous vehicles, and drug discovery.\
a - Arm based applications   -- powered by AWS Graviton processors\
z - \
Inf - AWS Inferentia Chips - Recommendation engines, forecasting, image and video analysis, advanced text analytics, document analysis, voice, conversational agents, translation, transcription, and fraud detection.\
\
<instance_type><generation><speciality>.<instance_size>\
c5n.xlarge\
m5dn.4xlarge\
r5a.8xlarge\
i3en.12xlarge\
\
A - AMD processors\
B - Block (EBS) bandwidth\
D - Disk storage\
E - EBS optimized\
G - Graviton processors\
N - Network bandwidth\
\
Hard disk -> EBS\
		-> Instance Store/Ephemeral - local to server - data lost on reboot\
\
Security groups, act as a firewall for associated Amazon EC2 instances, controlling both inbound and outbound traffic at the instance level.\
Security groups can also be used by AWS services such as Amazon RDS, Amazon Redshift, Amazon EMR, and Amazon ElastiCache.\
By default, Security Groups allow only outbound traffic and block all incoming traffic. To enable inbound traffic, we have to specify the protocol, port and source.\
\
http://public-ip/latest/meta-data/\
http://public-ip/latest/meta-data/ami-id\
curl -w "\\n" http://169.254.169.254/latest/meta-data/instance-type\
get -w "\\n" http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key\
get -w "\\n" http://169.254.169.254/latest/meta-data/iam/security-credentials/rolename\
curl -w http://169.254.169.254/latest/user-data/\
\
Spot instances - Even though you bid on a particular price, you will actually pay the spot price which will either be the same as bid price or less than bid price. If the spot price rises above bid price, then you will lose the instance.\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf5 VPC (Virtual Private Cloud)
\f0\b0\fs32 \
A controlled access private network where you can group together, store and deploy resources in whichever way best suits your needs.\
VPC is within region.\
Creating multiple Subnets allows you to create logical network divisions between resources.\
VPC is like a house and subnets are like rooms, security group is like firewall.\
Amazon VPC used to define network topology, including definitions for subnets, network access control lists, internet gateways, routing tables, and virtual private gateways. \
\
CIDR - Classless Inter-Domain Routing\
Subnets\'a0are segments/range of a VPC's IP address range that allow you to group resources based on security and operational needs.\
Subnetting is the process of splitting a CIDR block into smaller CIDR blocks within the same range by using different subnet masks.\
Subnet mask is used to give the size of subnet.\
Minimum and maximum masks for VPC CIDR block are /16 to /28. 		http://cidr.xyz\
\
Public subnet & Private subnet:\
Having multiple subnets allows you to create both private and public subnets.\
Public Subnets allows the resources within it to access and connect to the internet, and the outside world to connect to those resources, depending on certain security controls. A public subnet has a route to an Internet gateway, i.e., for a web server accessible from the Internet.\
Private Subnets are not directly accessible from the internet. And so private Subnets are protected from the outside world, providing a greater level of security. A private subnet has no route to an Internet gateway, i.e., for a database server only accessed within the VPC.\
Each subnet must reside entirely within one Availability Zone and can't space across AZs.\
\
Routing tables used to route traffic to different destinations.\
Route table is at VPC level, all subnet within VPC, can communicate.\
Each subnet in VPC must be associated with a route table, the table controls the routing for the subnet.\
Main route table controls the routing for all subnets that are not explicitly associated with any other route table.\
\
\pard\pardeftab720\partightenfactor0
\cf10 Network access controls lists (ACLs) act as a firewall for associated subnets, controlling both inbound and outbound traffic at the subnet level.\
NACL is at subnet level, so any instance in subnet with an associated NACL will follow rules of NACL.\
In a VPC, the default NACL is set to allow all inbound and outbound traffic. If you deploy a custom NACL, then all inbound and outbound traffic is blocked.\
Security Groups control traffic flow at the instance level whereas NACLs control traffic at the subnet level.\
Security Groups can thus be used to enable traffic flow between different types of instances within a subnet itself which NACLs cannot control.\
NACL and Security Groups work in collaboration with each other to offer a more complete security setup.\
Security groups are stateful: This means any changes applied to an incoming rule will be automatically applied to the outgoing rule.\
NACLs are stateless: This means any changes applied to an incoming rule will not be applied to the outgoing rule.\
Security group is the firewall of EC2 Instances whereas Network ACL Is the firewall of the Subnet.\
Security group support allow rules only (by default all rules are denied). Network ACL support allow and deny rules.\
\pard\pardeftab720\partightenfactor0
\cf5 \
Virtual Private Network (VPN) is a network connection between Amazon resources within an AWS VPC and data or services in local datacenter (by using Customer Gateway).\
Virtual Private Gateway (VPG) with its routing rules and to associate the VPG with VPC.\
Virtual Gateways (VGW) are used to create a VPN connection between VPC and corporate network outside of AWS.\'a0\
An\'a0Internet Gateway (IGW)\'a0is a horizontally scaled, redundant and highly available VPC component that allows communication between instances in VPC and the Internet. Public subnets must be able to route to an IGW attached to VPC. You can only have one Internet Gateway per VPC.\
\
VPC peering used to connect two or more VPCs together, using IPV4 or IPV6, as if they were a part of the same network.\
VPC flow logs - Flow logs capture IP traffic going in and out of network interfaces. And these flow logs can be created for subnets, entire VPC, or even a single interface.\
Flow log data can be published to Amazon CloudWatch Logs and Amazon S3.\
\
DB Subnet Group\'a0is a collection of subnets (typically private), that you create in a VPC, and designate for your DB instances. Each DB subnet group should have subnets in at least two Availability Zones in a given region.\
\
An\'a0Elastic IP address\'a0(EIP)\'a0is a dedicated/static and public IP address that you can associate with an EC2 instance.\
You can get 5 EIP\'92s per region by default. You can raise a support request to get additional Elastic IP Addresses.
\f3\fs30 \cf11 \cb1 \

\f0\fs32 \cf5 \cb6 \
AWS offers two kinds of NAT devices\'97a\'a0NAT gateway\'a0or a\'a0NAT instance.\
NAT Gateway (NGW) allows instances within a private subnet access out to the internet; however, access to the instances within these private subnets cannot be initiated from the internet.\
Network Address Translation instance (NAT instance) in a public subnet, in VPC, to enable instances in the private subnet to initiate outbound IPv4 traffic to the Internet or other AWS services, but prevent the instances from receiving inbound traffic initiated by someone on the Internet.\
NAT allows instances in private subnets to connect to internet.\cf12 \cb8 \
\cf5 \cb6 \
A VPC endpoint is a virtual device which allows you to connect VPC to another AWS service without traversing any gateway of any kind, such as an internet Gateway, a virtual gateway or a NAT gateway.\
\
Last digit of IP address\
0 - nw ID\
1 - gateway\
2 -\
3 -\
4 to 254 - hosts can use\
255 - broadcast\
\
Private IP address ranges in IPV4 - 
\fs34 192.168.*.*, 172.16.*.* , 10.*.*.*\

\fs32 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 ENI (Elastic Network Interface)
\f0\b0 \
An elastic network interface is a logical networking component in a VPC that represents a virtual network card.\
An elastic network interface (ENI) is a virtual network interface that you can attach to an instance in a VPC.\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf5 IAM (Identity and Access Management)
\f0\b0\fs32 \
Web service that allows us to control access to AWS resources.\
Using AWS IAM, we can create and manage AWS users and groups and use permissions to allow and deny users access to AWS resources.\
\
Users\
	-> Root user \
	-> IAM user -> Login with GUI, with username & password\
			   -> CLI or programmatic access, with Access key and secret Key\
	User names should contain alphanumeric characters, or _+=,.@-\
Groups	=>  Helpful to manage permissions for multiple IAM users using IAM groups.\
Roles => Roles provide temporary access with different credentials. used by more than one entity. Roles delegate access to users, applications, or services that don't normally have access to AWS resources.\
	IAM roles used to allow AWS services and resources to gain access to resources in the same or another account. Roles are universal across the AWS platform.
\f3\fs30 \cf11 \cb1 \
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf5 \cb6 	  => 	AWS service -> one AWS service can talk to another service\
		Another AWS account => two different AWS accounts\
		Web Identity -> sign-in with other services, like Facebook, Google\
		SAML\
Policies => Document that explicitly lists permissions. IAM Policies are written in JSON format to allow or deny access.\
	Standalone policies that you can attach to multiple users, groups, and roles in your AWS account.\
	Managed policies apply only to identities (users, groups, and roles) - not resources.\
		-> AWS-managed policies\
		-> Customer-managed policies\
\
We can enable MFA access\
AWS IAM Identity Federation used by third-party identity providers to authenticate to AWS Account.\
Access Advisor is a feature available on the IAM console details page which you can use to identify which services are actually used by a user, group or role.
\f3\fs30 \cf11 \cb1 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs32 \cf5 \cb6 \
\pard\pardeftab720\partightenfactor0

\fs36 \cf9 S3 (Simple Storage Service)
\f0\b0\fs32 \
Fully managed object storage service. Up to 5GB is free.\
Supports file sizes up to 5TB.\
Provides availability 99.99% and durability 99.999999999%.\
S3 buckets - to store objects\
   	Buckets are at region level\
	By default, buckets and objects are private.\
	Bucket can hold an unlimited number of objects.\
	No limit to the size of a bucket.\
	100 buckets per AWS account.\
Storage classes - Standard (default), Standard - IA (Infrequent Access), One Zone - IA, Intelligent-Tiering, Glacier, Reduced Redundancy\
Security - Bucket policies, ACLs, IAM policies, Data Encryption (SSE - server side encryption, CSE - client side encryption) AES-256, AWS-KMS\
Data management - Versioning, \
				Lifecycle rules -> Standard, Standard - IA (Infrequent Access), Glacier\
For data backup\
S3 is useful for static website hosting.\
Auditing is provided by access logs\
\
Versioning-enabled buckets enable us to recover objects from accidental deletion or overwrite.\
Versioning - Un-versioned (default), Versioning-enabled, Versioning-suspended\
		At bucket level only, not at file/directory level\
Pre-sign URL for upload/download - We can have temporary link for object in S3 bucket. This is ONLY through CLI or program.\
To restrict access to an entire S3 bucket, use bucket policies. To restrict access to an individual object, use access control lists.\
\
CORS - Cross Origin Resource Share, a way for web applications that are loaded in one domain to interact with resources in a different domain.\
S3 can use Cross-Region Replication to replicate the contents of a bucket from one region to another.\
S3 supporting CRR, cross region replication\
S3 will support multipart upload, for files larger than 5GB.\
Eventual consistency on update & delete of objects.\
Amazon S3 can be accessed via the web-based AWS Management Console, programmatically via the API and SDKs, or with third-party solutions (which use the API/SDKs).\
\
Object key, unique id for object, is the combination of bucket, key and version ID (optional).\
S3 Standard is ideal for performance-sensitive use cases and frequently used data.\
S3 Infrequent Access (IA) is optimized for long-lived and less frequently accessed data, such as backups and older data.\
S3 Reduced Redundancy Storage (RSS) is designed for noncritical, reproducible data stored at lower levels of redundancy standards than the Standard or IA classes, thus reducing cost.\
\
Maximum size of a file that you can upload by using the Amazon S3 console is 78 GB.\
S3 Transfer Acceleration uses CloudFront\'92s globally distributed edge locations to provide fast file transfers over long distances.\'a0\
\pard\pardeftab720\partightenfactor0

\f1\b \cf9 \
EBS (Elastic Block Store)
\f0\b0 \
Provides block level storage to EC2 instances. EBS service is simply a virtual hard drive.\
Amazon EBS is suited for applications that require a database, file system, or access to raw block-level storage.\
\
EBS volumes act as NAS/raw devices. Automatically replicated within their Availability Zone.\
EBS volume types 	- SSD solid-state drives -- for transactional workloads involving frequent read/write operations with small I/O size, where the dominant performance attribute is IOPS. Volume size can be - 1GB to 16TB\
					=> General purpose (10K IOPS)  -- for transactional load\
					=> Provisioned IOPS (32K IOPS) -- for large database workloads\
				- HDD hard disk drives -- for large streaming workloads where throughput (measured in MiB/s) is a better performance measure than IOPS. Volume size can be - 500GB to 16TB\
					=> Cold HDD  -- for infrequent access data\
					=> Throughput optimized HDD  -- for Big Data\
EBS volumes can be attached to any running instance in the same Availability Zone.\
EBS snapshots -> EBS volumes can be saved using snapshots. Can be in S3.\
\
EBS Magnetic volumes - Previous generation HDD, 200 IOPS\
Encrypted using AES-256\
KMS - Key Management Service => CMK, DEK\
EBS is not for temporary storage.\
\

\f1\b EFS (Elastic File System)
\f0\b0 \
Provides file level storage service. Provides simple, scalable file storage for use with Amazon EC2 instances.\
EFS manages all the file storage infrastructure, avoiding the complexity of deploying, patching, and maintaining complex file system deployments.\
Performance mode - general purpose, Max I/O.\
EFS file sync, to sync existing files to Amazon EFS.\
Will use NFSv4.\
You can mount an Amazon EFS file system on instances in only one VPC at a time.\
Multiple Amazon EC2 instances and on-premises servers can simultaneously access an Amazon EFS file system, so applications that scale beyond a single instance can access a file system.\
To access EFS file systems from on-premises, you must have an AWS Direct Connect connection between your on-premises datacenter and your Amazon VPC.\
\pard\pardeftab720\partightenfactor0

\fs40 \cf9 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trcbpat8 \tamarb480 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt\clvertalbase \clcbpat13 \clwWidth4020\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt100 \clpadl100 \clpadb100 \clpadr100 \gaph\cellx2880
\clvertalt\clvertalbase \clcbpat13 \clwWidth6200\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt100 \clpadl100 \clpadb100 \clpadr100 \gaph\cellx5760
\clvertalt\clvertalbase \clcbpat13 \clwWidth8620\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt100 \clpadl100 \clpadb100 \clpadr100 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0

\f4\b\fs30\fsmilli15429 \cf9 \cb1 AMAZON S3\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf9 AMAZON EBS\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf9 AMAZON EFS
\fs26\fsmilli13429 \cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trcbpat8 \tamarb480 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt\clvertalbase \clshdrawnil \clwWidth4020\clftsWidth3 \clbrdrt\brdrs\brdrw20\brdrcf14 \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt120 \clpadl0 \clpadb120 \clpadr200 \gaph\cellx2880
\clvertalt\clvertalbase \clshdrawnil \clwWidth6200\clftsWidth3 \clbrdrt\brdrs\brdrw20\brdrcf14 \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt120 \clpadl0 \clpadb120 \clpadr200 \gaph\cellx5760
\clvertalt\clvertalbase \clshdrawnil \clwWidth8620\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt120 \clpadl0 \clpadb120 \clpadr200 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0

\f5\b0\fs32 \cf9 Can be publicly accessible\
Web interface\
Object Storage\
Scalable\
Slower than EBS and EFS\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf9 Accessible only via the given EC2 Machine\
File System interface\
Block Storage\
Hardly scalable\
Faster than S3 and EFS\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf9 Accessible via several EC2 machines and AWS services\
Web and file system interface\
Object storage\
Scalable\
Faster than S3, slower than EBS\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trcbpat8 \tamarb480 \trbrdrl\brdrnil \trbrdrb\brdrs\brdrw20\brdrcf14 \trbrdrr\brdrnil 
\clvertalt\clvertalbase \clshdrawnil \clwWidth4020\clftsWidth3 \clbrdrt\brdrs\brdrw20\brdrcf14 \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt120 \clpadl0 \clpadb120 \clpadr200 \gaph\cellx2880
\clvertalt\clvertalbase \clshdrawnil \clwWidth6200\clftsWidth3 \clbrdrt\brdrs\brdrw20\brdrcf14 \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt120 \clpadl0 \clpadb120 \clpadr200 \gaph\cellx5760
\clvertalt\clvertalbase \clshdrawnil \clwWidth8620\clftsWidth3 \clbrdrt\brdrs\brdrw20\brdrcf14 \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt120 \clpadl0 \clpadb120 \clpadr200 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf9 Good for storing backups\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf9 Is meant to be EC2 drive\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf9 Good for shareable applications and workloads\cell \lastrow\row
\pard\pardeftab720\partightenfactor0

\f1\b \cf9 \cb6 \
Amazon Glacier
\f0\b0 \
Very low cost, long term, durable storage solution, suitable for infrequently accessed data.\
Doesn't provide instance access to data.\
Archives - any object similar to S3. Each object, called as archive, will have unique ID and description.\
\pard\pardeftab720\partightenfactor0
\cf5 \
\pard\pardeftab720\partightenfactor0
\cf9 Vaults - container for storing Glacier archive\
Data retrieval - Expedited, Standard, Bulk\
	\'95 Expedited retrievals are typically made available within 1 \'96 5 minutes.\
	\'95 Standard retrievals typically complete within 3 \'96 5 hours.\
	\'95 Bulk retrievals typically complete within 5 \'96 12 hour.\
Security - Encrypted using AES-256, vault access policies, vault lock policies\
We have to restore the files from Glacier to S3. There are retrieval cost (depending on the speed of retrieval).\
Durability - 99.999999999%\
Supports SSL/TLS encryption of data in transit and at rest.\
A single archive can be as large as 40TB.\
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 \
ELB (Elastic Load Balancer)
\f0\b0 \
ELB is used to direct & route traffic to multiple instances. Automatically distributes incoming application traffic across multiple EC2 instances.\
ELB is within the region\
Elastic Load Balancers will only send traffic to healthy EC2 instances. It will not try to restart the instance or terminate the instance.\
\
 - Application Load Balancer (ALB) => Routes traffic catering for multiple applications on the same EC2 instance. Advanced load balancing of HTTP/HTTPS traffic.\
							     Can send traffic to specific target groups/hosts. Functions at the application layer (layer 7 of OSI model).\
 - Network Load Balancer => Load balancing of TCP traffic. Will route requests at Transport layer (TCP) (layer 4 of OSI model).\
 - Classic Load Balancer => Routes traffic based on application and network information. Older generation LB. Will route requests at Transport layer (TCP) or Application layer (HTTP/HTTPS), SSL.\
\
Classic Load Balancer is ideal for simple load balancing of traffic across multiple EC2 instances, while the Application Load Balancer is ideal for applications needing advanced routing capabilities, microservices, and container-based architectures.\
If you need flexible application management, the recommendation is to use an Application Load Balancer. If extreme performance and static IP is needed, use a Network Load Balancer. If you have an existing application that was built within the EC2-Classic network, then use a Classic Load Balancer.\
\
Define load balancer\
	- Internal Load Balancer => internal IP address, can only be accessed from private AWS network, routes traffic to EC2 instances in private subnets.\
	- External Load Balancer => public IP address, receives traffic from the internet/clients and distributes to instances.\
Assign security groups\
Configure Security settings\
Configure health checks\
Add EC2 instances\
Add Tags\
\
Elastic Load Balancer configured with one or more listeners.\
Listener is configured with a protocol and a port for front-end (client to load balancer) connections, and a protocol and a port for back-end (load balancer to back-end instance) connections.\
\

\f1\b Auto Scaling
\f0\b0  \
Mechanism that automatically allows us to increase or decrease resources.\
Auto Scaling is well-suited for applications that experience hourly, daily, or weekly variability in usage.\
Auto Scaling defines a group with launch configurations and Auto Scaling policies.\
	- Launch Configuration 	-> template that an Auto Scaling Group uses to launch EC2 instances\
	- Auto Scaling Groups	-> group of EC2 instances\
Amazon CloudWatch alarms execute Auto Scaling policies to affect the size of fleet.\
\
- Better fault tolerance\
- Better availability\
- Better cost management\
\
Scaling Plans\
- Auto scaling minimum\
- Manual scaling\
- Scheduled scaling\
- On-demand scaling\
\
Auto Scaling automatically balances EC2 instances across zones when we configure multiple zones in Auto Scaling group settings.\
By default, you can only launch up to 20 instances per AWS account.
\f3\fs30 \cf11 \cb1 \
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf5 \cb6 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 ECS (Elastic Container Service)
\f0\b0 \
ECS used to run Docker-enabled applications packaged as container across a cluster of EC2 instances.\
Amazon Elastic Container Service is a highly scalable, high-performance\'a0container\'a0orchestration service that supports\'a0Docker\'a0containers and allows you to easily run and scale containerized applications on AWS.\
Amazon ECS cluster - comprised of a collection of EC2 instances that operate mostly the same as a single instance.\
ECS eliminates the need to install and operate container orchestration software, manage and scale a cluster of virtual machines, or schedule containers on those virtual machines.\
\

\f1\b EKS (EC2 Kubernetes Service)\
\
Fargate
\f0\b0 \
AWS Fargate is a compute engine for Amazon ECS that allows you to run\'a0containers\'a0without having to manage servers or clusters.\
AWS Fargate\'a0is an easy way to deploy containers on AWS.\
Fargate is like EC2 but instead of getting a virtual machine you get a container.\
\

\f1\b EMR (Elastic Map Reduce)
\f0\b0 \
Amazon EMR is the cloud-native big data platform, allowing teams to process vast amounts of data quickly and cost-effectively at scale.\'a0\
Using open source tools coupled with the dynamic scalability of\'a0EC2\'a0and scalable storage of\'a0S3.\
EMR gives analytical teams the engines and elasticity to run Petabyte-scale analysis for a fraction of the cost of traditional on-premise clusters.\
Jupyter-based\'a0EMR Notebooks\'a0for iterative development, collaboration and access to data stored across S3,\'a0DynamoDB and\'a0Redshift\'a0to reduce time to insight and quickly operationalize analytics.\
\
EMR service architecture consists of several layers\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls1\ilvl0\cf5 {\listtext	\uc0\u8226 	}Storage		- different file systems that are used with your cluster - HDFS, EMR File System, Local File System\
{\listtext	\uc0\u8226 	}Cluster Resource Management   -\'a0for managing cluster resources and scheduling the jobs for processing data - YARN\
{\listtext	\uc0\u8226 	}Data Processing Frameworks    - engine used to process and analyze data - Hadoop MapReduce and Spark\
{\listtext	\uc0\u8226 	}Applications and Programs     - to create processing workloads, leveraging machine learning algorithms, making stream processing applications, and building data warehouses - Hive, Pig, and Spark Streaming library\
\pard\pardeftab720\partightenfactor0
\cf5 \
Cluster is collection of EC2 instances. Each EC2 instance is called node. Each node has role/node type.\
Node types in EMR:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls2\ilvl0\cf5 {\listtext	\uc0\u8226 	}Master node: A node that manages the cluster by running software components to coordinate the distribution of data and tasks among other nodes for processing. The master node tracks the status of tasks and monitors the health of the cluster. Every cluster has a master node, and it's possible to create a single-node cluster with only the master node.\
{\listtext	\uc0\u8226 	}Core node: A node with software components that run tasks and store data in the Hadoop Distributed File System (HDFS) on cluster. Multi-node clusters have at least one core node.\
{\listtext	\uc0\u8226 	}Task node: A node with software components that only runs tasks and does not store data in HDFS. Task nodes are optional.\cf12 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf5 \cb6 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 Elastic Beanstalk
\f0\b0 \
Used to deploy web applications.\
With Elastic Beanstalk, you can quickly deploy and manage applications in AWS Cloud without worrying about infrastructure that runs those applications.\
Beanstalk provision and deploy appropriate resources to web applications, such as EC2, ELB, auto scaling, application health monitoring.\
AWS Elastic Beanstalk reduces management complexity without restricting choice or control.\
VPC, Elastic Beanstalk, CloudFormation, and IAM are all free.\
\
Create Application\
Upload Version\
Launch Environment\
Manage Environment\
\
Web Server Environment\
Worker Environment\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf5 AWS Lambda
\f0\b0\fs34 \
Lambda allows users to run code in response to events in a server less environment.\
Lambda will provision and utilize all infrastructure as a backend process, and user only need to provide the code to execute. \
Users are charged on an execution basis.\
Lambda functions\
	- Required resources\
	- Maximum execution timeout\
	- IAM role\
	- Handler name\
\
Lambda supports Python, Java, Go, C#, Node.js\
We can execute any script/command up to 15mins.\
Memory size can be 128MB-3008MB\
\
Up to 1 million requests & 400,000 GB-seconds are free per month. After this limit 20 cents per 1 million requests.\
\
There are two important arguments to the handler function \'96 event and context.\
Event object holds input data/parameters.\
The event object is dependent on the event source. So the structure of this event object varies from event to event and it acts as a source of input data for lambda handler.\
And context is what sets the general/run-time environment for Lambda function. \
So, you can use the context object to retrieve information about the context in which the function executes \'96 like the function name, remaining time, memory limit and other additional information.\
e.g.: get_remaining_time_in_millis, function_version, memory_limit_in_mb, client_context.client.app_version_name\
\
Invocation types\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls3\ilvl0\cf5 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Synchronous		- API Gateway event, Cognito event, SDK\
\ls3\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Asynchronous		- S3 event, SDK\
\pard\pardeftab720\partightenfactor0
\cf5 \
Event sources\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls4\ilvl0\cf5 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Push based		- API Gateway event, S3 event\
\ls4\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Pull/poll based	- DynamoDB streams event, Kinesis steams event, SQS event\
\pard\pardeftab720\partightenfactor0
\cf5 \
aws lambda update-function-code --function-name Search --zip-file fileb://resources/code.zip
\fs32 \
\
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 AWS Batch
\f0\b0 \
Used to manage and run batch computing workloads.\
AWS Batch creates and manages the compute resources in AWS account, giving users full control and visibility into the resources being used.\
Jobs\
Job definitions\
Job queues\
Job scheduling\
Compute environment - managed & unmanaged\
\

\f1\b Amazon Lightsail
\f0\b0 \
A virtual private server backed by AWS infrastructure. Lightsail is the easiest way to get started with AWS if you just need virtual private servers.\
Used to host simple websites, small applications and blogs.\
Lightsail includes everything you need to launch project quickly \'96 a virtual machine, SSD-based storage, data transfer, DNS management, and a static IP \'96 for a low, predictable price.\
You can manage instances using Lightsail console, Lightsail API, or Lightsail command line interface (CLI).\
\

\f1\b AWS WAF
\f0\b0 \
WAF - Web Application Firewall - provides additional security for web application tier.\
Protects application from common web exploits.\
\

\f1\b Route 53
\f0\b0 \
Route 53 is like DNS (resolution).\
DNS is a collection of rules and records which helps clients to understand how to reach a server through URLs.\
53 is DNS port number.\
Route53 offers three key services, record creation or the main registration, health checks and request handling to direct web request to the right server.\
Route 53 responds to queries to translate domain names into their corresponding IP addresses.\
Routing policies - simple, failover policy, waited round-robin, Geo based\
Health Checks can be used to determine if the primary site is online and if not, failover routing policy can redirect traffic to a secondary site.\
Can perform Domain Registration, DNS Management, Traffic Management, and Availability Monitoring.\
\
CNAME - URL points to another URL, works only for non-root domains\
Alias - URL points to AWS resource, works for root and non-root domains \
\
FTP - 20/21\
SSH port - 22\
Telnet port - 23\
http port - 80\
https port - 443\
\
HTTP Status Codes\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls5\ilvl0\cf5 {\listtext	\uc0\u8226 	}Informational responses (100\'96199)\
{\listtext	\uc0\u8226 	}Successful responses (200\'96299)\
{\listtext	\uc0\u8226 	}Redirects (300\'96399)\
{\listtext	\uc0\u8226 	}Client errors (400\'96499)\
{\listtext	\uc0\u8226 	}Server errors (500\'96599)\
\pard\pardeftab720\partightenfactor0
\cf5 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 AWS DirectConnect
\f0\b0 \
Can establish a dedicated network connection between on-premise network and an AWS direct connect location using industry standard 802.1q vlans.\'a0\
AWS Direct Connect provides 1 Gbps and 10 Gbps connections, and easily provision multiple connections if needed.\
Use AWS Direct Connect instead of establishing a VPN connection over the Internet to your Amazon VPC, avoiding the need to utilize VPN hardware that frequently can\'92t support data transfer rates above 4 Gbps.\
\

\f1\b AWS Storage Gateway
\f0\b0 \
Provide a gateway between datacenter's storage systems, such as your SAN, NAS, or DAS, and Amazon S3 in Glacier on AWS.\'a0\
Configurations - File gateway, \
			- Volume gateway - stored volume gateways, 	cached volume gateways\
			- Tape gateway => virtual tape library (VTL), virtual tapes, tape drives, storage gateway, media charger, archive\
Amazon Storage Gateway offers a way to upload data into the cloud either via the Internet or over a VPN tunnel (but is still restricted to the bandwidth available over the network).\
\

\f1\b AWS Snowball
\f0\b0 \
Used to securely transfer large amounts of data, either from on-premise datacenter to Amazon S3 or from Amazon S3 back to datacenter using a physical appliance.\
\pard\pardeftab720\partightenfactor0
\cf5 \ul \ulc5 petabyte\ulnone -scale data transport option that doesn\'92t require user to write any code or purchase any hardware to transfer your data.\
\

\f1\b AWS Snowmobile
\f0\b0 \
AWS Snowmobile is an even larger data transfer option that operates in \ul exabyte\ulnone -scale.\
Should only be used to move extremely large amounts of data into AWS.\
A Snowmobile is 45-foot-long ruggedized shipping container that is pulled by a semi-trailer truck.\
You can transfer 100 PB per Snowmobile.\
\

\f1\b AWS Trusted Advisor
\f0\b0 \
The status of checks provided by AWS Trusted Advisor is shown by using color coding on the dashboard page.\
Best practices and recommendation engine.\
Performance & security recommendations in\
	- cost optimisation -> EC2 Reserved Instance Optimization, Low Utilization EC2 Instances, Idle Load Balancers, Underutilized EBS Volumes, Unassociated EIP, Idle RDS Instances\
	- security	->  IAM Use/Password Policy, Security Groups - Specific Ports Unrestricted and Unrestricted Access, S3 Bucket Permissions, MFA on Root Account, RDS Security Group Access Risk\
	- fault tolerance -> EBS Snapshots, Load Balancer Optimization, Auto Scaling Group Resources, RDS Multi-AZ, Route 53 Name Server Delegations, ELB Connection Draining\
	- performance -> High Utilization EC2 Instances, Large Number of Rules in EC2 Security Group, Overutilized EBS Magnetic Volumes, EC2 to EBS Throughput Optimization, CloudFront Alternate Domain Names\
	- Service Limits\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs34 \cf15 Amazon DynamoDB
\f0\b0 \
NoSQL database from Amazon.\
Cloud native database, and it's designed for managing high volumes of records and transactions, without the need for provision capacity upfront.\
Fully managed NoSQL database service that offers high performance, predictable throughput, and low cost.\
\pard\tx566\pardeftab720\partightenfactor0
\cf15 DynamoDB is a fully managed, serverless, NoSQL key value and document database that delivers single digit millisecond performance at any scale.\
\pard\pardeftab720\partightenfactor0
\cf15 DynamoDB supports document & key-value data models.\
\pard\tx566\pardeftab720\partightenfactor0
\cf15 \
\pard\pardeftab720\partightenfactor0
\cf15 Will not use EC2.\
Amazon DynamoDB synchronously replicates data across three AZs in an AWS Region, giving high availability and data durability.\
3 copies of data will be stored within a region.\
DynamoDB supports\'a0eventually consistent\'a0and\'a0strongly consistent\'a0reads.\
Max 256 tables per region (soft limit).\
\
DynamoDB is a schema-less database that only requires a table name and primary key. \
Each Dynamo table requires a primary key that is used to partition data across DynamoDB servers. Optionally can have sort key.\
Table\'92s primary key is made up of one or two attributes that uniquely identify items, partition the data, and sort data within each partition.\
Two kinds of primary keys (or primary index or table index):-\
	- Partition Key  (or hash key)\
	- Partition Key and Sort Key (or range key) <- composite primary key\
Partition key is used to partition data across hosts for scalability and availability.\
Sort key allows sorting/searching within a partition.\
Combination of Primary key and sort key uniquely identifies each item in table.\
In Amazon DynamoDB, a table is a collection of items (or rows), and each item is a collection of attributes (or columns).\
Each attribute in an item is a name-value pair. An attribute can be a scalar (single-valued), a JSON document, or a set.\
Max size of partition key 2KB\
Max size of sort key 1KB\
Each table item/record can store up to 400KB of data.\
\pard\pardeftab720\partightenfactor0

\fs42 \cf15 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl150 \clpadb150 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl150 \clpadb150 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0

\f7\b\fs29 \cf16 \cb1 \kerning1\expnd0\expndtw0 RDBMS\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 DynamoDB\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0

\f3\b0 \cf16 Table\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Table\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Row\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Item\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Column\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Attribute\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Primary key\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Primary key\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Index\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Secondary index\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 View\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Global secondary index\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat19 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Nested table or object\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Map\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trwWidth6878\trftsWidth3 \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx4320
\clvertalc \clcbpat17 \clbrdrt\brdrs\brdrw20\brdrcf18 \clbrdrl\brdrs\brdrw20\brdrcf18 \clbrdrb\brdrs\brdrw20\brdrcf18 \clbrdrr\brdrs\brdrw20\brdrcf18 \clpadt120 \clpadl120 \clpadb120 \clpadr120 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 Array\cell 
\pard\intbl\itap1\pardeftab720\ri-720\partightenfactor0
\cf16 List\cell \lastrow\row
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf15 \cb6 \expnd0\expndtw0\kerning0
\

\fs36 Pros:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls6\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Highly available and durable\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Highly scalable with dynamic capacity\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Fast - Single digit millisecond latency, sub-second latency with DAX\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Fully Managed AWS database service\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
AWS Ecosystem\
\ls6\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Unlimited concurrent Read/Write operations\
\pard\pardeftab720\partightenfactor0
\cf15 \
Cons:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls7\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Non-relational, no SQL, no joins, limited transaction\
\ls7\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Eventually consistent, in many cases\
\ls7\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Data model dictated by query patterns\
\ls7\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Indexes increase cost\
\pard\pardeftab720\partightenfactor0
\cf15 \
Datatypes\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls8\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
String\
\ls8\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Number 		(for Int, Float & Date/Timestamp)\
\ls8\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Binary 		(for LOB, encrypted data)\
\ls8\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Boolean\
\ls8\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}Null\expnd0\expndtw0\kerning0
\
\ls8\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Set      		- String Set, Number Set & Binary Set\
\ls8\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
List 			(like Array)\
\ls8\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Map 		(for JSON documents)\
\pard\tx566\pardeftab720\partightenfactor0

\fs34 \cf15 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls9\ilvl0
\fs36 \cf15 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
Scalar types		- string, number, binary, boolean and null\
\ls9\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
Set types	 	- multiple scalar types	- string set, number set, binary set\
\ls9\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
Document types 	- complex with nested attributes	- list (like arrays) and map (for json)\
\pard\tx566\pardeftab720\partightenfactor0
\cf15 \
Operators\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls10\ilvl0\cf15 {\listtext	\uc0\u8226 	}a\'a0=\'a0b\'a0		\'97 true if the attribute\'a0a\'a0is equal to the value\'a0b\
{\listtext	\uc0\u8226 	}a\'a0<\'a0b\'a0		\'97 true if\'a0a\'a0is less than\'a0b\
{\listtext	\uc0\u8226 	}a\'a0<=\'a0b\'a0		\'97 true if\'a0a\'a0is less than or equal to\'a0b\
{\listtext	\uc0\u8226 	}a\'a0>\'a0b\'a0		\'97 true if\'a0a\'a0is greater than\'a0b\
{\listtext	\uc0\u8226 	}a\'a0>=\'a0b\'a0		\'97 true if\'a0a\'a0is greater than or equal to\'a0b\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls10\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}a <> B\expnd0\expndtw0\kerning0
\
\ls10\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Logical operators:\'a0AND, OR, NOT\
\pard\pardeftab720\partightenfactor0
\cf15 \
\pard\tx566\pardeftab720\partightenfactor0
\cf15 Functions\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0\cf15 {\listtext	\uc0\u8226 	}IN \
{\listtext	\uc0\u8226 	}a\'a0BETWEEN\'a0b\'a0AND\'a0c\'a0\'97 true if\'a0a\'a0is greater than or equal to\'a0b, and less than or equal to\'a0c.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls11\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
contains\
{\listtext	\uc0\u8226 	}begins_with (a,\'a0substr)\'97 true if the value of attribute\'a0a\'a0begins with a particular substring.\
\ls11\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
attribute_exists\
\ls11\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
attribute_not_exists\
\ls11\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
attribute_type\
\ls11\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
size
\fs32 \cf12 \cb8 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls12\ilvl0
\fs36 \cf15 \cb6 {\listtext	\uc0\u8226 	}Queries support EQ | LE | LT | GE | GT | BEGINS_WITH | BETWEEN\
\pard\pardeftab720\partightenfactor0
\cf15 \
TTL (time to live)\
TTL attribute is the expiry timestamp of table item.\
TTL compares current time, in epoch time format, to the time stored in the TTL attribute of an item.\
After expiry, record will be marked for deletion, which will be deleted within 48 hours.\
DynamoDB deletes expired items on a best-effort basis to ensure availability of throughput for other data operations.\
\
Roles\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls13\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
AmazonDynamoDBFullAccess
\fs32 \
\ls13\ilvl0
\fs36 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
AmazonDynamoDBFullAccessWithDataPipeline
\fs32 \
\ls13\ilvl0
\fs36 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
AmazonDynamoDBReadOnlyAccess\
\pard\tx566\pardeftab720\partightenfactor0

\fs34 \cf15 \
\pard\pardeftab720\partightenfactor0

\fs36 \cf15 Backups\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls14\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
On-demand Backup\
\ls14\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8259 	}\expnd0\expndtw0\kerning0
Point-in-Time Recovery\'a0\
\pard\pardeftab720\partightenfactor0

\fs34 \cf15 \
\pard\tx566\pardeftab720\partightenfactor0
\cf15 There are two ways to query DynamoDB table - \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls15\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Query (Index based search)\
\ls15\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Scan  (Full table scan)\
\pard\tx566\pardeftab720\partightenfactor0
\cf15 \
Capacity units\
	Read Capacity Units (RCU)\
	Write Capacity Units (WCU)\
1 RCU represents 1 strongly consistent read per second \
			   OR 2 eventually consistent reads per second\
1 WCU represents 1 write per second up to 1KB\
4KB per RCU and 1KB per WCU\
1 partition supports up to 1000 WCUs and 3000 RCUs.\
Max 40,000 RCUs & 40,000 WCUs per table (soft limit)\
Max 80,000 RCUs & 80,000 WCUs per account (soft limit)\
\
\pard\pardeftab720\partightenfactor0

\fs36 \cf15 Amazon DynamoDB has two\'a0read/write capacity modes\'a0for processing reads and writes on tables:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls16\ilvl0\cf15 {\listtext	\uc0\u8226 	}Provisioned (default, free-tier eligible)   - accommodates workloads as they ramp up or down to any previously reached traffic level.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls17\ilvl0\cf15 {\listtext	\uc0\u8226 	}On-demand   - pay-per-request pricing for read and write requests\
\pard\tx566\pardeftab720\partightenfactor0
\cf15 \
\pard\pardeftab720\partightenfactor0
\cf15 DynamoDB adaptive capacity\'a0enables application to continue reading and writing to hot partitions without being throttled, provided that traffic does not exceed table\'92s total provisioned capacity or the partition maximum capacity.
\f3\fs32 \cf20 \cb8 \
\pard\pardeftab720\partightenfactor0

\f0\fs34 \cf15 \cb6 \
\pard\tx566\pardeftab720\partightenfactor0
\cf15 Partitions\
Partitions store DynamoDB table's data.\
Table can have multiple partitions, managed internally by DynamoDB.\
Number of partitions depends on table size and provisioned capacity.\
One partition = 10GB of data (hard limit)\
\pard\pardeftab720\partightenfactor0

\f3\fs28 \cf21 \cb8 Each active shards in Dynamo Streams corresponds to a partition, so the number of active shards on the dynamo db stream are equal to the number of partitions on the table.
\f0\fs34 \cf15 \cb6 \
\pard\tx566\pardeftab720\partightenfactor0
\cf15 \
\pard\pardeftab720\partightenfactor0
\cf15 A Local Secondary Indexes (LSI) is an index that has the same partition key as the table, but a different sort key.\
When table is created, LSI should be created, we can't add/delete LSI later.\
We can create up to 5 LSIs per table.\
LSIs supports both eventual consistent\'a0and\'a0strongly consistent\'a0(index) reads.\
LSIs will use RCUs and WCUs of base table.\
Use LSI to avoid additional costs.\
LSI needs both partition key & sort key.\
\
A Global Secondary Indexes (GSI) is an index with a partition key and sort key that can be different from those on the table.\
Global Secondary Indexes (GSI) used to query efficiently over any attribute in DynamoDB table. GSIs can treat any table attribute as a key, even attributes not present in all items.\
GSI supports only eventual consistent\'a0(index) reads.\
We can create up to 20 GSIs per table.\
GSIs are stored separately from base table. Have separate RCUs and WCUs (not shared with base table).\
When query/scan using GSI, capacity units of index will be used, not base table RCUs/WCUs.\
We can create/delete GSIs to table anytime.\
Use GSI when application needs finer throughput control.\
\
DAX - DynamoDB Accelerator\
DAX, like MemCache, provides in-memory caching service for DynamoDB\
DAX prevents hot partitions\
DAX supports ONLY eventual consistent reads (not strong consistent reads)\
Item level operations, like read/write, are processed through DAX\
Table level operations are sent to DynamoDB directly (DAX is bypassed)\
Strongly consistent reads are served by DynamoDB directly, DAX is bypassed\
DAX is not good for write intensive applications\
DAX cluster consists of one or more (up to 10) nodes - master node & read replica nodes\
DAX caches - item cache(for GetItem & BatchGetItem, TTL - 5min), query cache (for Query & Scan, TTL - 5min)\
\
DynamoDB Streams\
DynamoDB Streams\'a0operations let you enable or disable a stream on a table, and allow access to the data modification records contained in a stream.\
Streams are time ordered sequence of item level changes in any DynamoDB table.\
DynamoDB captures information about every modification (creates, updates, or deletes) to data items in the table.\
\
DynamoDB Triggers\
Invocations -\'a0Number of times a function is invoked in response to an event or invocation API call,\'a0includes successful and failed invocations.\
Errors - Number of invocations that failed due to errors in the function.\
Iterator Age\'a0- Indicates how old the last record in the batch was when processing finished. Emitted for stream-based invocations only. We can use the iterator age to estimate the latency between when a record is added and when the function processes it.\
Dead Letter Errors - Incremented when Lambda is unable to write the failed event payload to your configured Dead Letter Queues.\
Duration - Elapsed time from when the function code starts executing as a result of an invocation to when it stops executing.\
Throttles - Number of Lambda function invocation attempts that were throttled due to invocation rates exceeding the customer\'92s concurrent limits.\
\
DynamoDB Trigger Parameters:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls18\ilvl0\cf15 {\listtext	\uc0\u8226 	}Batch size\'a0\'96 The number of records to read from a shard in each batch, up to 1000. Lambda passes all of the records in the batch to the function in a single call, as long as the total size of the events doesn't exceed the\'a0payload limit\'a0for synchronous invocation (6 MB).\
{\listtext	\uc0\u8226 	}Batch window\'a0\'96 Specify the maximum amount of time to gather records before invoking the function, in seconds.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls19\ilvl0\cf15 {\listtext	\uc0\u8226 	}Concurrent batches per shard\'a0\'96 Process multiple batches from the same shard concurrently.\
{\listtext	\uc0\u8226 	}Maximum age of record\'a0\'96 The maximum age of a record that Lambda sends to function.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls20\ilvl0\cf15 {\listtext	\uc0\u8226 	}On-failure destination\'a0\'96 An SQS queue or SNS topic for records that can't be processed. When Lambda discards a batch of records because it's too old or has exhausted all retries, it sends details about the batch to the queue or topic.\
{\listtext	\uc0\u8226 	}Retry attempts\'a0\'96 The maximum number of times that Lambda retries when the function returns an error.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls21\ilvl0\cf15 {\listtext	\uc0\u8226 	}Split batch on error\'a0\'96 When function returns an error, split the batch into two before retrying.\
\pard\pardeftab720\partightenfactor0

\fs36 \cf15 \
Global tables\
DynamoDB table replicas will be created in desired regions and maintained automatically.\
Global tables uses DynamoDB streams (with old & new images).\
Table must be empty before enabling replication.\
\
Optimistic locking\'a0is a strategy to ensure that the client-side item that you are updating or deleting is the same as the item in DynamoDB.\
Optimistic locking prevents from accidentally overwriting changes that were made by others.\
With optimistic lo
\fs34 cking, each item has an attribute that acts as a version number, which will be updated whenever item is updated.\
\
\pard\pardeftab720\partightenfactor0
\cf10 aws dynamodb help\
aws dynamodb describe-table help\
\
aws dynamodb list-tables\
aws dynamodb list-tables --region us-west-2\
aws dynamodb describe-table --table-name notes\
aws dynamodb describe-table --table-name prf_audit_entry\
aws dynamodb describe-table --table-name prd_audit_entry --region us-west-2\
aws dynamodb describe-table --table-name TestTable --query "Table.[TableName,TableStatus,ProvisionedThroughput]"\
aws dynamodb describe-limits [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton <value>] [--cli-auto-prompt <value>]\
\pard\pardeftab720\sl460\partightenfactor0
\cf10 sudo aws dynamodb describe-limits\
\pard\pardeftab720\partightenfactor0
\cf10 \
aws dynamodb create-table --attribute-definitions <value> --table-name <value> --key-schema <value> [--local-secondary-indexes <value>] [--global-secondary-indexes <value>] [--billing-mode <value>] [--provisioned-throughput <value>] [--stream-specification <value>] [--sse-specification <value>] [--tags <value>] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton <value>] [--cli-auto-prompt <value>]\
aws dynamodb create-table --table-name test --attribute-definitions AttributeName=user_id,AttributeType=S AttributeName=timestamp,AttributeType=N \
								--key-schema AttributeName=user_id,KeyType=HASH AttributeName=timestamp,KeyType=RANGE --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1\
sudo aws dynamodb create-table --table-name ete_audit_entry --attribute-definitions AttributeName=audit_key,AttributeType=S AttributeName=range_key,AttributeType=S --key-schema AttributeName=audit_key,KeyType=HASH AttributeName=range_key,KeyType=RANGE --provisioned-throughput ReadCapacityUnits=1,WriteCapacityUnits=1 --stream-specification StreamEnabled=true,StreamViewType=NEW_AND_OLD_IMAGES --sse-specification Enabled=true,SSEType=KMS,KMSMasterKeyId=string --tags Key=intuit:billing:appenv,Value=prf Key=intuit:billing:component,Value="swimlane b" Key=intuit:billing:user-app,Value="idaudit" Key=Name,Value="ete_audit_entry"\
aws dynamodb create-table --cli-input-json file://foobar-schema.json\
aws dynamodb update-table --table-name <value> [--attribute-definitions <value>] [--billing-mode <value>] [--provisioned-throughput <value>] [--global-secondary-index-updates <value>]\
\pard\pardeftab720\sl460\partightenfactor0
\cf10 [--stream-specification <value>] [--sse-specification <value>] [--replica-updates <value>] [--cli-input-json <value>] [--generate-cli-skeleton <value>]\
\pard\pardeftab720\partightenfactor0
\cf10 aws dynamodb update-table --table-name e2e_audit_entry --provisioned-throughput ReadCapacityUnits=5000,WriteCapacityUnits=3500\
aws dynamodb delete-table --table-name test\
\
aws dynamodb put-item --table-name test --item file://item.json\
aws dynamodb put-item --table-name Music --item '\{"Artist": \{"S": "No One You Know"\}, "SongTitle": \{"S": "Call Me Today"\}, "AlbumTitle": \{"S": "Somewhat Famous"\}\}' --return-consumed-capacity TOTAL\
aws dynamodb put-item --table-name ProductCatalog --item file://item.json --condition-expression "attribute_not_exists(Id)"\
\
aws dynamodb delete-item --table-name <value> --key <value> \cf22 [--expected <value>] [--conditional-operator <value>] \cf10 [--return-values <value>] [--return-consumed-capacity <value>] [--return-item-collection-metrics <value>] [--condition-expression <value>] [--expression-attribute-names <value>] [--expression-attribute-values <value>] [--cli-input-json <value>] [--generate-cli-skeleton <value>]
\f8\fs28 \cf23 \cb24 \

\f0\fs34 \cf10 \cb6 aws dynamodb delete-item --table-name test --key file://key.json\
aws dynamodb delete-item --table-name ProductCatalog --key '\{"Id": \{"N": "456"\}\}' --condition-expression "attribute_exists(Price)"\
aws dynamodb delete-item --table-name ProductCatalog --key '\{"Id":\{"N":"888"\}\}' --condition-expression "(ProductCategory IN (:cat1, :cat2)) and (Price between :lo and :hi)" --expression-attribute-values file://values.json\
aws dynamodb delete-item --table-name prf_audit_entry --key '\{"audit_key":\{"S":"123151459752994$USER_ID$0"\},"range_key":\{"S":"2019-06-27T14:39:48.035Z"\}\}'\
aws dynamodb delete-item --table-name prf_audit_entry --key '\{"audit_key":\{"S":"123151459752994$USER_ID$0"\},"range_key":\{"S":"2019-06-27T14:37:58.996Z"\}\}' --condition-expression "#ttlive = :ttzero" --expression-attribute-names '\{"#ttlive":"ttl"\}' --expression-attribute-values '\{":ttzero":\{"N":"0"\}\}'\
aws dynamodb delete-item --table-name "logs" --key "\{"file_name": \{"S": "20200322"\}\}' --condition-expression "contains(file_name, :file_name)" --expression-attribute-values file://logs.json\
\
aws dynamodb update-item --table-name <value> --key <value> \cf22 [--attribute-updates <value>]\cf10  \cf22 [--expected <value>]\cf10  [--conditional-operator <value>] [--return-values <value>] [--return-consumed-capacity <value>] [--return-item-collection-metrics <value>] [--update-expression <value>] [--condition-expression <value>] [--expression-attribute-names <value>] [--expression-attribute-values <value>] [--cli-input-json <value>] [--generate-cli-skeleton <value>]\
aws dynamodb update-item --table-name test --key file://key.json --update-expression "SET #t = :t" --expression-attribute-names file://attribute-names.json --expression-attribute-values file://attribute-values.json\
aws dynamodb update-item --table-name Thread --key file://key.json --update-expression "SET Answered = :zero, Replies = :zero, LastPostedBy = :lastpostedby" --expression-attribute-values file://expression-attribute-values.json --return-values ALL_NEW\
aws dynamodb update-item --table-name ProductCatalog --key '\{"Id":\{"N":"789"\}\}' --update-expression "SET Price = Price - :p" --expression-attribute-values '\{":p": \{"N":"15"\}\}' --return-values ALL_NEW\
aws dynamodb update-item --table-name ProductCatalog --key '\{"Id":\{"N":"789"\}\}' --update-expression "REMOVE Brand, InStock, QuantityOnHand" --return-values UPDATED_OLD\
aws dynamodb update-item --table-name ProductCatalog --key '\{"Id":\{"N":"789"\}\}' --update-expression "ADD QuantityOnHand :q" --expression-attribute-values '\{":q": \{"N": "5"\}\}' --return-values UPDATED_NEW\
aws dynamodb update-item --table-name ProductCatalog --key '\{"Id":\{"N":"789"\}\}' --update-expression "DELETE Color :p" --expression-attribute-values '\{":p": \{"SS": ["Yellow", "Purple"]\}\}' --return-values ALL_OLD\
aws dynamodb update-item --table-name MusicCollection --key file://key.json --update-expression "SET #Y = :y, #AT = :t" --expression-attribute-names file://expression-attribute-names.json --expression-attribute-values file://expression-attribute-values.json --return-values ALL_NEW\
aws dynamodb update-item --table-name qa_audit_entry --key '\{"audit_key": \{"S":"4620816365076156520$USER_ID$0"\}, "range_key": \{"S":"2019-06-30T01:54:47.225Z"\}\}' --update-expression "SET #tt = :tt" --expression-attribute-names '\{"#tt": "ttl"\}' --expression-attribute-values '\{":tt": \{"N":"1574670600"\}\}'\
aws dynamodb update-item --table-name qa_audit_entry --region us-east-2 --key '\{"audit_key": \{"S":"123146214006069$REALM_ID$0"\}, "range_key": \{"S":"2019-07-01T18:04:58.310Z"\}\}' --update-expression "SET #ttlive = :ttvalue" --expression-attribute-names '\{"#ttlive": "ttl"\}' --expression-attribute-values '\{":ttvalue": \{"N":"1574674660"\}\}'\
aws dynamodb update-item --table-name ProductCatalog --key '\{"Id": \{"N": "456"\}\}' --update-expression "SET Price = Price - :discount" --condition-expression "Price > :limit" --expression-attribute-values file://values.json\
aws dynamodb update-item --table-name e2e_audit_entry --key '\{"audit_key": \{"S":"4620816365607753029$USER_ID$0"\}, "range_key": \{"S":"2019-07-09T12:02:16.402000Z"\}\}' --update-expression "SET #ttlive = :ttvalue" --condition-expression "#ttlive = :ttzero" --expression-attribute-names '\{"#ttlive": "ttl"\}' --expression-attribute-values '\{":ttvalue": \{"N":"1575263900"\},":ttzero": \{"N":"0"\}\}'\
aws dynamodb update-item --table-name prf_audit_entry --key '\{"audit_key":\{"S":"123150643920259$USER_ID$0"\},"range_key":\{"S":"2019-06-27T23:52:02.117Z"\}\}' --update-expression "SET #ttlive = :ttvalue" --condition-expression "#ttlive = :ttzero" --expression-attribute-names '\{"#ttlive":"ttl"\}' --expression-attribute-values '\{":ttvalue":\{"N":"1575614846"\},":ttzero":\{"N":"0"\}\}'\
\
aws dynamodb get-item --table-name <value> --key <value> \cf22 [--attributes-to-get <value>]\cf10  [--consistent-read | --no-consistent-read] [--return-consumed-capacity <value>] [--projection-expression <value>] [--expression-attribute-names <value>] [--cli-input-json <value>] [--generate-cli-skeleton <value>]\
aws dynamodb get-item --table-name Products --key '\{"ID":\{"N":"1"\}\}'\
aws dynamodb get-item --table-name test --key file://read.json\
aws dynamodb get-item --table-name Music --attributes-to-get '["Artist", "Genre"]' --key '\{ "Artist": \{"S":"No One You Know"\}, "SongTitle": \{"S":"Call Me Today"\} \}'\
aws dynamodb get-item --table-name qa_audit_entry --key '\{"audit_key": \{"S":"4620816365076156520$USER_ID$0"\}, "range_key": \{"S":"2019-06-30T01:54:47.213Z"\}\}'\
aws dynamodb get-item --table-name ProductCatalog --key '\{"Id":\{"N":"1"\}\}' --consistent-read --projection-expression "Description, Price, RelatedItems" --return-consumed-capacity TOTAL\
aws dynamodb get-item --table-name qa_audit_entry --attributes-to-get '["audit_key", "range_key", "ttl"]' --key '\{ "audit_key": \{"S":"4620816365076156520$USER_ID$0"\}, "range_key": \{"S":"2019-06-30T01:54:47.240Z"\} \}'\
aws dynamodb get-item --table-name qa_audit_entry --attributes-to-get '["audit_key", "range_key", "ttl"]' --key '\{ "audit_key": \{"S":"4620816365076156520$USER_ID$0"\}, "range_key": \{"S":"2019-06-30T01:54:47.213Z"\} \}' --return-consumed-capacity TOTAL\
aws dynamodb get-item --table-name grants_PRD --key '\{"grantId": \{"S":"4901059886788158897"\}, "glLimitId": \{"S":"0"\}\}'\
aws dynamodb get-item --table-name qa_audit_entry --key '\{"audit_key": \{"S":"9130348736213286$USER_ID$0"\}, "range_key": \{"S":"2018-11-21T16:29:23.043000Z"\}\}' --generate-cli-skeleton\
\
aws dynamodb batch-get-item --request-items <value> [--return-consumed-capacity <value>] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton <value>] [--cli-auto-prompt <value>]\
aws dynamodb batch-get-item --request-items file://batch/reads.json\
aws dynamodb batch-write-item --request-items file://items.json			\cf25 # BatchWriteItem\'a0cannot update items\cf10 \
aws dynamodb batch-write-item --request-items <value> [--return-consumed-capacity <value>] [--return-item-collection-metrics <value>] [--cli-input-json <value>] [--generate-cli-skeleton <value>]\
aws dynamodb batch-write-item --request-items file://e2e_audit_items.json --return-consumed-capacity TOTAL --return-item-collection-metrics SIZE\
\
aws dynamodb query --table-name <value> [--index-name <value>] [--select <value>] [--attributes-to-get <value>] [--consistent-read | --no-consistent-read] [--key-conditions <value>] [--query-filter <value>] [--conditional-operator <value>] [--scan-index-forward | --no-scan-index-forward] [--return-consumed-capacity <value>] [--projection-expression <value>] [--filter-expression <value>] [--key-condition-expression <value>] [--expression-attribute-names <value>] [--expression-attribute-values <value>] [--cli-input-json <value>] [--starting-token <value>] [--page-size <value>] [--max-items <value>] [--generate-cli-skeleton <value>]\
aws dynamodb query --table-name test --key-condition-expression "user_id = :uid" --expression-attribute-values file://find-values.json\
aws dynamodb query --endpoint-url http://localhost:8000 --select ALL_ATTRIBUTES --table-name Music --key-condition-expression "Artist = :a" --expression-attribute-values  '\{":a":\{"S":"AC/DC"\}\}'\
aws dynamodb query --table-name music --key-condition-expression "artist = :v1 AND song = :v2" --expression-attribute-values file://values.json\
aws dynamodb query --table-name qa_audit_entry --key-condition-expression "audit_key=:pk and range_key = :yyyy" --expression-attribute-values '\{":yyyy":\{"S":"2010-02-30T01:54:47.225Z"\},":pk":\{"S":"4620816365076156520$USER_ID$0"\}\}'\
aws dynamodb query --table-name qa_audit_entry --key-condition-expression "audit_key=:pk and begins_with(range_key, :yyyy)" --expression-attribute-values '\{":yyyy":\{"S":"2010"\},":pk":\{"S":"4620816365076156520$USER_ID$0"\}\}'\
aws dynamodb query --table-name qa_audit_entry --key-condition-expression "audit_key=:pk" --filter-expression "event_code >= :code" --expression-attribute-values '\{":pk":\{"S":"4620816365076156520$USER_ID$0"\},":code":\{"S":"15"\}\}' \
aws dynamodb query --table-name music --key-condition-expression "artist = :v1" --filter-expression "Released = :yr" --expression-attribute-values file://values.json\
aws dynamodb query --table-name test --key-condition-expression "artist = :v1 and begins_with (SongTitle, :v2)" --expression-attribute-values file://input/values.json\
aws dynamodb query --select ALL_ATTRIBUTES --table-name Music --key-condition-expression "Artist = :a and begins_with(SongTitle, :t)" --expression-attribute-values '\{":a":\{"S":"The Beatles"\}, ":t": \{"S": "h"\}\}'\
aws dynamodb query --table-name Reply --key-condition-expression "Id = :id and begins_with(ReplyDateTime, :dt)" --expression-attribute-values file://values.json\
aws dynamodb query --table-name music --key-condition-expression "artist = :v1" --filter-expression "attribute_exists(Note)" --expression-attribute-values file://values.json\
aws dynamodb query --table-name music --key-condition-expression "artist = :v1" --filter-expression "Released in (:v2, :v3, :v4)" --expression-attribute-values file://values.json\
aws dynamodb query --table-name music --key-condition-expression "artist = :v1" --filter-expression "Released > :v2" --expression-attribute-values file://values.json\
aws dynamodb query --table-name test --key-condition-expression "artist = :v1 AND SongTitle between :v2 and :v3" --expression-attribute-values file://values.json\
aws dynamodb query --index-name location_index --table-name single_dragon_tables --keycondition location = starts_with("usa#arizona")\
aws dynamodb query --table-name teams --index-name college-id --expression-attribute-value file://query.json\
aws dynamodb query --table-name ConfigCatalog --key-conditions '\{ "Id" : \{"AttributeValueList": [\{"S":"app1"\}], "ComparisonOperator": "EQ"\}\}' | jq -r '.Items[0].Parameters.M."nfs#IP".S'\
aws dynamodb query --table-name Movies --projection-expression "title" --key-condition-expression "#y = :yyyy" --expression-attribute-names '\{"#y":"year"\}' --expression-attribute-values '\{":yyyy":\{"N":"1993"\}\}' --page-size 5\
aws dynamodb query --table-name qa_audit_entry --key-condition-expression "audit_key=:pk and range_key = :yyyy" --filter-expression "event_code > :code" --expression-attribute-values '\{":yyyy":\{"S":"2010-02-30T01:54:47.225Z"\},":pk":\{"S":"4620816365076156520$USER_ID$0"\},":code":\{"S":"15"\}\}' \
aws dynamodb query --table-name qa_audit_entry --key-condition-expression "audit_key=:pk" --filter-expression "begins_with(create_date, :yyyy)" --expression-attribute-values '\{":pk":\{"S":"4620816365076156520$USER_ID$0"\},":yyyy":\{"S":"2019"\}\}' \
aws dynamodb query --table-name UserOrdersTable --key-condition-expression "Username = :username" --expression-attribute-values '\{ ":username": \{ "S": "daffyduck" \} \}' --select COUNT\
\
aws dynamodb scan --table-name <value> [--index-name <value>] \cf22 [--attributes-to-get <value>]\cf10  [--select <value>] \cf22 [--scan-filter <value>] [--conditional-operator <value>]\cf10  [--return-consumed-capacity <value>] [--total-segments <value>] [--segment <value>] [--projection-expression <value>] [--filter-expression <value>] [--expression-attribute-names <value>] [--expression-attribute-values <value>] [\ul --consistent-read \ulnone | --no-consistent-read] [--cli-input-json <value> | --cli-input-yaml] [--starting-token <value>] [--page-size <value> | \ul 1000\ulnone ] [--max-items <value>] [--generate-cli-skeleton <value>] [--cli-auto-prompt <value>]\
aws dynamodb scan --table-name test\
aws dynamodb scan --table-name TABLE_NAME > export.json\
aws dynamodb scan --table-name qa_audit_entry --max-items 5\
aws dynamodb scan --table-name prf_audit_entry --max-items 20 --projection-expression "audit_key,range_key"\
aws dynamodb scan --table-name qa_audit_entry --region us-east-2\
aws dynamodb scan --table-name prf_audit_entry --region us-west-2 --page-size 1 --max-items 10 --query 'Items' --output json\
aws dynamodb scan --table-name prf_audit_entry --region us-west-2 --page-size 1 --max-items 10 --query 'Items' --output text\
aws dynamodb scan --table-name prf_audit_entry --region us-west-2 --page-size 1 --max-items 10 --query 'Items' --output yaml\
aws dynamodb scan --table-name prf_audit_entry --region us-west-2 --page-size 1 --max-items 10 --query 'Items' --output table\
aws dynamodb scan --table-name prf_audit_entry --region us-west-2 --page-size 3 --max-items 20 --query 'Items' --output text | awk '\{print $1\}' | sort | uniq\
aws dynamodb scan --table-name prf_audit_entry --max-items 10 --select ALL_ATTRIBUTES --query 'Items' \
aws dynamodb scan --table-name test --filter-expression "username = :user" --expression-attribute-values file://uname.json\
aws dynamodb scan --index-name dragon_stats_index --table-name single_dragon_tables --filter family = "green"\
aws dynamodb scan --table-name MusicCollection --filter-expression "Artist = :a" --projection-expression "#ST, #AT" --expression-attribute-names file://expression-attribute-names.json --expression-attribute-values file://expression-attribute-values.json\
aws dynamodb scan --table-name my-prod-table | jq '\{"my-local-table": [.Items[] | \{PutRequest: \{Item: .\}\}]\}' > data.json\
aws dynamodb scan --table-name qa_audit_entry --filter-expression "begins_with(create_date, :yyyy)" --expression-attribute-values '\{":yyyy":\{"S":"2018"\}\}'\
aws dynamodb scan --table-name qa_audit_entry --filter-expression "range_key < :yyyy" --expression-attribute-values '\{":yyyy":\{"S":"2018"\}\}' --projection-expression "audit_key, range_key"\
aws dynamodb scan --table-name e2e_audit_entry --filter-expression "range_key between :yr1 and :yr2" --expression-attribute-values '\{":yr1":\{"S":"2017"\},":yr2":\{"S":"2018"\}\}' --select COUNT\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls22\ilvl0
\f9\fs32 \cf23 \cb1 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
TotalSegments
\f3 \'a0denotes the number of workers that will access the table concurrently.\
\ls22\ilvl0
\f9 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Segment
\f3 \'a0denotes the segment of table to be accessed by the calling worker.
\f0\fs34 \cf10 \cb6 \
\pard\pardeftab720\partightenfactor0
\cf10 aws dynamodb scan --table-name e2e_audit_entry --filter-expression "range_key between :yr1 and :yr2" --expression-attribute-values '\{":yr1":\{"S":"2017"\},":yr2":\{"S":"2018"\}\}' --select COUNT --total-segments 3 --segment 2\
aws dynamodb scan --table-name e2e_audit_entry --filter-expression "range_key between :yr1 and :yr2" --expression-attribute-values '\{":yr1":\{"S":"2017"\},":yr2":\{"S":"2018"\}\}' --select COUNT --total-segments 15 --segment 10\
aws dynamodb scan --table-name Movies --projection-expression "title" --filter-expression 'contains(info.genres,:gen)' --expression-attribute-values '\{":gen":\{"S":"Sci-Fi"\}\}' --page-size 100 --debug\
aws dynamodb scan --table-name prf_audit_entry --projection-expression "audit_key,range_key"--max-items 50 --filter-expression "#ttlive = :ttzero" --expression-attribute-names '\{"#ttlive": "ttl"\}' --expression-attribute-values '\{":ttzero": \{"N":"0"\}\}' > output_prf_keys.json\
sudo aws dynamodb scan --table-name e2e_audit_entry --projection-expression "audit_key, range_key" --filter-expression "range_key < :oldest_date" --expression-attribute-values '\{":oldest_date":\{"S":"2019-07-01"\}\}' --max-items 10 --total-segments 2 --segment 1 --query "Items[*]"\
\
aws dynamodb create-backup --table-name test --backup-name test-backup-2Nov2019\
aws dynamodb describe-backup --backup-arn <ARN>\
aws dynamodb restore-table-from-backup --target-table-name MusicRestored --backup-arn <ARN>\
delete-backup\
list-backups\
describe-continuous-backups\
update-continuous-backups\
\
aws dynamodb describe-time-to-live --table-name <value> [--cli-input-json <value>] [--generate-cli-skeleton <value>]\
aws dynamodb describe-time-to-live --table-name qa_audit_entry\
aws dynamodb update-time-to-live --table-name <value> --time-to-live-specification <value> [--cli-input-json <value>] [--generate-cli-skeleton <value>]\
aws dynamodb update-time-to-live --table-name qa_audit_entry '\{"Enabled": true, "AttributeName": "ttl"\}'\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl446\partightenfactor0
\ls23\ilvl0\cf10 aws dynamodb update-time-to-live --table-name TTLExample --time-to-live-specification "Enabled=true, AttributeName=ttl"\
\pard\pardeftab720\partightenfactor0
\cf10 \
\pard\pardeftab720\sl380\partightenfactor0
\cf10 aws dynamodb list-tags-of-resource --resource-arn arn:aws:dynamodb:us-east-1:123456789012:table/Movies\
\pard\pardeftab720\sl460\partightenfactor0
\cf10 aws dynamodb tag-resource --resource-arn arn:aws:dynamodb:us-west-2:123456789012:table/MusicCollection --tags Key=Owner,Value=blueTeam\
\pard\pardeftab720\partightenfactor0
\cf10 untag-resource\
\
create-global-table\
describe-contributor-insights\
describe-endpoints\
describe-global-table\
describe-global-table-settings\
\pard\tx220\tx720\pardeftab720\li720\fi-720\sl446\partightenfactor0
\cf10 aws dynamodb describe-table-replica-auto-scaling --table-name <value> [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton <value>] [--cli-auto-prompt <value>]\
aws dynamodb describe-table-replica-auto-scaling --table-name prf_audit_entry\
\pard\pardeftab720\partightenfactor0
\cf10 list-contributor-insights\
list-global-tables\
restore-table-to-point-in-time\
transact-get-items\
transact-write-items\
update-contributor-insights\
update-global-table\
update-global-table-settings\
update-table-replica-auto-scaling\
wait\
wizard\
\
\pard\pardeftab720\sl460\partightenfactor0
\cf10 aws dynamodbstreams list-streams\
aws dynamodbstreams describe-stream --stream-arn <value> [--limit <value>] [--exclusive-start-shard-id <value>] [--cli-input-json | --cli-input-yaml] [--generate-cli-skeleton <value>] [--cli-auto-prompt <value>]\
aws dynamodbstreams describe-stream --stream-arn 
\f3\fs28 \cf12 \cb8 arn:aws:dynamodb:us-east-2:316317974279:table/prd_audit_entry/stream/2019-05-20T13:43:31.918
\f0\fs34 \cf10 \cb6 \
aws dynamodbstreams describe-stream --stream-arn 
\f3\fs28 \cf12 \cb8 arn:aws:dynamodb:us-east-2:316317974279:table/prd_audit_entry/stream/2019-05-20T13:43:31.918 | grep "StartingSequenceNumber" | wc -l
\f0\fs34 \cf10 \cb6 \
aws dynamodbstreams describe-stream --stream-arn arn:aws:dynamodb:us-west-1:123456789012:table/Music/stream/2019-10-22T18:02:01.576\
aws dynamodbstreams get-records --shard-iterator "arn:aws:dynamodb:us-west-1:123456789012:table/Music/stream/2019-10-22T18:02:01.576|1|AAAAAAAAAAGgM3Y..."\
aws dynamodbstreams get-shard-iterator --stream-arn arn:aws:dynamodb:us-west-1:12356789012:table/Music/stream/2019-10-22T18:02:01.576 --shard-id shardId-00000001571780995058-40810d86 --shard-iterator-type LATEST\
\pard\pardeftab720\partightenfactor0
\cf10 \
aws lambda create-event-source-mapping --function-name my-function --batch-size 500 --starting-position LATEST --event-source-arn arn:aws:dynamodb:us-east-2:123456789012:table/my-table/stream/2019-06-10T19:26:16.525\
aws lambda update-event-source-mapping --uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b --maximum-retry-attempts 2 --maximum-record-age-in-seconds 3600 --destination-config '\{"OnFailure": \{"Destination": "arn:aws:sqs:us-east-2:123456789012:dlq"\}\}'\
aws lambda get-event-source-mapping --uuid f89f8514-cdd9-4602-9e1f-01a5b77d449b\
aws lambda invoke --function-name my-function out\
\
APIs\
create_table(TableName=table, AttributeDefinitions=attribute_definitions, KeySchema=key_schema, ProvisionedThroughput=iops)\
describe_table(TableName=table)\
update_table(TableName=table_name, ProvisionedThroughput=\{ 'ReadCapacityUnits': new_read_capacity, 'WriteCapacityUnits': new_write_capacity \}  )\
delete_table(TableName=table_name)\
\pard\pardeftab720\partightenfactor0
\cf15 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs32 \cf15 Amazon Aurora
\f0\b0 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf15 MySQL & PostgreSQL compatible enterprise class RDBMS database.\
Aurora combines performance & availability of high-end commercial databases with the simplicity & cost-effectiveness of open source databases.\
Up to 5 times throughput of MySQL and 3 times throughput of PostgreSQL.\
\
Default port number 3306\
Scales up to 64TB\
Offers 99.99% availability\
Caching layer is different from database process.\
\
Capacity type\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls24\ilvl0\cf15 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Provisioned\
\ls24\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Provisioned with Aurora parallel query enabled\
\ls24\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u10003 
\f0 	}\expnd0\expndtw0\kerning0
Serverless\
\pard\pardeftab720\partightenfactor0
\cf15 \
Aurora maintain 6 copies across 3 Availability Zones.\
Backups are stored in S3.\
Backtrack - Flashback to point in time (up to 72 hours), PITR without changing name, other properties.\
\
Aurora DB cluster consists of one or more DB instances and a cluster volume that managers the data for those DB instances.\
  Primary DB instance - one instance supports read/write operations. \
  Replica - Up to 15 read replicas\
Master & Replicas will point to same shared storage.\
Read quorum - 3 copies out of 6 and write quorum - 4 copies out of 6\
\
\pard\pardeftab560\slleading20\partightenfactor0
\cf15 Physical replication, called Aurora Global Database, uses dedicated infrastructure that leaves databases entirely available to serve application, and can replicate to up to five secondary regions with typical latency of under a second. It's available for both Aurora MySQL and Aurora PostgreSQL.\
\
Cross Region Read Replica - Aurora MySQL also offers logical cross-region read replica feature that supports up to five secondary AWS regions. It is based on single threaded MySQL binlog replication, so the replication lag will be influenced by the change/apply rate and delays in network communication between the specific regions selected.\
\pard\pardeftab720\partightenfactor0
\cf15 \
\pard\pardeftab560\slleading20\partightenfactor0
\cf15 Amazon Aurora builds its storage volumes in 10 GB logical blocks called protection groups. The data in each protection group is replicated across six storage nodes. Those storage nodes are then allocated across three AZs in the region in which the Amazon Aurora cluster resides.\
\
On each storage node, the records first enter an in-memory queue. Log records in this queue are deduplicated. After the records are persisted, the log record is written to an in-memory structure called the update queue. In the update queue, log records are first coalesced and then used to create data pages. If one or more log sequence numbers (LSNs) are determined to be missing, the storage nodes employ a protocol to retrieve the missing LSN(s) from other nodes in the volume. After the data pages are updated, the log record is backed up and then marked for garbage collection. Pages are then asynchronously backed up to Amazon S3. When the write becomes durable by being written to the hot log, the storage node will acknowledge receipt of the data. After at least four of the six storage nodes acknowledge receipt, the write is considered successful, and acknowledgement is returned to the client application. 
\f3\fs32 \cf0 \cb1 \kerning1\expnd0\expndtw0 \

\f0\fs34 \cf15 \cb6 \expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0
\cf15 Cluster endpoint always points to primary instance. If primary instance fails and is replaced, then the cluster endpoint points to the new primary instance.\
Failover takes less than 30 seconds.\
\
To simulate Aurora system crash\
--alter system crash [instance | dispatcher | node ];\
--alter system simulate <name> percent read replica failure ....;\
--alter system simulate <name> percent disk failure ....;\
--alter system simulate <name> percent disk congestion ....;\
\
DB parameter groups hold engine config values.\
DB cluster parameter groups affect entire cluster.\
\
Aurora cloning\
\
Aurora serverless - on-demand, autoscaling database configuration/endpoint.\
ACU - Aurora Capacity Unit\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs32 \cf15 Amazon ElastiCache
\f0\b0 \
Managed data cache service built from the open source Redis and Memcached database engines, as a managed service, Amazon ElastiCache can improve application performance by providing a frontline cache to respond to read requests made to an application or to a database.\
ElastiCache is a web service that makes it easy to deploy and run Memcached or Redis protocol-complaint server nodes in the cloud.\
\
Lazy loading\
Write through\
TTL\
\
ElasticCache for Redis			- for Geospatial apps, pub/sub messaging, leaderboards, meaning streaming, job queue, Machine Learning\
ElasticCache for Memcached\
\

\f1\b Amazon Neptune
\f0\b0 \
Graph database service that makes it easy to build and run applications that need to use a lot of queries and look ups to quickly visualize data.\
Amazon Neptune is an AWS native graph database engine, its optimized for storing data relationships and querying a graph quickly and efficiently.\
Neptune supports both the Property Graph model and the Resource Description Framework (RDF), providing the choice of two graph APIs: TinkerPop and RDF/SPARQL.\
\

\f1\b Amazon Redshift
\f0\b0 \
Data warehouse database\
Redshift is ideal for data warehousing, performing back office analysis and aggregation of data from multiple sources.\
Default port number - 5439\
\

\f1\b Amazon QLDB (Quantum Ledger DataBase)
\f0\b0 \
Amazon QLDB is a fully managed ledger database that provides a transparent, immutable, and cryptographically verifiable transaction log \uc0\u8206 owned by a central trusted authority. \
For systems of record, supply chain, registrations and banking transactions.\
QLDB tracks each and every application data change and maintains a complete and verifiable history of changes over time.\
Ledger databases are used for centralized, trusted authority to maintain a scalable, complete and cryptographically verifiable record of transactions.
\f3\fs28 \cf26 \cb8 \
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf5 \cb6 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf15 Amazon DocumentDB\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf15 with MongoDB compatibility\
Amazon DocumentDB is designed from the ground-up to give you the performance, scalability, and availability when operating mission-critical MongoDB workloads at scale. \
In Amazon DocumentDB, the storage and compute are decoupled, allowing each to scale independently.\'a0\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs34 \cf15 Amazon Timestream 
\f0\b0\fs32 \
Timestream is a purpose built, time-series database service for collecting, storing, and processing time-series data.\
Amazon Timestream\'a0is a fast, scalable, fully managed time series database service for IoT and operational applications that makes it easy to store and analyze trillions of events per day at 1/10th the cost of relational databases.\
It is designed to efficiently store and process time-series data by time intervals.\
As data grows over time, Timestream's adaptive query processing engine understands its location and format, making data simpler and faster to analyze.\
It also automates roll ups, retention, tiering, and compression of data so you have an easier time of managing the data.\
\pard\pardeftab720\partightenfactor0
\cf5 \

\f1\b AWS Data Pipeline
\f0\b0 \
AWS data pipeline used to process and move data between different AWS data services.\
AWS Data Pipeline is a web service used to process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals.\

\f1\b \
AWS Code Pipeline
\f0\b0 \
\

\f1\b Amazon CloudFront
\f0\b0 \
Acts as CDN (Content Delivery Network).\
Distributes data requested through web traffic closer to end user via edge locations.\
A web service that speeds up distribution of static and dynamic web content.\
CloudFront distribution will create and regularly update copies of content on Amazon service around the world.\
CloudFront caches content to edge locations.\
Distributions - Web distributions, RTMP distributions\
RTMP can stream media files using Adobe Media Server via the Adobe Real-Time Messaging Protocol (RTMP).\
Cloud Front Origin can be an S3 Bucket, EC2 or ELB.\cf15 \
\cf5 \

\f1\b SQS (Simple Queue Service)
\f0\b0 \
High available environment for data within configuration streams.\
Allows you to create your own applications for data extraction.\
Size of the message is 256KB.\
After retention period, the service (or unprocessed message) in message queue, will go to dead-letter queue (DLQ).\
Standard queue & FIFO queue (order not guaranteed).\
Visibility timeout, hides message for, 30sec (default) to 12hr (max).\
Maximum retention period - 4 days to 14 days.\
\

\f1\b SNS (Simple Notification Service)
\f0\b0 \
SNS topic\
Publisher & subscriber model\
Used in fan-out (to notify many services from single source)\
Uses JSON format\
\

\f1\b AWS Config\

\f0\b0 AWS Config can capture resource changes, act as resource inventory, can store configuration history for individual resources, can provide a snapshot in time of current resource configurations, enable notifications of when a change has occurred on a resource.\
\
Components - AWS resources, configuration items (CI), configuration streams, configuration history, configuration snapshots, configuration recorder, config rules, resource relationships, SNS topics, S3 buckets and AWS config permissions.\
\
AWS Config is a managed service that provides AWS resource inventory information and enables you to record configuration change history to enable security and governance requirements.\
With AWS Config, you can discover both existing and deleted resources at any point in time.\
\

\f1\b AWS CloudWatch
\f0\b0 \
Elastic Load Balancing and EC2 instances feed metrics to Amazon CloudWatch.\
Amazon CloudWatch, a metrics repository, used to view graphs, set alarms for troubleshooting, spot trends, and take automated action based on the state. \
Default monitoring interval is 5 minutes. We have to enable 
\f10\i detailed monitoring
\f0\i0  to change interval to 1 minute.\
Default metrics - CPU, N/W, Disk and Status\
Memory monitoring (metric) is not enabled by default. Amazon CloudWatch works on the Hypervisor level and does not monitor memory utilization.\
For custom metrics, we have to install CloudWatch agents.\
An alarm can be in the following three states: \'95 OK \'95 Alarm \'95 Insufficient_Data\
\

\f1\b AWS CloudTrail
\f0\b0 \
AWS CloudTrail is a service that is used to track, audit, and monitor all API requests made in your AWS account, making it an effective security analysis tool.\
AWS CloudTrail is used with AWS Config to help you identify who made the change and when, and with which API.\
Event history allows you to view, search, and download the past 90 days of activity in AWS account.\
You can create two types of trails for an AWS account:\
	A trail that applies to all regions\
	A trail that applies to one region\

\f1\b \
AWS Migration Hub\

\f0\b0 AWS Migration Hub functions as an auditor and collector of information helping you understand the IT environment you may plan to migrate.\'a0\
AWS Discovery - Discovery connector & Discovery Agent\
AWS Server Migration Service\
\

\f1\b CloudHSM
\f0\b0 \
Hardware security token, used for encryption\
AWS CloudHSM enables you to generate and use your encryption keys on a FIPS 140-2 Level 3 compliant HSM.\
CloudHSM protects keys with exclusive, single-tenant access to tamper-resistant HSMs in Amazon Virtual Private Cloud (VPC).\
HSM - Hardware Security Module\
\

\f1\b AWS Cloud9
\f0\b0 \
Provides IDE environment to write code.\
Cloud-based integrated development environment (IDE) that lets you write, run, and debug code with just a browser.\
\

\f1\b CloudFormation
\f0\b0 \
CloudFormation is like terraform, provides IaC (Infrastructure as Code)\
sections in code - resources, parameters, mappings, outputs\
Common layers - Network, Database, Application\
Use JSON and YAML formats to design CloudFormation templates.\
CloudFormation used to build a template of infrastructure as code which you can configure to exact specification that can be then be used to deploy servers that meet the configuration and security requirements for the business.\
AWS CloudFormation templates define the AWS resources required for the product, the relationships between resources, and the parameters that end users can plug in when they launch the product to configure security groups, create key pairs, and perform other customizations.
\f3\fs30 \cf11 \cb8 \

\f0\fs32 \cf5 \cb6 \

\f1\b AWS Systems Manager (SSM)
\f0\b0 \
Run command\
State manager\
Patch Management\
Parameter store\
\

\f1\b AWS OpsWork
\f0\b0 \
Works with Chef and Puppet\
\

\f1\b API Gateway
\f0\b0 \
Fully managed AWS service to create, deploy, maintain, secure APIs.\
Controls API access via IAM.\
\
RTO & RPO\
\

\f1\b Security Token Service (STS)\

\f0\b0 AWS Security Token Service (AWS STS) provides trusted users with temporary security credentials that can control access to AWS resources.\
These credentials are generated dynamically and provided to the user when requested.\
\

\f1\b AWS Directory
\f0\b0 \
AWS Directory Services allows you to manage directories in the cloud.\
Amazon Cloud Directory is a highly available, multi-tenant, directory-based store in AWS.\
\

\f1\b Amazon Inspector
\f0\b0 \
Automated security assessment service that can be set up to run within AWS account.\
Once enabled, the Amazon Inspector Service performs security assessments on applications of an EC2.\
\

\f1\b AWS Shield
\f0\b0 \
AWS Shield offers DDOS protection and comes as both basic and advance levels.\
\

\f1\b Amazon QuickSight
\f0\b0 \
Analyze data using the business intelligence tools.\
\

\f1\b AWS Database Migration Service (DMS) 
\f0\b0 \cf27 \
\cf5 To import data into an Amazon RDS DB instance.\
Makes it easy to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores.\
\

\f1\b AWS Total Cost of Ownership (TCO)
\f0\b0 \
Helps you reduce Total Cost of Ownership (TCO) by reducing the need to invest in large capital expenditures and providing a pay-as-you-go model that empowers you to invest in the capacity you need and use it only when the business requires it.\
- Server costs\
- Storage costs\
- Network costs\
- IT labor costs\
\

\f1\b AWS Glue
\f0\b0 \
Fully managed, serverless, ETL service.\
Glue works on top of Apache Spark environment to provide scale-out execution for data transformation jobs.\
Kinesis Data Firehose references table definitions stored in AWS Glue.\
AWS Glue is batch-oriented and it does not support streaming data.\
AWS Glue is the perfect choice if you want to create data catalog and push data to Redshift spectrum.\
\
AWS Glue Data Catalog is the metadata store for our dataset, so we can query the data with Athena.\
AWS Glue Crawlers - to scan data sets and populate the Glue Data Catalog.\
Output of crawler is one/more metadata tables defined in Data Catalog.\
Classifiers infer the schema of data.\
\
ETL job\
\
DPU - Data Processing Unit\
\

\f1\b Amazon Athena
\f0\b0 \
Athena is serverless, so there is no infrastructure to manage, and pay only for the queries that you run.
\f1\b \

\f0\b0 Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL.\
\
Create database mydb;\
Select * from gluedb.citydata limit 100;\

\f1\b \
Amazon Rekognition
\f0\b0 \
Amazon Rekognition can detect, analyze, and compare text, scenes and faces for a wide variety of user verification and identify content that should be removed.\
\

\f1\b AWS Managed Services
\f0\b0 \
AWS Managed Services provides simple and efficient means to make controlled changes to your infrastructure.\
For example, if you want to deploy an EC2 stack, or change your RDS database configuration settings, AWS Managed Services enables you to quickly and easily make the request through a dedicated self-service console.\
AWS Managed Services follows ITIL, a popular IT service management framework used by many Enterprises.\
\

\f1\b AWS Service Catalog
\f0\b0 \
To create a customized portfolio for each type of user in organization and selectively grant access to the appropriate portfolio.\
When you add a new version of a product to a portfolio, that version is automatically available to all current users.\
\

\f1\b AWS Organizations
\f0\b0 \
AWS Organizations can help you consolidate multiple AWS Accounts so that you can centrally manage them.\
\

\f1\b Amazon Cost Explorer
\f0\b0 \
Amazon Cost Explorer provides historical cost associated with an EC2 Instance usage.\
\

\f1\b AWS Budgets
\f0\b0 \
AWS Budgets gives you the ability to set custom budgets that alert when costs or usage exceed budgeted amount.\
Budgets can be tracked at the monthly, quarterly, or yearly level, and you can customize the start and end dates.\
You can further refine budget to track costs associated with multiple dimensions, such as AWS service, linked account, tag, and others.\
Budget alerts can be sent via email and/or Amazon Simple Notification Service (SNS) topic.\
\

\f1\b Amazon WorkSpaces
\f0\b0 \
Amazon WorkSpaces is a managed, secure, cloud desktop service. \
Used to provision either Windows or Linux desktops in just a few minutes and quickly scale to provide thousands of desktops to workers across the globe.\
You can pay either monthly or hourly, just for the WorkSpaces you launch, which helps you save money when compared to traditional desktops and on-premises VDI solutions.\
Amazon WorkSpaces helps you eliminate the complexity in managing hardware inventory, OS versions and patches, and Virtual Desktop Infrastructure (VDI), which helps simplify your desktop delivery strategy.\
\

\f1\b Amazon Elastic Transcoder
\f0\b0 \
Amazon Elastic Transcoder is media transcoding in the cloud.\
It is designed to be a highly scalable, easy to use and a cost effective way for developers and businesses to convert (or \'93transcode\'94) media files from their source format into versions that will playback on devices like smartphones, tablets and PCs.\
\

\f1\b Amazon SageMaker (
\f0\b0 AWS Machine Learning
\f1\b  deprecated)
\f0\b0 \
Fully managed Cloud based machine learning service, with Extensive collection of popular Machine Learning Algorithms.\
Build \'96 Jupyter Notebook development environment \
Train \'96 Managed Training infrastructure\
Deploy \'96 Scalable Hosting infrastructure\
\

\f1\b AWS Cognito
\f0\b0 \
Used for federated access\
Cognito service used to manage user identities on behalf of our applications.\
Provides secure access to AWS resources across multiple devices platforms and applications.\
\

\f1\b AWS Kinesis
\f0\b0 \
Fully managed AWS service for real time processing of streaming data at massive scale.\
Used to collect, process and analyze data/video streams in real time.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls25\ilvl0\cf5 {\listtext	
\f6 \uc0\u10003 
\f0 	}Video Streams - Capture, process, and store video streams for analytics and machine learning.\
{\listtext	
\f6 \uc0\u10003 
\f0 	}Data Streams - Build custom applications that analyze data streams using popular stream-processing frameworks.\
{\listtext	
\f6 \uc0\u10003 
\f0 	}Data Firehose - Load data streams into AWS data stores.\
{\listtext	
\f6 \uc0\u10003 
\f0 	}Data Analytics - Process and analyze streaming data using SQL or Java.\
\pard\pardeftab720\partightenfactor0
\cf5 \
Amazon Kinesis Data Firehose is a fully managed service for delivering real-time\'a0streaming data\'a0to S3, Redshift, Elasticsearch Service and Splunk.\
Amazon Kinesis Video streams - ingests, stores, encrypts and indexes video streams for real-time and batch analytics.\
\
Amazon Kinesis Data streams - ingests and stores data streams for processing.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls26\ilvl0\cf5 \kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Kinesis Producer library\
\ls26\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Kinesis Client library\
\ls26\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Kinesis Agent\
\ls26\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Kinesis Connector library\
\ls26\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f6 \uc0\u9642 
\f0 	}\expnd0\expndtw0\kerning0
Kinesis Storm Spout\
\pard\pardeftab720\partightenfactor0
\cf5 Shard is the base throughput unit of Kinesis data stream, provides 1MB/sec input and 2MB/sec output.\
Record is the unit of data stored in Kinesis data stream.\
\
\pard\pardeftab720\partightenfactor0
\cf27 aws kinesis list-streams\
aws kinesis put-record --stream-name ks01 --partition-key 01 --data HelloWorld\cf5 \
\
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 Managed Streaming for Kafka (MSK)
\f0\b0 \
Provisions Kafka cluster in AWS and manages Kafka upgrades.\
Amazon MSK manages setup, provisioning, AWS integrations and on-going maintenance of Kafka clusters.\
MSK uses & manages Apache ZooKeeper.\
\
\pard\pardeftab720\partightenfactor0
\cf27 aws kafka list-clusters\
aws kafka describe-clusters --cluster-arn arn:aws:kafka:......\cf5 \
\
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 Amazon ElasticSearch
\f0\b0 \
Amazon Elasticsearch is a managed service that simplifies the deployment, operation and scaling of an Elasticsearch cluster.\
Elasticsearch is an open-source search and analytics engine for use cases such as log analytics, real-time application monitoring, full-text search, security intelligence, business analytics and clickstream analysis.\
Elasticsearch is an open-source, RESTful, distributed search and analytics engine built on Apache Lucene.\
Provides support for open source Elasticsearch APIs, managed\'a0Kibana, integration with\'a0Logstash\'a0and other AWS services, and built-in alerting and SQL querying.\
Zone awareness to automatically replicate data across two AZs.\
\
ELK stack - ElasticSearch Logstash Kibana stack\
\
curl 'ES end-point'\
curl 'vpc-appname-****.us-west-2.es.amazonws.com'\
curl 'vpc-appname-****.us-west-2.es.amazonws.com/_cat/nodes?v'\
curl 'vpc-appname-****.us-west-2.es.amazonws.com/_cat/indices?v'\
curl -XPUT 'vpc-appname-****.us-west-2.es.amazonws.com/newindex?pretty'\
\

\f1\b AWS CloudSearch
\f0\b0 \
Provides full text search, in local file or S3 object or DynamoDB tables.\
Configuration service\
Document service\
Search service\
\

\f1\b AWS X-Ray
\f0\b0 \
AWS X-Ray is a great way to monitor how data is moving through application.\
Service that collects data about requests that application serves, and provides tools to view, filter, and gain insights into that data to identify issues and opportunities for optimization.\
\

\f1\b AWS Lake Formation
\f0\b0 \
A service that makes it easy to set up a secure data lake in days.\
A data lake is a centralized, curated, and secured repository that stores data, both in its original form and prepared for analysis.\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf27 Amazon RDS
\f0\b0  (Relational Database Service)
\fs32 \
Distributed relational database service by Amazon Web Services.\
RDS is the core platform for running relational databases on AWS.\
RDS is PasS for RDBMS.\
A web service that makes it easy to set up, operate, and scale a relational database in the cloud.\
MySQL, Aurora, MariaDB, Microsoft SQL Server, Oracle, PostgreSQL database.\
Amazon RDS for MySQL provides two distinct replication features:- Multi-AZ deployments and read replicas.\
Basic building block of Amazon RDS is the DB instance, an isolated database environment in the cloud.\
Amazon RDS Automated Backups are stored in S3. Retention period for Amazon RDS automated backups can be between 1-35 days.\
Amazon RDS offers free backup up to the size of your database.\
Hours of use, Additional Storage and Number of Requests determine the overall cost of RDS Instance.\
\
aws rds create-db-instance \\\
    --engine oracle-se1 \\\
    --db-instance-identifier mydbinstance \\\
    --allocated-storage 20 \\ \
    --db-instance-class db.m1.small \\\
    --db-security-groups mydbsecuritygroup \\\
    --db-subnet-group mydbsubnetgroup \\\
    --master-username masterawsuser \\\
    --master-user-password masteruserpassword \\\
    --backup-retention-period 3\
\
aws rds create-db-instance ^\
    --engine oracle-se2 ^\
    --db-instance-identifier oradbinstance ^\
    --allocated-storage 40 ^\
    --db-instance-class db.t3.small ^\
    --db-security-groups mydbsecuritygroup ^\
    --db-subnet-group mydbsubnetgroup ^\
    --master-username masterawsuser ^\
    --master-user-password masteruserpassword ^\
    --backup-retention-period 3\
  \
aws rds describe-db-instances                                \
 \
aws rds modify-db-instance \\\
    --db-instance-identifier mydbinstance \\\
    --backup-retention-period 7 \\\
    --no-auto-minor-version-upgrade \\\
    --no-apply-immediately\
\
aws rds modify-db-instance ^\
    --db-instance-identifier mydbinstance ^\
    --backup-retention-period 7 ^\
    --auto-minor-version-upgrade ^\
    --apply-immediately\
\
aws rds download-db-log-file-portion --db-instance-identifier mydbinstance --log-file-name trace/sqlnet-parameters --output text \
\
aws rds modify-db-instance \\\
    --db-instance-identifier <mydbinstance> \\\
    --engine-version <12.1.0.2.v10> \\\
    --option-group-name <default:oracle-ee-12-1> \\\
    --db-parameter-group-name <default.oracle-ee-12.1> \\\
    --allow-major-version-upgrade \\\
    --no-apply-immediately\
aws rds modify-db-instance --db-instance-identifier example-test \\\
    --db-parameter-group-name\'a0example-goldengate --apply-immediately\
aws rds reboot-db-instance\'a0\\\
    --db-instance-identifier example-test \
\
aws rds create-db-snapshot --db-snapshot-identifier "<your-snapshot-name>" --db-instance-identifier \'93<your-instance-identifier>\'94\
aws rds modify-db-snapshot --db-snapshot-identifier <mydbsnapshot> --engine-version <11.2.0.4.v12>  --option-group-name <default:oracle-se1-11-2>\
\
aws rds create-db-parameter-group --db-parameter-group-name example-goldengate \\\
    --description "Parameters to allow GoldenGate" --db-parameter-group-family oracle-ee-11.2\
aws rds modify-db-parameter-group --db-parameter-group-name example-goldengate \\\
    --parameters "ParameterName=compatible, ParameterValue=11.2.0.4, ApplyMethod=pending-reboot"\
\
 https://rds.amazonaws.com/\
    ?Action=CreateDBInstance\
    &AllocatedStorage=250\
    &BackupRetentionPeriod=3\
    &DBInstanceClass=db.m1.large\
    &DBInstanceIdentifier=mydbinstance\
    &DBSecurityGroups.member.1=mysecuritygroup\
    &DBSubnetGroup=mydbsubnetgroup\
    &Engine=oracle-se1\
    &MasterUserPassword=masteruserpassword\
    &MasterUsername=masterawsuser\
    &SignatureMethod=HmacSHA256\
    &SignatureVersion=4\
    &Version=2018-10-31\
    &X-Amz-Algorithm=AWS4-HMAC-SHA256\
    &X-Amz-Credential=AKIADQKE4SARGYLE/20140305/us-west-1/rds/aws4_request\
    &X-Amz-Date=20180305T185838Z\
    &X-Amz-SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date\
    &X-Amz-Signature=b441901545441d3c7a48f63b5b1522c5b2b37c137500c93c45e209d4b3a064a3\
\
https://rds.amazonaws.com/\
    ?Action=ModifyDBInstance\
    &ApplyImmediately=false\
    &AutoMinorVersionUpgrade=false\
    &BackupRetentionPeriod=7\
    &DBInstanceIdentifier=mydbinstance\
    &SignatureMethod=HmacSHA256\
    &SignatureVersion=4\
    &Version=2014-10-31\
    &X-Amz-Algorithm=AWS4-HMAC-SHA256\
    &X-Amz-Credential=AKIADQKE4SARGYLE/20131016/us-west-1/rds/aws4_request\
    &X-Amz-Date=20131016T233051Z\
    &X-Amz-SignedHeaders=content-type;host;user-agent;x-amz-content-sha256;x-amz-date\
    &X-Amz-Signature=087a8eb41cb1ab0fc9ec1575f23e73757ffc6a1e42d7d2b30b9cc0be988cff97\
\
SDK/APIs\
create_db_instance(...)\
describe_db_instances()\
modify_db_instance(...)\
create_db_snapshot(...)\
delete_db_instance(...)\
restore_db_instance_from_db_snapshot(...)\
create_db_subnet_group(...)\cf5 \
\pard\pardeftab720\partightenfactor0
\cf27 \
SELECT * FROM TABLE(rdsadmin.rds_file_util.read_text_file(p_directory => 'BDUMP', p_filename  => 'sqlnet-parameters'));\
SELECT text FROM table(rdsadmin.rds_file_util.read_text_file('BDUMP','rds-rman-validate-nnn.txt'));\
SELECT * FROM table(rdsadmin.rds_file_util.listdir('BDUMP')) order by mtime;\
select * from table(RDSADMIN.RDS_FILE_UTIL.LISTDIR('DATA_PUMP_DIR')) order by mtime;\
\pard\pardeftab720\partightenfactor0

\f1\b \cf5 \
AWS CLI  
\f2\b0\fs34 \cf7 \cb8 Amazon Web Services Command Line Interface
\f0\fs32 \cf5 \cb6 \
pip install awscli\
pip install awscli --upgrade --user\
\
aws [options] <command> <subcommand> [parameters]\
\
aws --version\
	aws-cli/2.0.46 Python/3.7.4 Darwin/19.6.0 exe/x86_64\
	aws-cli/1.16.102 Python/2.7.16 Linux/4.14.171-136.231.amzn2.x86_64 botocore/1.12.92\
	aws-cli/1.15.4 Python/3.6.3 Darwin/16.7.0 botocore/1.10.4\
aws help\
aws ec2 help\
aws autoscaling create-auto-scaling-group help\
\
aws configure\
$ aws configure --profile user2\
\
output format\
json\
text\
table\
export AWS_DEFAULT_OUTPUT="text"\
\
$ aws s3 ls\
aws s3 ls s3://satya-sparks/NYC_Parking_Tickets/\
aws s3 ls s3://satya-sparks --recursive --human-readable --summarize\
aws s3 ls s3://mybucket --recursive\
aws s3 ls --profile ml_user\
aws s3 cp mnist.csv s3://thirumani-bucket\
aws s3 cp /tmp/dir/ s3://thirumani-bucket/ --recursive\
aws s3 cp /tmp/my_dir/ s3://thirumani-bucket/ --recursive --exclude "*" --include "*.jpg"\
sudo aws s3 cp $OUTDIR.tar.gz s3://sr-cache/perfResults/$DATE_DIR/$RUN_ID/\
aws s3 cp -R <dir_name>\
time aws s3 cp --recursive --quiet . s3://test_bucket/test_smallfiles/\
aws s3 cp s3://mybucket/test.txt test_local.txt\
aws s3 cp s3://srcbucket/ s3://destbucket/ --recursive --exclude "a*" --exclude "b*"\
aws s3 cp s3://mybucket/test.txt s3://mybucket/test2.txt --acl public-read-write\
aws s3 cp file.txt s3://mybucket/ --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers full=emailaddress=user@example.com\
aws s3 mv test.txt s3://mybucket/test2.txt\
aws s3 mv s3://mybucket/test.txt s3://mybucket/test2.txt\
aws s3 mv s3://mybucket/ s3://mybucket2/ --recursive --exclude "mybucket/another/*"\
aws s3 rm s3://mybucket/test2.txt\
aws s3 rm s3://mybucket --recursive\
aws s3 mb s3://mybucket				# make bucket\
aws s3 mb s3://mybucket --region us-west-1\
aws s3 rb s3://mybucket				# remove bucket\
aws s3 rb s3://bucket-name --force\
aws s3 sync . s3://mybucket\
time aws s3 sync --quiet . s3://test-bucket/test_randfiles/\
aws s3 sync s3://mybucket s3://mybucket2\
aws s3 website s3://my-bucket/ --index-document index.html --error-document error.html\
aws s3api head-object --bucket test-bucket --key test_bigfiles/bigfile\
aws s3 presign s3://sthirumani/test.png\
aws s3 presign s3://sthirumani/test.png --expires-in 3600\
\pard\pardeftab720\sl340\partightenfactor0
\cf5 aws s3 ls s3://bucket/ --recursive | grep '\\.pdf$'				# any file that ended in pdf\
\pard\pardeftab720\sl320\partightenfactor0
\cf5 grep -r Key /PATH/TO/MOUNT/POINT/ 	# Mount S3 using\'a0s3fs\
\pard\tx566\pardeftab720\partightenfactor0
\cf5 \
\pard\pardeftab720\partightenfactor0
\cf5 aws iam create-user --user-name Satya\
aws iam create-group --group-name SysAdmin\
aws iam add-user-to-group --group-name SysAdmin --user-name Satya\
aws iam list-users\
aws iam list-users --output table\
aws iam list-users --query 'Users[*].[UserName,CreateDate]'\
aws iam list-users --query 'Users[*].\{Name:UserName,CreateDate:CreateDate\}'\
aws iam list-groups-for-user --user-name Satya\
aws iam list-groups\
aws iam get-user-policy --user-name myuser --policy-name mypolicy\
aws iam put-group-policy --group-name SysAdmin --policy-name admin-policy --policy-document file://sytem_admin_policy_doc.json\
aws iam list-group-policies --group-name SysAdmin\
aws iam delete-group-policy --group-name SysAdmin --policy-name admin-policy\
aws iam remove-user-from-group --user-name myuser --group-name mygroup\
aws iam delete-group --group-name SysAdmin\
aws iam list-server-certificates\
aws iam delete-server-certificate --server-certificate-name test [certificate_name]\
aws acm list-certificates\
aws acm delete-certificate --certificate-arn [certificate_with_specified_arn]\
\pard\tx566\pardeftab720\partightenfactor0
\cf5 \
aws ec2 describe-regions\
aws ec2 describe-availability-zones --region region-name\
sudo aws gamelift describe-ec2-instance-limits\
\pard\pardeftab720\partightenfactor0
\cf5 aws ec2 describe-images    --owners self    --query 'reverse(sort_by(Images,&CreationDate))[:5].\{id:ImageId,date:CreationDate\}'\
\
$ aws ec2 start-instances --instance-ids i-1348636c\
aws ec2 stop-instances --instance-ids i-1348636c\
aws ec2 terminate-instances --instance-ids i-1348636c\
$ aws ec2 describe-instances\
aws ec2 describe-instances --filter Name=instance-type,Values=t2.nano\
aws ec2 describe-instances --filters Name=instance-state-name,Values=stopped\'a0--region\'a0eu-west-1\'a0--output json\'a0| \'a0jq \'a0-r \'a0.Reservations[].Instances[] .StateReason.Message\
aws ec2 describe-instances --filters "Name=tag:Type,Values=Build" \\\
  --query "Reservations[0].Instances[0].PublicDnsName" | sed 's/"\\(.*\\)"/http:\\/\\/\\1\\/manage/'\
aws ec2 describe-instances --filters "Name=instance.group-id,Values=sg-1908e47c" | grep -w "i-........" | cut -f8\
aws ec2 run-instances --image-id ami-785bae10 --count 1 --instance-type t2.micro --key-name newpair.pem --security-group new-sg\
aws ec2 run-instances --image-id ami-785bae10 --count 2 --instance-type t2.micro --key-name MyKeyPair.pem --subnet-id subet-ad20432b\
\
aws ec2 describe-key-pairs\
aws ec2 describe-key-pairs --key-name MyKeyPair\
aws ec2 create-key-pair --key-name MyKeyPair --query 'KeyMaterial' --output text > MyKeyPair.pem\
aws ec2 create-key-pair --key-name newpair.pem\
aws ec2 delete-key-pair --key-name MyKeyPair\
\
aws ec2 delete-vpc --vpc-id vpc-a01106c2\
aws ec2 describe-tags --output table\
aws ec2 describe-spot-price-history help\
aws ec2 create-security-group --group-name my-sg --description "My security group"\
aws ec2 describe-security-groups | grep -i fw-1\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf5 aws ec2 describe-vpcs --region us-west-2\
aws ec2 describe-vpcs --region us-west-2 --filter "Name=tag:Name,Values=Web VPC"\
aws ec2 describe-vpc-attribute --region us-west-2 --attribute enableDnsHostnames --vpc-id vpc-24be405c\
\pard\pardeftab720\partightenfactor0
\cf5 aws ec2 describe-subnets | grep 10.2\
$ aws ec2 create-network-interface --subnet-id subnet-08202861 --description "2ndEIP" --groups sg-1908e47c --private-ip-address 10.20.0.11        \
aws ec2 describe-network-interfaces | egrep "PRIVATEIP.*10.20" | cut -f3-4 | sort\
aws ec2 allocate-address --domain vpc\
$ aws ec2 attach-network-interface --network-interface-id eni-46c51d23 --instance-id i-846943c5 --device-index 1\
$ aws ec2 associate-address --allocation-id eipalloc-54534336 --network-interface-id eni-46c51d23\
$ aws ec2 detach-network-interface --attachment-id eni-attach-2473bd6a\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf5 aws ec2 attach-internet-gateway --vpc-id "vpc-0a63bc555cf1539c2" --internet-gateway-id "igw-05f3e0ecc2a444c3d" --region us-west-2\
\pard\pardeftab720\partightenfactor0
\cf5 aws ec2 create-route --route-table-id rtb-g8ff4ea2 --destination-cidr-block 10.0.0.0/16 --vpc-peering-connection-id pcx-1a2b3c4d\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf5 \
\pard\tx566\pardeftab720\partightenfactor0
\cf5 aws emr add-steps --cluster-id j-XXXXXXXX --steps file://./step.json\uc0\u8232 aws emr add-steps --cluster-id j-XXXXXXXX --steps Type=IMPALA,Name='Impala program',ActionOnFailure=CONTINUE,Args=--impala-script,s3://myimpala/input,--console-output-path,s3://myimpala/output\
\pard\pardeftab720\partightenfactor0
\cf5 aws emr list-steps --cluster-id j-3SD91U2E1L2QX\
aws emr create-cluster --ami-version=3.3.0 --applications Name=Hue Name=Hive Name=Pig --use-default-roles --ec2-attributes KeyName=myKey --instance-groups \\\
  InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m3.xlarge InstanceGroupType=CORE,InstanceCount=2,InstanceType=m1.large\
\pard\pardeftab720\sl460\partightenfactor0
\cf5 aws emr create-cluster \\\
    --release-label emr-5.14.0 \\\
    --service-role EMR_DefaultRole \\\
    --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole \\\
    --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.large InstanceGroupType=CORE,InstanceCount=2,InstanceType=m4.large\
\pard\pardeftab720\partightenfactor0
\cf5 aws emr create-cluster --ec2-attributes '\{"InstanceProfile":"DataPipelineDefaultResourceRole","ServiceAccessSecurityGroup":"sg-0c5d3eb789241a641","SubnetId":"subnet-c94d7192","EmrManagedSlaveSecurityGroup":"sg-0814ebb68a5e428f6","EmrManagedMasterSecurityGroup":"sg-0aeb734548c57820b"\}' --service-role DataPipelineDefaultRole --release-label emr-5.30.1 --log-uri 's3n://idl-ingestion-316317974279-prd/idaudit/logs/idaudit_prod_export_logs' --steps '[\{"Args":["org.apache.hadoop.dynamodb.tools.DynamoDBExport","s3://idl-ingestion-316317974279-prd/idaudit/historicalData/idaudit~prd_audit_entry/2020-09-03-09-51-00","prd_audit_entry","1.1","25000"],"Type":"CUSTOM_JAR","ActionOnFailure":"TERMINATE_CLUSTER","Jar":"s3://dynamodb-dpl-us-west-2/emr-ddb-storage-handler/4.11.0/emr-dynamodb-tools-4.11.0-SNAPSHOT-jar-with-dependencies.jar","Properties":"","Name":"DDB export"\}]' --name 'idaudit_prod_export' --instance-groups '[\{"InstanceCount":1,"EbsConfiguration":\{"EbsBlockDeviceConfigs":[\{"VolumeSpecification":\{"SizeInGB":64,"VolumeType":"gp2"\},"VolumesPerInstance":4\}]\},"InstanceGroupType":"MASTER","InstanceType":"m5.4xlarge"\},\{"InstanceCount":10,"EbsConfiguration":\{"EbsBlockDeviceConfigs":[\{"VolumeSpecification":\{"SizeInGB":10000,"VolumeType":"gp2"\},"VolumesPerInstance":1\}]\},"InstanceGroupType":"CORE","InstanceType":"r5.24xlarge"\}]' --configurations '[\{"Classification":"mapred-site","Properties":\{"mapreduce.map.memory.mb":"16042","mapreduce.map.java.opts":"-Xmx12834m"\}\},\{"Classification":"emrfs-site","Properties":\{"fs.s3.enableServerSideEncryption":"true"\}\}]' --scale-down-behavior TERMINATE_AT_TASK_COMPLETION --region us-west-2 --tags 'intuit:billing:user-app=IDAudit' 'intuit:billing:appenv=prd' 'intuit:billing:component=swimlane b'\
\
aws rekognition detect-labels --image "S3Object=\{Bucket=photo-collection,Name=photo.jpg\}" --region us-west-2\
\
aws dynamodb list-tables\
aws dynamodb list-tables --region us-west-2\
aws dynamodb scan --table-name clients_table\
aws dynamodb delete-table --table-name clients_table\
\
aws application-autoscaling describe-scaling-activities --service-namespace dynamodb\
aws application-autoscaling register-scalable-target --service-namespace dynamodb --resource-id "table/TestTable" --scalable-dimension "dynamodb:table:WriteCapacityUnits" --min-capacity 5 --max-capacity 10
\f8\fs29\fsmilli14880 \cf21 \cb28 \

\f0\fs32 \cf5 \cb6 \
aws rds create-db-instance \\\
    --engine MySQL \\\
    --db-instance-identifier mydbinstance \\\
    --allocated-storage 20 \\ \
    --db-instance-class db.m1.small \\\
    --db-security-groups mydbsecuritygroup \\\
    --db-subnet-group mydbsubnetgroup \\\
    --master-username masterawsuser \\\
    --master-user-password masteruserpassword \\\
    --backup-retention-period 3\
aws rds create-db-instance ^\
    --engine oracle-se2 ^\
    --db-instance-identifier oradbinstance ^\
    --allocated-storage 40 ^\
    --db-instance-class db.t2.small ^\
    --db-security-groups mydbsecuritygroup ^\
    --db-subnet-group mydbsubnetgroup ^\
    --master-username masterawsuser ^\
    --master-user-password masteruserpassword ^\
    --backup-retention-period 4\
aws rds create-option-group \\\
    --option-group-name testoptiongroup \\\
    --engine-name oracle-ee \\\
    --major-engine-version 12.1 \\\
    --option-group-description "Test option group"  \
aws rds describe-db-instances\
aws rds modify-db-instance \\\
    --db-instance-identifier mydbinstance \\\
    --backup-retention-period 7 \\\
    --no-auto-minor-version-upgrade \\\
    --no-apply-immediately\
aws rds modify-db-instance ^\
    --db-instance-identifier mydbinstance ^\
    --backup-retention-period 7 ^\
    --auto-minor-version-upgrade ^\
    --apply-immediately\
aws rds download-db-log-file-portion --db-instance-identifier mydbinstance --log-file-name trace/sqlnet-parameters --output text\
aws rds modify-db-instance \\\
    --db-instance-identifier <oradbinstance> \\\
    --engine-version <12.1.0.2.v10> \\\
    --option-group-name <default:oracle-ee-12-1> \\\
    --db-parameter-group-name <default.oracle-ee-12.1> \\\
    --allow-major-version-upgrade \\\
    --no-apply-immediately\
aws rds modify-db-snapshot --db-snapshot-identifier <mydbsnapshot> --engine-version <11.2.0.4.v12>  --option-group-name <default:oracle-se1-11-2>\
\
aws configservice get-resource-config-history\
\
aws sns create-topic --name "app-dba-notification"\
aws sns subscribe --topic-arn <your-arn-here> --protocol email --notification-endpoint <your-email>\
\
aws cloudwatch put-metric-alarm --alarm-name "Write throughput" --alarm-description "write-throughput" --actions-enabled --alarm-actions "<your-arn>" --metric-name "WriteThroughput" --namespace "AWS/RDS" --statistic "Maximum" --dimensions "Name=DBInstanceIdentifier,Value=<your-instance-identifier>" --period 300 --evaluation-periods 2 --threshold 48000 --comparison-operator GreaterThanOrEqualToThreshold --treat-missing-data notBreaching\
aws cloudwatch put-metric-alarm --alarm-name DDB-UserErrors --alarm-description "Alarm when UserErrors in DynamoDB exceeds 0" --namespace AWS/DynamoDB --metric-name UserErrors --statistic Sum --period 60 --evaluation-periods 1 --threshold 0 --comparison-operator GreaterThanThreshold --unit Count --alarm-actions <SNS TOPIC ARN>\
\
aws logs start-query --log-group-name /aws/lambda/<function> --start-time <start-time-of -errors> --end-time <end-time-of-error> --query-string "filter @message like /Error/"\
sudo aws logs start-query --log-group-name /aws/lambda/idaudit-ddb-to-firehose-b-prd-us-west-2 --start-time 1598522938 --end-time 1598523781 --query-string "filter @message like /Error/"\
\pard\pardeftab720\sl360\partightenfactor0

\f8\fs27\fsmilli13880 \cf29 \cb1 fields @timestamp, @message | sort @timestamp desc | limit 20\
fields @timestamp, @message | filter @message like /6207367600000000003909172510/\
\pard\pardeftab720\partightenfactor0
\cf29 filter @message like /Exception/   | stats count(*) as exceptionCount by bin(1h)  | sort exceptionCount desc\
\pard\pardeftab720\sl334\partightenfactor0
\cf29 fields @message | filter @message not like /Exception/
\fs22\fsmilli11160 \cf21 \cb28 \
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf5 \cb6 \
aws logs get-query-results --query-id <query-id>\
aws logs get-log-events --log-stream-name 2020/08/27/[\\$LATEST]6adc1f10f7d441b7970df1c283bcb8d3 --log-group-name /aws/lambda/idaudit-ddb-to-firehose-b-prd-us-west-2\
\
complete -C '/usr/local/aws/bin/aws_completer' aws\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs34 \cf0 \cb1 \kerning1\expnd0\expndtw0 AWS Certifications
\f0\b0 \
jayendrapatil.com\
\pard\pardeftab720\partightenfactor0

\f11\fs32 \cf30 \cb8 \expnd0\expndtw0\kerning0
\ul \ulc30 http://satya-hadoop.blogspot.com/2019/01/aws-cloud-practitioner-exam.html
\f0 \cf5 \cb6 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf23 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 \kerning1\expnd0\expndtw0 ssh -i <key.pem> root@<public-dns>\
\pard\tx566\pardeftab720\partightenfactor0
\cf0 ssh -i ~/thirumani-key-pair.pem hadoop@ec2-18-188-11-180.us-east-2.compute.amazonaws.com\
ssh -i ~/thirumani-key-pair-west.pem ec2-user@ec2-34-215-179-220.us-west-2.compute.amazonaws.com\
ssh -i ~/075786066407.pem ec2-user@35.162.5.193\
\pard\tx566\pardeftab720\partightenfactor0
\cf5 \cb6 \expnd0\expndtw0\kerning0
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\pardirnatural\partightenfactor0
\cf31 \cb32 \kerning1\expnd0\expndtw0 \CocoaLigature0 sudo wget https://pjreddie.com/media/files/mnist_train.csv\
sudo wget https://pjreddie.com/media/files/mnist_train.csv --no-check-certificate\
\pard\tx566\pardeftab720\partightenfactor0
\cf31 sudo wget https://cdn.mysql.com//Downloads/Connector-J/mysql-connector-jav a-5.1.44.tar.gz\
\
sudo mkdir /var/lib/accumulo\
ACCUMULO_HOME=/var/lib/accumulo; export ACCUMULO_HOME\cf5 \cb6 \expnd0\expndtw0\kerning0
\CocoaLigature1 \
\pard\pardeftab720\partightenfactor0
\cf0 \cb1 \kerning1\expnd0\expndtw0 \
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs48 \cf5 \cb6 \expnd0\expndtw0\kerning0
GCP 
\f2\b0 \cf7 \cb8 Google Cloud Platform
\f0\fs32 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf2 =============================================================\
\pard\pardeftab720\partightenfactor0

\f12\b0\fs34 \cf33 \cb8 \expnd0\expndtw0\kerning0
A region is a specific geographical location where user can run resources. Each region has one or more zones.\
\pard\pardeftab720\partightenfactor0

\fs32 \cf7 \cb1 A\'a0
\f13\i zone
\f12\i0 \'a0is an isolated location within a region. 
\fs34 \cf33 \cb8 Resources that live in a zone are referred to as zonal resources.\
Google has 22+ regions, 61+ zones, 130+ network edge locations.\
\pard\pardeftab720\partightenfactor0
\cf25 \cb1 \

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrt\brdrnil \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalc \clcbpat34 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt160 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalc \clcbpat34 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt160 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalc \clcbpat34 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt160 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0

\fs32 \cf8 Region\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf8 Zones\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf8 Location\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0

\fs28 \cf7 asia-east1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Changhua County, Taiwan\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 asia-east2\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Hong Kong\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 asia-northeast1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Tokyo, Japan\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 asia-northeast2\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Osaka, Japan\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 asia-south1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Mumbai, India\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 asia-southeast1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Jurong West, Singapore\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 australia-southeast1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Sydney, Australia\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 europe-north1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Hamina, Finland\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 europe-west1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 b, c, d\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 St. Ghislain, Belgium\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 europe-west2\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 London, England, UK\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 europe-west3\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Frankfurt, Germany\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 europe-west4\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Eemshaven, Netherlands\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 europe-west6\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Z\'fcrich, Switzerland\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 northamerica-northeast1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Montr\'e9al, Qu\'e9bec, Canada\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 southamerica-east1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Osasco (S\'e3o Paulo), Brazil\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 us-central1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c, f\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Council Bluffs, Iowa, USA\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 us-east1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 b, c, d\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Moncks Corner, South Carolina, USA\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 us-east4\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Ashburn, Northern Virginia, USA\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 us-west1\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 The Dalles, Oregon, USA\cell \row

\itap1\trowd \taflags1 \trgaph108\trleft-108 \trbrdrl\brdrnil \trbrdrt\brdrnil \trbrdrr\brdrnil 
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx2880
\clvertalt \clcbpat35 \clwWidth3104\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx5760
\clvertalt \clcbpat35 \clwWidth6528\clftsWidth3 \clbrdrt\brdrnil \clbrdrl\brdrnil \clbrdrb\brdrnil \clbrdrr\brdrnil \clpadt140 \clpadl160 \clpadb160 \clpadr160 \gaph\cellx8640
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 us-west2\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 a, b, c\cell 
\pard\intbl\itap1\pardeftab720\partightenfactor0
\cf7 Los Angeles, California, USA\cell \lastrow\row
\pard\pardeftab720\partightenfactor0

\fs34 \cf25 \
Comparison between AWS & GCP\
\pard\pardeftab720\partightenfactor0

\fs36 \cf25 \ul 	
\f14\b AWS						GCP
\f12\b0 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf25 \ulnone   Availability Zone		Zone\
	VPC						Virtual Private Cloud\
	IAM						IAM\
	Market Place			Market Place\
	EC2						Compute Engine (VM)\
	S3						Cloud Storage\
	EFS						Filestore\
	EBS						Persistent Disk\
ElastiCache/CloudFront	Cloud CDN\
	Device Farm			Cloud Test Lab\
Elastic Beanstalk			App Engine\
	RDS						Cloud SQL\
	Redshift				BigQuery\
	DynamoDB				Bigtable\
ElasticContainerService	Kubernetes Engine\
	Lambda					Cloud Functions\
	API Gateway			Cloud Endpoints\
Elastic Transcoder		Cloud Video\
	Fargate					Cloud Run\
Elastic Load Balancer		Cloud Load Balancing\
	Direct Connect			Cloud Interconnect\
	Route 53				Domains, Cloud DNS\
	EMR						Cloud DataProc\
	Kinesis					Cloud Dataflow\
SNS/Kinesis Data Streams	Pub/Sub\
	Data Pipeline			Cloud Composer\
	CloudWatch				Stackdriver\
	Auto Scaling			Autoscaler\
	Cloud Trail			Cloud Logging\
	AWS Snowball			Transfer Appliance\
	Tags 					Labels\
\pard\tx566\pardeftab720\partightenfactor0
\cf25 	Reserved instances		VM with committed use discounts\
\pard\pardeftab720\partightenfactor0

\fs44 \cf25 \
\pard\pardeftab720\sl360\partightenfactor0

\fs34 \cf25 Organization\'a0resource is the\'a0root node\'a0of the Cloud Resource Hierarchy and all resources that belong to an organization are grouped under the organization node.\
\pard\pardeftab720\partightenfactor0
\cf25 GCP Project is an organizing entity for Google Cloud resources. It contains resources and services \'97 it may hold a pool of virtual machines, a set of databases, and a network that connects them with one another.\
Projects also contain settings and permissions, which specify security rules and who has access to what resources.\
Project represents a billable unit.\
Projects may be organised into folders, for logical grouping.\
\pard\pardeftab720\sl360\partightenfactor0
\cf25 Folder resources\'a0provide an\'a0additional grouping mechanism\'a0and isolation boundaries between projects.
\f3\fs30 \cf11 \cb8 \

\f12\fs34 \cf25 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf25 Resources are fundamental components of GCP. Resources belongs to a project.\
Resources are either global, regional or zone based.\
\
Organization => Folders => Projects => Resources
\f13\i \
\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls27\ilvl0
\f14\i0\b \cf25 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Compute
\f12\b0 : houses a variety of machine types that support any type of workload. The different computing options let you decide how involved you want to be with operational details and infrastructure amongst other things.\
\ls27\ilvl0
\f14\b \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Storage
\f12\b0 : data storage and database options for structured or unstructured, relational or non relational data.\
\ls27\ilvl0
\f14\b \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Networking
\f12\b0 : services that balance application traffic and provision security rules amongst other things.\
\ls27\ilvl0
\f14\b \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Stackdriver
\f12\b0 : a suite of cross-cloud logging, monitoring, trace, and other service reliability tools.\
\ls27\ilvl0
\f14\b \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Tools
\f12\b0 : services for developers managing deployments and application build pipelines.\
\ls27\ilvl0
\f14\b \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Big Data
\f12\b0 : services that allow you to process and analyze large datasets.\
\ls27\ilvl0
\f14\b \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Artificial Intelligence
\f12\b0 : a suite of APIs that run specific AI (artificial intelligence) and ML (machine learning) tasks on the Google Cloud platform.
\fs32 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf33 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs40 \cf33 \ul Cloud IAM (Identity and Access Management)
\f12\b0\fs34 \cf25 \cb1 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf22 Service that has access permissions (roles), which allow you to work with GCP resources in the project.\
\pard\pardeftab720\partightenfactor0
\cf22 \cb8 IAM controls access by defining who (identity/members) has what access (role) for which resource(permissions).\
Users => Roles => Permissions\
\

\f14\b Identity/members:
\f12\b0 \
	Google account\
	Google group\
	Service account	- belongs to an application or VM. Identified by unique email address. Types - User managed/Google managed.\
	G Suite domain\
	Cloud identity domain\
	allAuthenticatedUsers\
	allUsers\
\

\f14\b IAM Permissions
\f12\b0  determine the operations performed on a resource.\
Permissions can't be assigned directly to members/users.\
	Permissions - <service>.<resource>.<verb> \
	e.g.: compute.instances.start, storage.buckets.list\
IAM policy binds identity to roles which contains permissions.\
\

\f14\b IAM Roles
\f12\b0  - collection of permissions\
One or more permissions are assigned to roles.\
Primitive roles - Owner, Editor, Viewer, Browser\
Predefined roles - compute.admin, pubsub.publisher etc.\
Custom roles\
\pard\pardeftab720\partightenfactor0

\fs36 \cf25 \cb1 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs40 \cf33 \cb8 \ul \ulc33 GCP Network services\
\pard\pardeftab720\partightenfactor0

\f12\b0\fs36 \cf36 \cb1 \ulnone Network service tiers - Standard & Premium tiers (default)\
Standard tier uses regular connectivity based on ISP networks.\
Premium tier delivers traffic via Google's premium backbone.\
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf36 VPC (Virtual Private Cloud)
\f12\b0\fs36 \
Offers private & hybrid networking.\
Google Cloud VPC is a software defined network providing private networks for VMs.\
VPC network is a global resource with regional subnets (VPC network is global & subnet is regional).\
Each VPC has it's own Firewall.\
Firewall rules allow/restrict traffic within subnets.\
We can have 100 subnets per VPC.\
Modes - Auto mode & custom mode\
\pard\pardeftab720\partightenfactor0

\f14\b \cf36 \
\pard\pardeftab720\partightenfactor0

\fs38 \cf36 Cloud Load Balancing
\fs34 \cb8     
\f12\b0\fs36 \cb1 (similar to AWS ELB)
\f14\b \
\pard\pardeftab720\partightenfactor0

\f12\b0\fs34 \cf36 \cb8 Network load balancers balance the load of systems based on incoming IP protocol data like address, port and protocol type.\
Load balancers route traffic evenly to multiple endpoints.\
Two types of GCP load balancers - Network load balancer, HTTP(s) load balancer.\
Network load balancer balances \ul \ulc36 regional\ulnone  TCP/UDP traffic. \
HTTP(S) load balancing provides \ul global\ulnone  load balancing for HTTP(S) requests destined for GCE instances.\
\
Maglev provides NLB.\
Default response timeout of 30 seconds.\
HTTP(s) TCP session timeout of 10 minutes.\
\
Global HTTP(S)\
Global SSL Proxy\
Global TCP Proxy\
Regional\
Regional internal\
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf36 \cb1 Cloud Interconnect
\f12\b0\fs34 \cb8  - to connect on-premise systems to Google Cloud.\
\pard\pardeftab720\partightenfactor0

\fs36 \cf36 \cb1 Hybrid connectivity extends local DC to GCP. Using Cloud Interconnect, Cloud VPN and Peering.\
\pard\pardeftab720\partightenfactor0

\fs34 \cf36 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b \cf36 VPC Peering
\f12\b0 \
\pard\pardeftab720\partightenfactor0

\fs36 \cf36 \cb1 Virtual Private Cloud (VPC) Network Peering allows private RFC1918 connectivity across two VPC networks, regardless of whether or not they belong to the same project or the same organization.
\fs34 \cb8 \
VPC Peering advantages - Network latency & cost will be less, Provides network security.\
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf36 \cb1 Cloud VPN
\f12\b0\fs34 \cb8  - provides secure access to GCP resources through public internet.\
\
\pard\pardeftab720\partightenfactor0

\fs36 \cf36 \cb1 Public IPs - Ephemeral & Static IP addresses\
\pard\pardeftab720\partightenfactor0

\fs34 \cf10 gcloud compute addresses list				# to list Public IPs
\fs36 \cf36 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf36 \cb8 \
PoP	 - Edge Point-of-presence  - Google connects it's network to rest of internet via peering.\
Google Global Cache (GGC) - Like AWS CloudFront\
TCP BBR - Bottleneck Bandwidth & Roundtrip propagation time\
\pard\pardeftab720\partightenfactor0

\f14\b \cf36 \
\pard\pardeftab720\partightenfactor0

\fs38 \cf36 \cb1 Cloud CDN
\fs34 \cb8 			
\f12\b0\fs36 \cb1 (similar to AWS ElastiCache/CloudFront)
\fs34 \cf25 \
\pard\pardeftab720\partightenfactor0
\cf36 \cb8 Cache static content that is served from Cloud Storage.\
Cache load-balanced frontend content that comes from VM instances.\
CDN works with HTTP(S) load balancer.\
Google CDN uses complete URI as the cache key.\
Cache hit, cache miss, cache fill\
\pard\pardeftab720\partightenfactor0
\cf33 \
\pard\pardeftab720\partightenfactor0

\f14\b \cf36 Cloud DNS	\
\pard\pardeftab720\partightenfactor0

\f12\b0 \cf36 Create managed zones.
\f14\b \
\pard\pardeftab720\partightenfactor0

\f12\b0 \cf33 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs40 \cf33 \ul GCP Compute Services
\f12\b0\fs34 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf37 Infrastructure-as-a-service (Iaas) by Compute Engine.\
Platform-as-a-service (PaaS) by App Engine.\
Containers-as-a-service (Caas) by Kubernetes Engine.\
Functions-as-a-service (Faas) by Cloud Functions (like AWS Lambda).\
Container registry manages Docker images.\
\
Google uses KVM hyphervisor\
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf37 \cb1 Google 
\fs38 \cb8 Compute Engine (GCE)			
\f12\b0\fs36 \cb1 (similar to AWS EC2)
\fs34 \cb8 \
Google Compute Engine used to manage virtual machines (VMs) running different operating systems, including multiple flavours of Linux and Windows Server, on Google infrastructure.\
Certain Compute Engine resources live in regions or zones.\
Virtual machine Instances and persistent disks (standard disks/SSD) live in a zone.\
\pard\pardeftab720\partightenfactor0
\ls28\ilvl0\cf37 VMs are based on machine types with varied CPU and RAM configuration.\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls28\ilvl0\cf37 \cb1 \kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
General-purpose machine types\
\ls28\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Compute-optimized machine types\
\ls28\ilvl0\kerning1\expnd0\expndtw0 {\listtext	\uc0\u8226 	}\expnd0\expndtw0\kerning0
Memory-optimized machine types\
\pard\tx566\pardeftab720\partightenfactor0
\cf37 \
Instance groups - to group VM instances or to create group of VM instances.\
	Managed groups\
	Unmanaged groups\
Instance templates - Used to create template for VM instances. We can create instance group from template.\
Disks (like EBS volume) - Each VM instance will have at lease one disk (as a boot disk/persistent storage).\
VM snapshots - used for periodic backup of data.\
	Snapshot of Compute Engine persistent disk to backup disk, which can be used to recover lost data, transfer contents to new disk.\
VM images (like AMI)\
committed use discounts (like reserved instances) - Committing to vCPU and memory usage for long term, like 1 or 3 years.\
Sustained use discounts\
Metadata - to specify key-value pairs that are available to all VM instances (in the project).\
Health checks - to determine VM instance is healthy or not, by sending HTTP requests to instance.\
Zones - will confirm GCP zone availability status.\
Operations - records global operations.\
Quotas - displays Compute Engine quota. Some resources have global and regional quotas.\
\
\pard\pardeftab720\sl360\partightenfactor0
\cf37 Preemptible VM\'a0is an instance that you can create and run at a\'a0much lower price\'a0than a normal instance.\'a0Compute Engine\'a0might terminate\'a0(preempt) these instances if it requires access to those resources for other tasks.\
\
\pard\pardeftab720\sl340\partightenfactor0
\cf37 Google\'a0Cloud Launcher\'a0offers\'a0ready-to-go development stacks, solutions, and services to accelerate development.\'a0\
\pard\tx566\pardeftab720\partightenfactor0
\cf37 \
\pard\pardeftab720\partightenfactor0
\cf10 gcloud compute instances list\
gcloud compute instances list --filter='labels.production:fin'\
gcloud compute instances\
gcloud compute instances create --help\
gcloud compute instances create test-instance\
gcloud compute instances create vm2 --machine-type n1-standard-2 --zone asia-south1-c\
gcloud compute instances create celab2 --machine-type n1-standard-2 --zone $ZONE\
gcloud compute instances describe <your_vm>
\fs32 \cf37 \
\pard\tx566\pardeftab720\partightenfactor0
\cf37 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf37 Google 
\fs38 \cb8 App Engine (GAE)				
\f12\b0\fs36 \cb1 (similar to AWS 
\fs34 Elastic Beanstalk
\fs36 )
\fs32 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf37 \cb8 App Engine provides PaaS.\
Fully managed platform for deploying web apps at scale.\
Supports multiple languages, frameworks and libraries.\
Available in two environments\
	- Standard		- Apps will run in sandbox\
	- Flexible		- uses Docker containers to deploy and scale apps\
\
Task queues\
Security scans\
Blobstore\
Memcache\
\
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 gcloud app deploy \
\pard\pardeftab720\partightenfactor0
\cf37 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf37 \cb1 Google 
\fs38 \cb8 Kubernetes Engine (GKE)		
\f12\b0\fs36 \cb1 (similar to AWS ECS)
\fs34 \cb8 \
Google Kubernetes Engine provides a managed environment for deploying, managing, and scaling containerized applications using Google infrastructure.\
Kubernetes Engine clusters are powered by the\'a0Kubernetes,\'a0open source cluster management system.\
Kubernetes Engine environment consists of multiple machines grouped together to form a\'a0container cluster.\
GKE uses Kubernetes objects to create and manage cluster's resources.\
Kubernetes provides deployment\'a0object for deploying stateless applications like web servers.\'a0Service\'a0objects define rules and load balancing for accessing application from the Internet.\
Auto scaling, automatic upgrades and node auto-repair are some of the unique features of GKE.\
Kubernetes has a control plane (master nodes) and worker node. GKE provisions worker nodes as GCE VMs.\
\pard\pardeftab720\partightenfactor0
\cf33 \
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 kubectl run hello-server --image=gcr.io/google-samples/hello-app:1.0 --port 8080\
kubectl run nginx --image=inginx:1.15.7\
kubectl expose deployment hello-server --type="LoadBalancer"\
kubectl expose deployment nginx --port=80 --type=LoadBalancer\
kubectl get services\
kubectl get service hello-server\
kubectl get pods\
kubectl get pods -l "app=nginx"\
kubectl get deployments\
kubectl get deployment -l "run=nginx"\
kubectl apply -f demo_deploy.yaml\
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf25 \
\pard\pardeftab720\partightenfactor0

\f12\b0\fs34 \cf37 \cb8 Google Container Registry\'a0-\'a0GCR\
Repository of images\
Google Container Registry provides secure, private Docker repository storage on GCP.\cf33 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf25 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf37 Google Cloud Functions (GCF)			
\f12\b0 (similar to AWS Lambda)
\f14\b \
\pard\pardeftab720\partightenfactor0

\f12\b0\fs34 \cf37 \cb8 Cloud Functions is a serverless execution environment for building and connecting cloud services.\
To write simple, single-purpose functions that are attached to events emitted from cloud infrastructure and services.\
Serverless compute environments execute code in response to an event or invoked using trigger.\
e.g. Listen and respond to a file upload to Cloud Storage, a log change, or an incoming message on a Cloud Pub/Sub topic.\
Supports JavaScript, Python 3 and Go.\
Cloud functions can have asynchronous and synchronous triggers.\
\
index.js\
package.json\
\
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 gcloud functions delete get --trigger-http\
gcloud functions deploy kgSearch --runtime nodejs8 --trigger-http\
gcloud functions logs read --limit 100\
gcloud functions delete kgSearch\
\pard\pardeftab720\partightenfactor0
\cf33 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf37 \cb1 Cloud Run
\f12\b0\fs34 \cb8 \
Full managed GCP service, which brings serverless to containers.\
- Cloud Run\
- Cloud Run for Anthos\
\pard\pardeftab720\partightenfactor0
\cf33 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs40 \cf33 \ul GCP Storage services
\f12\b0\fs34 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf38 Storage services add persistence and durability to applications.\
Can be used to store unstructured data and folders/files.\
 - Object Storage\
 - Block Storage\
 - File System\
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf38 \cb1 Google Cloud Storage (GCS)
\f12\b0\fs36   (similar to AWS S3)
\fs34 \cb8 \
Unified object storage for a variety of applications.\
Google Cloud Storage allows world wide storage and retrieval of any amount of data at any time. \
Used to serve website content, storing data for archival and disaster recovery, or distributing large data objects to users via direct download.\
Applications can store & retrieve objects through API.\
Can scale up to exabytes.\
Provides 99.999999999% durability.\
Location types - data can be stored in single region, dual region, multiple regions.\
\
Can be used to store high frequency and low frequency access of data.\
storage classes:\
Standard - high frequency access, optimized for reduced latency\
Nearline - low frequency access, data accessed less than once a month (or) for backups, min 30 days\
Coldline - for data accessed least frequently, data accessed less than once a quarter (or) for DR data, min 90 days\
Archive - for data accessed least frequently, data accessed less than once a year (or) for DR/legal data, min 365 days\
\
Access control:\
Fire-grained\
uniform\
\
Retention period\
\
Operations:\
 	- Strongly consistent\
	- Eventual consistent\
\pard\pardeftab720\partightenfactor0
\cf33 \
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 gsutil help\
gsutil help mb\
gsutil ls\
gsutil ls gs://bucket-satya\
gsutil ls -l gs://bucket-satya\
\pard\pardeftab720\sl280\partightenfactor0
\cf10 gsutil ls -L gs://bucket_40\
\pard\pardeftab720\partightenfactor0
\cf10 gsutil ls -a gs://bucket-thirumani\
gsutil ls -r $OUTPUT_PATH\
gsutil cat gs://bucket-thirumani/demo.txt\
gsutil cat gs://bucket-thirumani/demo.txt#15951926063544647\
gsutil mb gs://bucket-satya/								# make bucket\
gsutil mb -l $REGION gs://$BUCKET_NAME\
\
gsutil cp ada.jpg gs://bucket-satya\
gsutil cp -r data gs://$BUCKET_NAME/data\
gsutil cp -r gs://bucket-satya/ada.jpg .\
gsutil cp ../test.json gs://$BUCKET_NAME/data/test.json\
gsutil -u [PROJECT_ID] cp gs://bucket-thirumani/test.json gs://$BUCKET_NAME/test.json\
gsutil mv gs://bucket-thirumani/ada.jpg gs://bucket-satya\
gsutil -m cp -r gs://enron_emails/allen-p .       	# multithread\
gsutil -m cp gs://cloud-samples-data/ml-engine/census/data/* data/\
gsutil -m rsync -r data gs://$BUCKET_NAME/data\
gsutil rm gs://BUCKET-NAME/ada.jpg\
\pard\pardeftab720\sl280\partightenfactor0
\cf10 gsutil rm -r gs://bucket_40/ \
\pard\pardeftab720\partightenfactor0
\cf10 gsutil du gs://bucket-satya/	\
\
gsutil versioning set on gs://satya-bucket\
gsutil acl ch -u AllUsers:R gs://bucket-satya/ada.jpg			# to make it public\
\pard\pardeftab720\sl280\partightenfactor0
\cf10 gsutil acl ch -u user@gmail.com:W gs://bucket_40\
\pard\pardeftab720\partightenfactor0
\cf10 gsutil acl ch -d AllUsers gs://BUCKET-NAME/ada.jpg\
\pard\pardeftab720\sl280\partightenfactor0
\cf10 gsutil acl ch -d user@gmail.com gs://bucket_40\
\pard\pardeftab720\partightenfactor0
\cf10 gsutil notification watchbucket [-i ChannelID] [-t ClientToken] ApplicationUrl gs://$BUCKET_NAME\
gsutil rewrite -s coldline gs://bucket-satya/photo.jpg\
gsutil signurl -d 10m ~/key.json gs://satya-bucket/demo.txt\
\pard\pardeftab720\partightenfactor0
\cf33 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf38 Transfer data to Cloud Storage buckets from Amazon S3, HTTP/HTTPS servers or other buckets.\
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf38 \cb1 Transfer Appliance
\f12\b0\fs34 \cb8  		(similar to AWS Snowball)\
used to quickly/securely transfer large amounts of data (more than 20 TB) to GCP via a high capacity storage server that you lease from Google and ship to our datacenter.\
\

\f14\b\fs38 \cb1 Persistent Disks
\fs34 \cb8 		
\f12\b0\fs36 \cb1 (similar to AWS EBS)
\fs34 \cb8 \
Persistent Disks provides block storage for GCE VMs.\
Can have one writer and multiple readers.\
Disk size can up to 64TB.\
Supports SSD & HDD.\
SSD offers best throughput for IO intensive applications.\
Storage types - Zonal, Regional, Local\
\

\f14\b\fs38 \cb1 Cloud Filestore
\fs34 \cb8 			
\f12\b0\fs36 \cb1 (similar to AWS EFS)\
Managed file storage service for GCE & GKE.\
NAS like filesystem interface and shared file system.\
All\'a0internal IP addresses\'a0in selected VPC network can connect to Cloud Filestore instance.\
Standard - 1TB\
Premium - 2+ TB to 64TB\
Filestore has built-in zonal storage redundancy for data availability.
\fs34 \cf36 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf33 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs40 \cf33 \ul GCP Database services
\f12\b0\fs34 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf39 For structured data use Cloud SQL, BigQuery, Bigtable, Spanner, Datastore, Memorystore.\
For unstructured data use Cloud Storage, Persistent Disk.\
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf39 Google Cloud Bigtable			
\f12\b0\fs36 \cf40 \cb1 (similar to AWS DynamoDB)
\fs34 \cf39 \cb8 \
Cloud Bigtable\'a0is Google's NoSQL Big Data database service.\
Cloud Bigtable is a fully managed, wide-column NoSQL database that offers low latency and replication for high availability.\
It's the same database that powers Google Search, Analytics, Maps, and Gmail.\
Bigtable instances used for workload, then use Bigtable HBase client to develop applications, as well as other standard open-source big data tools.\
Cloud Bigtable is an alternative to Apace HBase column-oriented database in VMs.\
\
Cloud Bigtable stores data in\'a0tables, which contain\'a0rows. Each row is identified by a\'a0row key.\
Data in a row is organized into\'a0column families, or groups of columns. A\'a0column qualifier\'a0identifies a single column within a column family.\
A\'a0cell\'a0is the intersection of a row and a column. Each cell can contain multiple\'a0
\f13\i versions
\f12\i0 \'a0of a value.\
Bigtable only scan on keys, and not on values. Bigtable needs an index.\
\pard\pardeftab720\partightenfactor0
\cf40 Queries are executed as index scan on Bigtable.\
\pard\pardeftab720\partightenfactor0
\cf39 \
Master table, tablet servers, memtable, SStables.\
\
Use\'a0cbt\'a0command\'a0to connect to a Cloud Bigtable instance, perform basic administrative tasks, and read and write data in a table.\
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 cbt help\
cbt version\cf33 \cb8 \
\cf10 \cb1 cbt createtable my-table\
cbt createfamily my-table cf1\
cbt ls\
cbt ls my-table\
cbt set my-table r1 cf1:c1=test-value\
cbt read my-table\
cbt deletetable my-table\
cbt doc\
cbt mddoc\
cbt listappprofile\
cbt listsnapshots\
cbt getsnapshot\
cbt listinstances\
cbt listclusters\
\pard\pardeftab720\partightenfactor0
\cf39 \cb8 \
Use HBase shell to connect to a Cloud Bigtable instance, perform basic administrative tasks, and read and write data in a table.\cf33 \
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 create 'my-table', 'cf1'\
list\
put 'my-table', 'r1', 'cf1:c1', 'test-value'	\cf33 \cb8 		# Put the value\'a0test-value\'a0in the row\'a0r1, using the column family\'a0cf1\'a0and the column qualifier\'a0c1:\
\cf10 \cb1 scan 'my-table'\
disable 'my-table'\
drop 'my-table'\
exit\cf33 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf40 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf39 Datastore
\fs34 \cf40  
\f12\b0 \
NoSQL (document) database which provides High availability and scalability, durability.\
Datastore depends on Bigtable.\
Google Cloud Datastore is masterless. And relies on Megastore which supports transactions.\
Datastore uses sharding.\
Datastore suits for semi-structured application data, durable key-value data.\
\
Datastore admin can be used to do backup/restore, export/import etc.\
\
Entity is like a row, kind is like table, property is like column, in RDBMS.\
Entities whose keys have the same root form an entity group (like foreign keys).\
\
We can query using GQL (Google Query Language).\cf33 \
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 select * from EmpTable where salary > 1000\
select * from EmpTable where __key__ = Key(EmpTable, 'Satya100')\
select * from student where __key__ has ancestor Key(student, 289389013433) \cf33 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf40 GQL does NOT have Joins, Group by, Aggregate functions, count(*).\
\pard\pardeftab720\partightenfactor0
\cf33 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf39 Firestore
\fs34 \cf40  
\f12\b0 \
Firestore is NoSQL database (document database).\
Next generation Datastore platform.\
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf25 \cb1 \
\pard\pardeftab720\partightenfactor0

\fs38 \cf39 \cb8 BigQuery			
\f12\b0\fs36 \cf40 \cb1 (similar to AWS Redshift)
\fs34 \cb8 \
BigQuery\'a0is a fully managed petabyte-scale data warehouse that runs on Google Cloud Platform.\
Data analysts and data scientists can quickly query and filter large datasets, aggregate results, and perform complex operations without having to worry about setting up and managing servers.\
BigQuery comes with cloud shell or a web console.\
BigQuery is serverless and scalable, has in-memory Business Intelligence (BI) Engine and machine learning built in.\
BigQuery can pull data from Cloud Storage, Bigtable, Spreadsheets/Google Drive.\
Automatically replicates data to keep seven day history of changes.\
BigQuery Data Transfer service - getting data from third party services to BigQuery.\
BigQuery Storage service\
BigQuery Query service\
BQML - BigQuery Machine Learning\
\
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 bq help\
bq utility\
\
select language, sum(views) from `samples.benchmark` where title like "%google%" group by language order by views desc;\
\pard\pardeftab720\partightenfactor0
\cf33 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf39 Google Cloud SQL
\fs34 \cf40 		
\f12\b0\fs36 \cb1 (similar to AWS RDS)
\fs34 \cb8 \
Cloud SQL\'a0is a fully managed RDBMS database service that makes it easy to set up, maintain, manage, and administer relational (PostgreSQL, MySQL and SQL Server) databases in cloud.\
RDBMS service which simplifies setup, maintenance and administration of database instances.\
\
There are two formats of data accepted by Cloud SQL: dump files (.sql) or CSV files (.csv).\
Cloud SQL supports - MySQL, PostgreSQL, SQL server.\
\
Encryption - External encryption - SSL or Cloud SQL Proxy\
		      Internal encryption - Google's internal network, cables, temporary files and backups\
Automatic storage increase - storage checked every 30 seconds, added automatically if needed.\
We connect to Cloud SQL from - Compute Engine, App Engine, Cloud shell, Cloud SQL Proxy, IP (SSL)\
Backups - incremental\
Automated backups & Manual backups\
\pard\pardeftab720\partightenfactor0
\cf40 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf10 gcloud sql instances list\
gcloud sql instances describe [INSTANCE_NAME]| grep connectionName\
gcloud sql connect demo\
gcloud sql connect labs-demo --user=root\
	CREATE DATABASE bike;\
	USE bike;\
	CREATE TABLE london1 (start_station_name VARCHAR(255), num INT);\
\
mysql --host=35.224.**.*** --user=root --password\
\pard\pardeftab720\partightenfactor0
\cf40 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf39 Google Cloud Spanner
\fs36 \cf40 \cb1 \
\pard\pardeftab720\partightenfactor0

\f12\b0\fs34 \cf40 \cb8 Brings best of RDBMS & NoSQL databases.\
Provides high availability, strong consistency, transactional reads & writes.\
Supports ACID transactions.\
Cloud Spanner is useful for relational, structured and semi-structured data.\
Spanner designed for large scale, distributed database for both OLTP & OLAP workloads.\
SSTables did not perform well for OLTP & OLAP. So Spanner uses Ressi, a new low-level storage format.\
Data is replicated synchronously with globally strong consistency.\
Provides 99.999% SLA\
\
Spanner instances will have databases, databases will have tables.\
Spanner instances will have (storage) nodes.\
Region types - Read-write, Read-only, Witness\
\
10,000 QPS (Queries Per Second) of reads\
 2,000 QPS of writes\
\
Datatypes - Array, bool, bytes, date, float64, int64, string, struct, timestamp\
Hotspots\
Interleave - like foreign keys - relation between two tables\
Timestamp bounds - for multi regional instances\
\pard\pardeftab720\partightenfactor0
\cf40 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf10 gcloud spanner databases create <db_id> --instance=<instance_id>\
gcloud spanner databases execute-sql <db_id> --instance=<instance_id> --sql='select * from table where key=1'\cf33 \cb8 \
\cf10 \cb1 gcloud spanner databases list --instance=<instance_id>\
gcloud spanner instances list\
\pard\pardeftab720\partightenfactor0
\cf10 \uc0\u8234 gcloud spanner instances delete test-instance\uc0\u8236 \
\uc0\u8234 gcloud spanner instances update test-instance\uc0\u8236  --nodes=3\
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf40 \
\pard\pardeftab720\partightenfactor0

\fs38 \cf39 \cb8 Google Cloud Memorystore
\fs36 \cf40 \cb1 \
\pard\pardeftab720\partightenfactor0

\f12\b0\fs34 \cf40 \cb8 Fully managed in-memory data store service for Redis.\
Store data up to 300GB and network throughput of 12 Gbps.
\f14\b\fs36 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf25 \
\pard\pardeftab720\partightenfactor0

\fs40 \cf33 \cb8 \ul GCP Big Data/Data Analytics services
\f12\b0\fs34 \cf38 \ulnone \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf41 \cb1 Cloud Pub/Sub			
\f12\b0\fs36 (similar to AWS SNS/Kinesis Data Streams)
\fs34 \cb8 \
Google Cloud Pub/Sub is a messaging service for exchanging event data among applications and services. \
\pard\pardeftab720\partightenfactor0

\fs32 \cf41 \cb1 Cloud Pub/Sub is an asynchronous messaging service designed to be highly reliable and scalable. End-to-end encryption.\
\pard\pardeftab720\partightenfactor0

\fs34 \cf41 \cb8 A producer of data publishes messages to a Cloud Pub/Sub topic.\
A consumer creates a subscription to that topic. \
Subscribers either pull messages from a subscription or are configured as webhooks for push subscriptions.\
Used to ingesting data at scale.\
At-least-once delivery and exactly-once processing.\
Global by default.
\fs32 \cb1 \

\fs34 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 gcloud pubsub subscriptions pull --auto-ack MySub\
\pard\pardeftab720\partightenfactor0
\cf41 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf41 Cloud Dataflow			
\f12\b0\fs36 (similar to AWS Kinesis)
\fs34 \
\cb8 Google Cloud Dataflow managed service, serverless, for transforming and enhancing data in stream and batch modes.\
To process data in real-time or in batch mode.\
Based on Apache Beam open source project.\
ETL tool in GCP.\
Inbound data (from Pub/Sub) can be queried, processed and extracted for target environment.\
Can be integrated with Cloud Pub/Sub, BigQuery and Cloud Machine Learning, and Apache Kafka.\
Intelligently scales to millions of QPS (Queries per second)\
\pard\pardeftab720\partightenfactor0
\cf41 \cb1 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf41 Cloud Dataproc			
\f12\b0\fs36 (similar to AWS EMR)
\fs34 \
\cb8 Google Cloud Dataproc is managed Big Data service for running Apache Hadoop and Spark jobs.\
Dataproc automates (Hadoop) cluster management.\
Connect with GCS to separate compute and storage.\
Clusters can be quickly created/resized from three to hundreds of nodes.\
Can run MapReduce jobs.\
Lift-and-shift existing Hadoop workloads.\
Resize clusters effortlessly.\
We can use preemptable VMs for cost saving.\
\

\f14\b\fs38 \cb1 Cloud Datalab		
\f12\b0 (similar to 
\fs34 \cb8 Jupyter Notebooks)
\f14\b\fs38 \cb1 \

\f12\b0\fs34 \cb8 Google Cloud Datalab is used for data exploration, analysis and visualize data and machine learning.\
Built on top of open source Jupyter Notebooks.\
Runs on Compute Engine and can connect to multiple cloud services.\
Supports Python, SQL and JavaScript languages.\
\

\f14\b\fs38 \cb1 Google Genomics
\f12\b0\fs34 \cb8 \
An API to store, process, explore and share genomics data using the standards, that includes support for managing datasets, reads and variants; searching & slicing; and string access control for sharing.\
\
BI Engine\cf10 \cb1 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf25 \
\pard\pardeftab720\partightenfactor0

\fs40 \cf33 \cb8 \ul GCP AI & ML services
\fs36 \cf25 \cb1 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf42 Cloud AI Building Blocks\
\pard\pardeftab720\partightenfactor0

\f12\b0\fs34 \cf42 \cb8 GCP AI building blocks provide AI through simple REST calls.\
Customisation of ML/AI model is not supported/possible.\
Sight -> Vision, Video\
Conversation -> Dialogflow, Cloud Text-to-Speech API, Cloud Speech-to-Text API\
Language -> Translation, Natural Language\
Structured data -> AutoML tables, Recommendations AI, Cloud Inference API
\f14\b\fs36 \cb1 \
\
Cloud AutoML\

\f12\b0\fs34 \cb8 Google Cloud AutoML enables training high quality models specific to a business problem.\
Enables training custom models on datasets.\
AutoML services - Sight (vision, video intelligence), Language (NLP, translation), Structure data (tabular data)
\f14\b\fs36 \cb1 \
\
AI Platform\

\f12\b0\fs34 \cb8 Google AI Platform provides end-to-end ML pipelines on-premises and cloud.\
Built on top of Kubeflow, on open source ML project, based on Kubernetes.\
AI Platform includes tools for data preparation, training and inference.\
Ingest data -> Prepare -> Preprocess -> Discover -> Develop -> Train -> Test & Analyze -> Deploy
\f14\b\fs36 \cb1 \
\
AI Hub\

\f12\b0\fs34 \cb8 Google hosted ML artifact repository to discover, share and deploy ML models.\
Google AI Hub has private & public content.\
AI Hub includes - Kubeflow pipeline components, Jupyter Notebooks, TensorFlow modules, VM images, trained models.\
\
Cloud TPU\
TPU - Tensor Processing Unit\cf33 \
\
\pard\pardeftab720\partightenfactor0

\f14\b\fs40 \cf33 \ul GCP DevOps services
\f12\b0\fs34 \cf25 \cb1 \ulnone \
\pard\pardeftab720\partightenfactor0
\cf43 Google DevOps services provide tools and frameworks for automation.\
\
Cloud Source Repositories\
to store and track source code. Private Git repository. Commit will trigger build, test and deploy code.\
\
Cloud Build\
automates CI/CD, integrated with Cloud Source Repo, Docker, GitHub and Bitbucket. Custom workflow to deploy across multiple environments.\
\
Deployment Manager\cf33 \cb8 \
\cf43 \cb1 Provides repeatable deployments.\
\
Container Registry\
central repository for storing, securing and managing Docker container images & repositories.\
\
\pard\pardeftab720\partightenfactor0
\cf37 \cb8 Anthos\
Modern application development platform.\
\
Core components:\
 - Application Development - Cloud Run for Anthos\
 - Service Management - Anthos Service Mesh\
 - Container Management - Anthos GKE\
 - Config & Policy Management - Anthos Config Management\
 - Operations Management - Stackdriver\
\pard\pardeftab720\partightenfactor0
\cf43 \cb1 \
\pard\pardeftab720\partightenfactor0
\cf33 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs40 \cf33 \ul GCP Management services
\f12\b0\fs34 \ulnone \
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf25 \cb1 Stackdriver				
\f12\b0\fs34 \cb8 (similar to AWS CloudWatch)\cf38 \
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls29\ilvl0\cf33 \kerning1\expnd0\expndtw0 {\listtext	
\f15 \uc0\u9642 
\f12 	}\expnd0\expndtw0\kerning0
Error reporting\
\ls29\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f15 \uc0\u9642 
\f12 	}\expnd0\expndtw0\kerning0
Monitoring\
\ls29\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f15 \uc0\u9642 
\f12 	}\expnd0\expndtw0\kerning0
Trace	- latency reporting and sampling, per-URL latency and stats\
\ls29\ilvl0\kerning1\expnd0\expndtw0 {\listtext	
\f15 \uc0\u9642 
\f12 	}Logging\
{\listtext	
\f15 \uc0\u9642 
\f12 	}Debugger\
{\listtext	
\f15 \uc0\u9642 
\f12 	}Profiler\expnd0\expndtw0\kerning0
\
\pard\pardeftab720\partightenfactor0
\cf33 \
Stackdriver Monitoring provides visibility into the performance, uptime, and overall health of cloud-powered applications.\
Stackdriver collects metrics, events, and metadata from Google Cloud Platform, AWS, hosted uptime probes, application instrumentation, and a variety of common application components including Cassandra, Nginx, Apache Web Server, Elasticsearch, and many others.\
Stackdriver ingests that data and generates insights via dashboards, charts, and alerts.\
Stackdriver alerting helps in integrating with Slack, PagerDuty, HipChat, Campfire etc.\cf38 \
\cf33 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf25 \cb1 KMS (Key Management Service)
\f12\b0\fs34 \cf33 \cb8 \
Cloud KMS\'a0is a cryptographic key management service on GCP.\
In order to encrypt the data, you need to create a KeyRing and a CryptoKey.\
KeyRings are useful for grouping keys. Keys can be grouped by environment (like\'a0test,\'a0staging, and\'a0prod).\
\
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 gcloud services enable cloudkms.googleapis.com\
gcloud kms keyrings create $KEYRING_NAME --location global\
gcloud kms keyrings add-iam-policy-binding $KEYRING_NAME --location global --member user:$USER_EMAIL --role roles/cloudkms.cryptoKeyEncrypterDecrypter\
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf25 \
\pard\pardeftab720\partightenfactor0

\fs38 \cf25 Cloud SDK
\f12\b0\fs34 \cf33 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 sudo yum install google-cloud-sdk\
\pard\pardeftab720\partightenfactor0
\cf33 \cb8 \
\pard\pardeftab720\partightenfactor0

\f14\b\fs38 \cf25 \cb1 APIs
\f12\b0\fs34 \cf33 \cb8 \
APIs (Application Programming Interfaces) are software programs that give developers access to computing resources and data.\
Companies from many different fields offer publicly available APIs so that developers can integrate specialized tools, services, or libraries with their own applications and codebase.\
Google offers APIs that can be applied to many different fields and sectors. APIs are often used in web development, machine learning, data science, and system administration workflows.\
API library\'a0offers quick access, documentation, and configuration options for 200+ Google APIs.\
\
APIs that utilize HTTP protocol use\'a0HTTP request methods/verbs for transmitting client requests to servers. Most commonly used HTTP request methods are\'a0GET,\'a0POST,\'a0PUT and\'a0DELETE.\
GET\'a0request method is used by a client to fetch data from a server.\
PUT\'a0method replaces existing data or creates data if it does not exist.\
POST\'a0method is used primarily to create new resources.\
DELETE\'a0method will remove data or resources specified by the client on a server.\
\
Endpoints\'a0are access points to data or computing resources hosted on a server and they take the form of an\'a0HTTP URI.\
\pard\pardeftab720\partightenfactor0
\cf25 \cb1 \
Google Cloud IoT - Cloud IoT Core, Edge TPU\
Apigee API Platform -\
API Analytics -\
Cloud Endpoints (similar to AWS API Gateway) -\
\
Billing account will have multiple projects.\
Billing Roles - Billing account administrator, user, viewer\
\pard\pardeftab720\partightenfactor0

\f14\b\fs36 \cf25 \
\pard\pardeftab720\partightenfactor0

\fs38 \cf25 Cloud Shell
\f12\b0\fs32 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf33 \cb8 Cloud Shell is an in-browser command prompt execution environment that allows you to enter commands at a terminal prompt to manage resources and services in GCP project.\
Google Cloud Shell,\'a0is a virtual machine that is loaded with development tools, provides\'a0gcloud\'a0command-line access to computing resources hosted on Google Cloud Platform.\
Cloud Shell is a Debian-based virtual machine with a persistent 5GB home directory, which makes it easy to manage GCP projects and resources.\
A\'a0cluster\'a0consists of at least one\'a0cluster master\'a0machine and multiple worker machines called\'a0nodes. Nodes are\'a0Compute Engine virtual machine (VM) instances\'a0that run the Kubernetes processes necessary to make them part of the cluster.\
\pard\pardeftab720\partightenfactor0

\fs32 \cf25 \cb1 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf33 \cb8 gcloud, which is used for many tasks on the platform, like resource management and user authentication.\cf10 \cb1 \
\
gcloud help\
gcloud help compute instances create\
gcloud -h\
gcloud --help\
\
gcloud info\
\
gcloud init\
gcloud init --console-only\
gcloud init --skip-dignostics\
\
gcloud config set\
gcloud config set compute/zone asia-south1-c\
gcloud config set compute/region us-central1\
gcloud config set account `ACCOUNT`\
gcloud config set project my-project1\
gcloud config configurations create\
gcloud config list\
gcloud config list project\
gcloud config list --all\
\
gcloud auth list					# list the active account name\
gcloud auth list --limit=1\
gcloud auth configure-docker\
gcloud auth application-default login\
\
gcloud iam service-accounts list\
gcloud iam service-accounts create proxy-user --display-name "[SERVICE_ACCOUNT_NAME]"\
gcloud iam service-accounts keys create key.json --iam-account [SERVICE_ACCOUNT_EMAIL]\
\
gcloud projects list\
gcloud projects list --limit=4\
gcloud projects create my-test-project\
gcloud projects describe my-test-project\
gcloud projects get-iam-policy my-test-project\
gcloud projects delete my-test-project\
\
gcloud compute\
gcloud compute regions list\
gcloud compute zones list\
\
gcloud compute instances list\
gcloud compute instances list --filter='labels.production:fin'\
gcloud compute instances\
gcloud compute instances create --help\
gcloud compute instances create test-instance\
gcloud compute instances create vm2 --machine-type n1-standard-2 --zone asia-south1-c\
gcloud compute instances create celab2 --machine-type g1-small --zone $ZONE --boot-disk-size=16GB\
gcloud compute instances describe <vm-name>\
\
gcloud compute instance-templates create nginx-template --metadata-from-file startup-script=startup.sh\
gcloud compute target-pools create nginx-pool\
gcloud compute instance-groups managed create nginx-group --base-instance-name nginx --size 2 --template nginx-template --target-pool nginx-pool\
gcloud compute instance-groups managed set-named-ports nginx-group --named-ports http:80\
\
gcloud compute disks create mydisk --size=200GB --zone us-central1-c\
gcloud compute instances attach-disk celab --disk mydisk --zone us-central1-c\
\
gcloud compute ssh celab2 --zone [YOUR_ZONE]\
gcloud compute ssh instance-1 --zone asia-south1-c\
\
gcloud compute addresses list				# to list Public IPs\
\
gcloud compute firewall-rules list\
gcloud compute firewall-rules create www-firewall --allow tcp:80\
gcloud compute routes list\
\
gcloud compute forwarding-rules list\
gcloud compute forwarding-rules create nginx-lb --region us-central1 --ports=80 --target-pool nginx-pool\
gcloud compute forwarding-rules create http-content-rule --global --target-http-proxy http-lb-proxy --ports 80\
\
gcloud compute http-health-checks create http-basic-check\
\
gcloud compute backend-services create nginx-backend --protocol HTTP --http-health-checks http-basic-check --global\
gcloud compute backend-services add-backend nginx-backend --instance-group nginx-group --instance-group-zone us-central1-a --global\
gcloud compute url-maps create web-map --default-service nginx-backend\
gcloud compute target-http-proxies create http-lb-proxy --url-map web-map\
\
gcloud compute project-info describe --project <project_ID>\
gcloud compute project-info describe --project labs-gcp-2a77cda8a2059\
\
gcloud compute images list\
\
gcloud components list\
gcloud components install beta\
gcloud components install kubectl\
gcloud components remove cbt\
gcloud components update\
\
gcloud app deploy\
gcloud app deploy -v std-v1\
\
gcloud beta interactive\
\
gcloud functions delete get --trigger-http\
gcloud functions deploy kgSearch --runtime nodejs8 --trigger-http\
gcloud functions logs read --limit 100\
gcloud functions delete kgSearch\
\
gcloud sql instances list\
gcloud sql instances describe [INSTANCE_NAME]| grep connectionName\
gcloud sql connect demo\
gcloud beta sql connect my-master\
gcloud sql connect labs-demo --user=root\
\
gcloud spanner databases create <db_id> --instance=<instance_id>\
gcloud spanner databases execute-sql <db_id> --instance=<instance_id> --sql='select * from table where key=1'\cf33 \cb8 \
\pard\pardeftab720\partightenfactor0
\cf10 \cb1 gcloud spanner databases list --instance=<instance_id>\
gcloud spanner instances list\
\pard\pardeftab720\partightenfactor0
\cf10 \uc0\u8234 gcloud spanner instances delete test-instance\uc0\u8236 \
\
gcloud container clusters list\
gcloud container clusters create [CLUSTER-NAME]\
gcloud container clusters create satya-ke-cluster\
gcloud container clusters get-credentials [CLUSTER-NAME]
\fs32 \cf25 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf10 gcloud container clusters get-credentials satya-ke-cluster\
gcloud container clusters delete [CLUSTER-NAME]\
gcloud container clusters delete satya-ke-cluster\
\
\pard\pardeftab720\sl300\partightenfactor0
\cf10 gcloud docker -- push gcr.io/$DEVSHELL_PROJECT_ID/hello-node:1.0\
\pard\pardeftab720\partightenfactor0
\cf10 \
gcloud pubsub subscriptions pull --auto-ack MySub\
\
gcloud ai-platform local train \\\
    --module-name trainer.task \\\
    --package-path trainer/ \\\
    --job-dir $MODEL_DIR \\\
    --train-files $TRAIN_DATA \\\
    --eval-files $EVAL_DATA \\\
    --train-steps 1000 \\\
    --eval-steps 100\
gcloud ai-platform local predict --model-dir output/export/census/<timestamp> --json-instances ../test.json\
gcloud ai-platform jobs stream-logs $JOB_NAME\
gcloud ai-platform models create $MODEL_NAME --regions=$REGION\
gcloud ai-platform models list\
\
gcloud services enable cloudkms.googleapis.com\
gcloud kms keyrings create $KEYRING_NAME --location global\
gcloud kms keyrings add-iam-policy-binding $KEYRING_NAME --location global --member user:$USER_EMAIL --role roles/cloudkms.cryptoKeyEncrypterDecrypter\
\
\pard\pardeftab720\partightenfactor0

\f16\b\fs36 \cf37 Oracle Cloud Infrastructure (OCI)
\f17\b0\fs34 \
OCI region - Localized geographic area. Group of Availability Domains (AD). 21 regions.\
Availability Domain (AD) - multiple fault-tolerant and completely independence DCs within a region.\
Fault Domains (FD) - logical DC - Each AD has 3 FDs. Grouping of hardware and infrastructure within and AD to provide anti-affinity.\
Compartment - Collection of related resources. Root Compartment can hold all the cloud resources.\
\

\f16\b\fs36 Identity (IAM)
\f17\b0\fs34 \
	- Users\
	- Groups - OCI Admin, Applications, End Users\
	- Dynamic Groups\
	- Policies\
	- Compartments\
	- Federation\
\

\f16\b\fs36 Networking
\f17\b0\fs34 \
VCN\
VPN\
LBaas\
FastConnect\
\

\f16\b\fs36 Compute
\f17\b0\fs34 \
 - Bare Metal\
 - VM\
 	- Standard/CPUs\
 	- GPU\
\

\f16\b\fs36 Storage
\f17\b0\fs34 \
NVMe, Block, File, Object, Archive\
\
Containers & Kubernetes\
\
Database\
\

\f16\b\fs36 Autonomous Database (ADB)
\f17\b0\fs34 \
Autonomous Transaction Processing (ATP) - For transactions/batch workloads - Application Development, ML, IoT\
Autonomous Data Warehouse (ADW) - For Analytic workloads - Data Warehouse, Data Lake, Data Mart, ML\
\
Oracle ADB runs on Exadata systems hosted on OCI datacenter.\
Oracle ADB Database storage is on Exadata Storage Servers directly attached to Exadata system.\
\
NO need - to create indexes\
		- to partition tables\
		- of tablespaces\
		- to specify compression\
\
Machine Learning algorithm processes changes to find new optimal plans and indexes.\
SLA - 99.95%\
\
Oracle Machine Learning (OML) - Provides built-in web-based notebooks, based on Apache Zeppelin.\
Oracle Data Visualisation Desktop - Personal and single user desktop application, for data analysis and visualisation.\
\

\f16\b\fs36 Edge
\f17\b0\fs34 \
DNS\
WAF\
DDoS\
Email\
\
\pard\pardeftab720\partightenfactor0

\f16\b\fs38 \cf38 \kerning1\expnd0\expndtw0 Docker
\f17\b0\fs32 \
Tool for creating and managing containers.\
Container is independent and standardised unit of software/application.\
Same container always yields the exact same application and execution behaviour, no matter where or by whom it might be executed.\
\
Images are blueprints/templates for containers, defined in \ul Dockerfile\ulnone .\
Images are read-only and container the application as well as the necessary application environment (OS, tools, runtime etc).\
Images are either pre-built or custom made.\
Containers are running instances of Images, when started a thing read-write layer is added on top of the image.\
Multiple Containers can therefore be started based on one and the same Image. All Containers run in isolation, i.e. they don't share any application state or written data.\
\
Volumes are folders on host machine hard drive which are mounted into containers.\
Volumes persist if a container shuts down. If a container (re)starts and mounts a volume, any data inside of that volume is available in the container.\
Container can write data into a volume and read data from volume.\
Anonymous Volumes: Created via -v /some/path/in/container and removed automatically when a container is removed because of --rm added on the docker run command\uc0\u8232 Named Volumes: Created via -v some-name:/some/path/in/container and NOT removed automatically \
Bind Mounts are very similar to Volumes - the difference is the developer, set the path on host machine that should be connected to some path inside of a Container.  Don't use Bind Mounts if you just want to persist data.\
\
docker --help\
docker ps --help\
docker -v\
\
docker ps\
docker ps -a			# all\
\
docker run d2aa8b98fb1a\cf37 \
\cf38 docker run -p 3000:3000 d2aa8b98fb1a			# port\
docker run -p 3000:80 test\
docker run -p 2000:1000 -d c2aa8b98fb1d			# detach\
docker run -p 2000:80 -d --rm c2aa8b98fb1d 		# removes the container once stopped\
docker run -p 2000:80 -d --name test_con c2aa8b98fb1d \
docker run -p 2000:80 --name test_con second_build:v2\
docker run -d --name mongodb mongo\cf37 \
\cf38 docker run -it node npm init\
\
docker start test_con\
docker start -a test_con		# attach\
docker start -a -i test_con	\
docker stop test_con\
\
docker pull node   		# Node.js\
docker run node\cf37 \
\cf38 docker run -it node		# interactive and TTY\
\
docker build .\
docker build -t first_build:latest .\
docker build -t second_build:v2 .\
\
docker attach test_con\
docker logs test_con\
docker logs -f test_con\
\
docker rm test_con\
docker rm test_con1 test_con2 test_con3\
\
docker images\
docker rmi c2aa8b98fb1d	# remove image\
docker image prune			# remove all unused images\
docker image prune	 -a\
docker image inspect c2aa8b98fb1d\
\
docker container inspect mongodb\
\
docker history <image>\
\
docker cp test.txt con33:/test\
docker cp sample/. con:/test\
docker cp con:/test dummy/\
\
docker tag node-demo hello-world\
docker tag node-demo:latest hello-world:v3\
\
docker login\
\
docker push <image_name>\
docker push satya/hello-world\
docker pull <image_name>\
docker pull satya/hello-world\
\
docker volume create feed-files\
docker run -d -p 2000:80 --name feed-app -v feed:/app/feed feed-node:volumes									# anonymous volumes, Docker managed\
docker run -d -p 2000:80 --name feed-app -v feed:/app/feed -v /Users/sthirumani/docker:/app feed-node:volumes		# bind mounts, user defined volumes\
docker run -d -p 2000:8000 --name --env PORT=8000 feed-app -v feed:/app/feed feed-node:volumes									\
docker volume ls\
docker volume rm feed\
docker volume prune\
docker volume inspect feed\
docker volume create VOL_NAME\
\
docker network create my-network\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls30\ilvl0\cf38 docker network create --driver bridge my-net\
\pard\pardeftab720\partightenfactor0
\cf38 docker network ls\
docker run -d --name mongodb --network my-network mongo\
docker run -d --name fav --network my-network --rm -p 3000:3000 fav-node\
\
docker exec my_container \
docker exec -it my_container npm init\
\
\pard\pardeftab720\partightenfactor0

\fs34 \cf38 \ul Dockerfile examples:
\fs32 \ulnone \
FROM node\
FROM python\
\
WORKDIR /app\
\
COPY 
\fs34 . .
\fs32 \
COPY 
\fs34 .
\fs32  /app\
COPY 
\fs34 package.json .
\fs32 /\
\
RUN npm install\
RUN node server.js\
\
ARG DEFAULT_PORT=80\
\
ENV PORT 80\
ENV $DEFAULT_PORT\
\
EXPOSE 80\
EXPOSE $PORT\
\
VOLUME [ "/app/feed" ]\
\
CMD ["node", "server.js"]\
CMD ["npm", "start"]\
CMD ["python", "input.py"]\
\

\fs34 \ul .dockerignore examples:
\fs32 \ulnone \
node_modules\
test_dir\cf37 \
\pard\tx566\pardeftab720\partightenfactor0
\cf38 Dockerfile\
.git\
\pard\pardeftab720\partightenfactor0
\cf38 \
\pard\pardeftab720\partightenfactor0

\fs34 \cf38 \ul .env examples:\
\pard\pardeftab720\partightenfactor0

\fs32 \cf38 \ulnone .env file can contain all environment variables.\
PORT=8000\
\
docker-commands.txt\
\
\pard\pardeftab720\partightenfactor0

\fs34 \cf38 \ul docker-compose.yaml examples:
\fs32 \ulnone \
version:"3.8"\
services:\
	mongodb:\
		image: 'mongo'\
		volumes:\
			- data:/data/db\
		environment:\
			# MONGO_INITDB_ROOT_USERNAME=root\
	backend:\
\
	frontend:\
\pard\pardeftab720\partightenfactor0
\cf37 \
\cf38 docker-compose up\
docker-compose up -d\
docker-compose up --build\
docker-compose up -d server php mysql\
docker-compose down\
docker-compose down -v\
docker-compose build\
docker-compose run --rm npm init\
\pard\pardeftab720\partightenfactor0
\cf10 \

\f16\b\fs36 Kuberneters
\f17\b0\fs32 \
An open source system for orchestrating container deployments.\
Helps in automatic deployments, scaling and load balancing, managing/monitoring containers and deployments.\
Kubernetes is like docker-compose for multiple machines.\
\
Pod - a (Docker) container\
Worker nodes - Machines/instances run the containers of application\
Masters manage the cluster and the nodes that are used to host the running applications.\
\
\pard\pardeftab720\sl420\sa120\partightenfactor0
\cf10 version: v1.20\
\
kubectl version --client\
\pard\pardeftab720\partightenfactor0
\cf38 \
\cf37 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs38 \cf2 Terraform
\f0\b0\fs32 \cf0 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf2 =============================================================\
\pard\pardeftab720\partightenfactor0

\f0\b0 \cf0 Terraform written in Go.\
Terraform is an infrastructure as code software by HashiCorp, first released in 2014. It allows users to define a datacenter infrastructure in a high-level configuration language, from which it can create an execution plan to build the infrastructure such as OpenStack or in a service provider such as IBM Cloud, AWS, Microsoft Azure or Google Cloud Platform. Infrastructure is defined in a HCL Terraform syntax or JSON format.\
\
Terraform is\'a0a tool for building, changing, and versioning infrastructure safely and efficiently.\'a0\
Terraform can help with multi-cloud by having one workflow for all clouds. The infrastructure Terraform manages can be hosted on public clouds like Amazon Web Services, Microsoft Azure, and Google Cloud Platform, or on-prem in private clouds such as VMWare vSphere,\'a0OpenStack, or CloudStack.\
Terraform treats infrastructure as code (IaC) so you never have to worry about you infrastructure drifting away from its desired configuration.\
\
Terraform is an\'a0open source project\'a0maintained by HashiCorp for\'a0building, changing, and combining infrastructure safely and efficiently. Some of the benefits of using Terraform to provision and manage infrastructure include:\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls31\ilvl0\cf0 {\listtext	\uc0\u8226 	}Version-controlled infrastructure as code\
{\listtext	\uc0\u8226 	}Multi-cloud support with over 100 providers including Amazon Web Services, Microsoft Azure, and Google Cloud Platform\
{\listtext	\uc0\u8226 	}Separate plan and apply stages\'a0so you can verify changes to your infrastructure before they happen\
{\listtext	\uc0\u8226 	}A public registry of modules that\'a0make provisioning common groups of resources\'a0easy\
\pard\pardeftab720\partightenfactor0
\cf0 \
Terraform v0.12\
Terraform v0.11\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 * provider.aws: version = "~> 1.30"
\f15\fs26 \cf44 \cb45 \CocoaLigature0 \
\pard\pardeftab720\partightenfactor0

\f0\fs32 \cf0 \cb1 \CocoaLigature1 \
Versions\
	- Open Source\
	- Pro Enterprise\
	- Premium Enterprise\
\
HCL - HashiCorp Configuration Language\
\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 File Extensions:
\f0\b0 \
.tf				# Terraform configuration files\
.tfstate			# Terraform keeps track\'a0of the infrastructure it manages by maintaining\'a0state\
.tfvars\
.tfplan\
.tf.json\
\pard\tx220\tx720\pardeftab720\li720\fi-720\partightenfactor0
\ls32\ilvl0\cf0 .terraform: 		# A directory created by the\'a0init\'a0command that includes the AWS Terraform provider plugin\
\pard\pardeftab720\partightenfactor0
\cf0 \
variable referencing - terraform interpolation\
$\{var.var_name\}\
$\{var.map_var_name["key"]\}\
$\{var.list_var_name\}\
$\{var.list_var_name[index]\}\
$\{var.var_name[count.index]\}\
$\{self.attribute\}\
$\{type.var_name.attribute\}\
$\{data.type.var_name.attribute\}\
\
$\{condition ? true_expression : false_expression\}\
<, <=, >, >=, ==, !=, &&, ||, !\
\
\pard\pardeftab720\partightenfactor0

\f1\b \cf0 Functions:
\f0\b0 \
length(variable)\
	$\{length(var.azs)\}\
	$\{length(split(",", "a,b,c"))\}\
element(list, index)\
	$\{element(var.subnet_cidr, count.index)\}\
concat(list1, list2, ...)\
	$\{concat(aws_instane.db.*.tags.Name, aws_instane.web.*.tags.Name)\}\
lookup(map, key, [default])\
	$\{lookup(var.ec2-ami, var.region)\}\
file() 				# reads the contents of the file into a string\
  	$\{file("user_data.sh")\}\
join(delimiter, list)\
	$\{join(",", var.ami_list)\}\
  	tags = \{\
    		Name = "$\{join("-", list(var.environment-name, "frontend"))\}"\
  	\}\
contains(list, element)\
	$\{contains(var.list_of_strings, "element")\}\
	contains(list("fish", "curry"), "bone")\
merge(map1, map2, ...)\
	$\{merge(map("a","b"), map("c","d"))\}\
replace(string, search, replace)\
\
$ export AWS_ACCESS_KEY_ID="anaccesskey"\
$ export AWS_SECRET_ACCESS_KEY="asecretkey"\
$ export AWS_DEFAULT_REGION="us-west-2"\
$ export AWS_PROFILE=personal\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 variable instance_count \{ default = 2 \}\
\pard\pardeftab720\partightenfactor0
\cf0 variable "aws_secret_key" \{ default = "23dsfsdf549" \}\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0
\cf0 variable "key" \{\
	type = "string"\
	default = "value"\
\}\
\
variable "os" \{\
	type = "list"\
	default = ["el1", "el2", "el3"]\
\}\
\
variable "ec2-ami" \{\
	type = "map"\
	default = \{\
		ap-south-1 = "ami-23sdf"\
		us-west-2 = "ami-e523a"\
	\}\
\}\
\
variable "is_active" \{ default=false \}\
\
TF_VAR_image=foo terraform apply\
export TF_VAR_zones='["us-east-1b", "us-east-1d"]'\
\
provider "aws" \{ \
	access_key = "423sfdksdfs"\
	secret_key = "s3242k77kkrweo"\
	region = "us-west-2"				\
\}\
\
provider "google" \{\
	credentials = "$\{file("acc.json)\}"\
	project = "gcp-project"\
	region = "us-central1"\
  	version = "1.8.0"\
\}\
\
provider "azurerm" \{\
  version = "< 2"\
\}\
\
provider "aws" \{\
  region                  = "us-east-1"\
  shared_credentials_file = "/Users/tf_user/.aws/creds"\
  profile                 = "customprofile"\
\}\
\
provider "aws" \{\
  assume_role \{\
    role_arn     = "arn:aws:iam::ACCOUNT_ID:role/ROLE_NAME"\
    session_name = "SESSION_NAME"\
    external_id  = "EXTERNAL_ID"\
  \}\
\}\
\
cat > main.tf <<EOF\
provider "aws" \{\
  version = "< 2"\
  region  = "ap-south-1"\
  alias = "Mumbai"\
\}\
EOF\
\
output NAME \{\
	value = op-var-value\
\}\
output "public_dns" \{\
	value = "$\{aws_instance.myec2.public_dns\}"\
\}\
output "backend_ips" \{\
  value = "$\{aws_instance.backend.*.public_ip\}"\
\}\
output "vpc_id" \{ value = "$\{data.aws_security_group.sec_group.vpc_id\}" \}\
output "intra_route_table_ids" \{\
  description = "List of IDs of intra route tables"\
  value       = ["$\{aws_route_table.intra.*.id\}"]\
\}\
output "frontnend_ips" \{\
  value = "$\{module.backend.ips\}"\
\}\
output "vpc_cidr_block" \{\
  description = "The CIDR block of the VPC"\
  value       = "$\{element(concat(aws_vpc.this.*.cidr_block, list("")), 0)\}"\
\}\
\
\
locals \{\
	instance_ids = ."$\{concat(aws_instance.blue.*.id, aws_instance.green.*.id)\}"\
\}\
locals \{\
  default_frontend_name = "$\{join("-",list(var.environment-name, "frontend"))\}"\
  default_backend_name  = "$\{join("-",list(var.environment-name, "backend"))\}"\
\}\
locals \{\
  default_name = "$\{join("-", list(terraform.workspace, "example"))\}"\
\}\
locals \{\
  regions = \{\
    staging    = "us-west-1"\
    production = "us-east-1"\
  \}\
\
  instance_count = \{\
    staging    = "1"\
    production = "2"\
  \}\
\}\
\
resource "resource_type" "identifier" \{\
  argument1 = value1\
  argument2 = value2\
\}\
\
resource "aws_vpc" "main" \{\
	cidr_block = "190.160.0.0/16"\
	instance_tenancy = "default"\
	\
	tags \{\
		Name = "Main"\
		Location = "Bangalore"\
	\}\
\}\
\
resource "aws_vpc" "this" \{\
  count = "$\{var.create_vpc ? 1 : 0\}"\
\
  cidr_block           = "$\{var.cidr\}"\
  instance_tenancy     = "$\{var.instance_tenancy\}"\
  enable_dns_hostnames = "$\{var.enable_dns_hostnames\}"\
  enable_dns_support   = "$\{var.enable_dns_support\}"\
\
  tags = "$\{merge(map("Name", format("%s", var.name)), var.vpc_tags, var.tags)\}"\
\}\
\
resource "aws_vpc_dhcp_options" "this" \{\
  count = "$\{var.create_vpc && var.enable_dhcp_options ? 1 : 0\}"\
\
  domain_name          = "$\{var.dhcp_options_domain_name\}"\
  domain_name_servers  = ["$\{var.dhcp_options_domain_name_servers\}"]\
  ntp_servers          = ["$\{var.dhcp_options_ntp_servers\}"]\
  netbios_name_servers = ["$\{var.dhcp_options_netbios_name_servers\}"]\
  netbios_node_type    = "$\{var.dhcp_options_netbios_node_type\}"\
\
  tags = "$\{merge(map("Name", format("%s", var.name)), var.dhcp_options_tags, var.tags)\}"\
\}\
\
resource "aws_vpc_dhcp_options_association" "this" \{\
  count = "$\{var.create_vpc && var.enable_dhcp_options ? 1 : 0\}"\
\
  vpc_id          = "$\{aws_vpc.this.id\}"\
  dhcp_options_id = "$\{aws_vpc_dhcp_options.this.id\}"\
\}\
\
resource "aws_key_pair" "deployer" \{\
  key_name   = "deployer-key"\
  public_key = "ssh-rsa AAAAB3NzaC1yc2E5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6mXSrbX8ZbabVohBK41 email@example.com"\
\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 resource "aws_subnet" "web_subnet_1" \{\
  vpc_id            = "$\{aws_vpc.web_vpc.id\}"\
  cidr_block        = "192.168.100.0/25"\
  availability_zone = "us-west-2a"\
\
  tags \{\
    Name = "Web Subnet 1"\
  \}\
\}\
\
resource "aws_subnet" "web_subnet" \{\
  # Use the count meta-parameter to create multiple copies\
  count             = 2\
  vpc_id            = "$\{aws_vpc.web_vpc.id\}"\
  # cidrsubnet function splits a cidr block into subnets\
  cidr_block        = "$\{cidrsubnet(var.network_cidr, 1, count.index)\}"\
  # element retrieves a list element at a given index\
  availability_zone = "$\{element(var.availability_zones, count.index)\}"\
\
  tags \{\
    Name = "Web Subnet $\{count.index + 1\}"\
  \}\
\}\
\pard\pardeftab720\partightenfactor0
\cf0 \
resource "aws_instance" "web" \{\
  ami           = "ami-0fb83677"\
  instance_type = "t2.micro"\
  subnet_id     = "$\{aws_subnet.web_subnet_1.id\}"\
  \
  tags \{\
    Name = "Web Server 1"\
  \}\
\}\
\
resource "aws_instance" "west_frontend" \{\
  count             = 2\
  depends_on        = ["aws_instance.west_backend"]\
  provider          = "aws.us-west-1"\
  ami               = "ami-07585467"\
  availability_zone = "$\{var.us-west-zones[count.index]\}"\
  instance_type     = "t2.micro"\
\
  lifecycle \{\
    prevent_destroy = true\
  \}\
\}\
\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 resource "aws_instance" "web_ec2" \{\
  count         = "$\{var.instance_count\}"\
  # lookup returns a map value for a given key\
  ami           = "$\{lookup(var.ami_ids, "us-west-2")\}"\
  instance_type = "t2.micro"\
  # Use the subnet ids as an array and evenly distribute instances\
  subnet_id     = "$\{element(aws_subnet.web_subnet.*.id, count.index % length(aws_subnet.web_subnet.*.id))\}"\
  \
  tags \{\
    Name = "Web Server $\{count.index + 1\}"\
  \}\
\
  lifecycle \{\
    create_before_destroy = true			# opposite is prevent_destroy=true\
  \}\
\
  timeouts \{\
    create = "60m"\
    delete = "2h"\
  \}  \
\}\
\
resource "aws_instance" "frontend" \{\
  availability_zone      = "$\{var.us-east-zones[count.index]\}"\
  ami                    = "ami-66506c1c"\
  instance_type          = "t2.micro"\
  key_name               = "$\{var.key_name\}"\
  vpc_security_group_ids = ["$\{var.sg-id\}"]\
\
  lifecycle \{\
    create_before_destroy = true\
  \}\
\
  connection \{\
    user        = "ec2-user"\
    type        = "ssh"\
    private_key = "$\{file(var.pvt_key)\}"\
  \}\
\
  provisioner "file" \{\
    source      = "./frontend"\
    destination = "~/"\
  \}\
\
  provisioner "remote-exec" \{\
    inline = [\
      "chmod +x ~/frontend/run_frontend.sh",\
      "cd ~/frontend",\
      "sudo ~/frontend/run_frontend.sh",\
    ]\
  \}\
\}\
\pard\pardeftab720\partightenfactor0
\cf0 \
Provisioners are used to execute scripts on a local/remote machine, as part of resource creation/destruction.\
resource "null_resource" "ansible-main" \{\
  provisioner "local-exec" \{\
    command = "ansible-playbook -e sshKey=$\{var.pvt_key\} -i '$\{aws_instance.backend.public_ip\},' ./ansible/setup-backend.yaml -v"\
  \}\
\
  depends_on = ["aws_instance.backend"]\
\}\
\
count\'a0metaparameter allows you to create multiple copies of a resource\
\
# Internet gateway to reach the internet\
resource "aws_internet_gateway" "web_igw" \{\
  vpc_id = "$\{aws_vpc.web_vpc.id\}"\
\}\
\
resource "aws_internet_gateway" "this" \{\
  count = "$\{var.create_vpc && length(var.public_subnets) > 0 ? 1 : 0\}"\
\
  vpc_id = "$\{aws_vpc.this.id\}"\
\
  tags = "$\{merge(map("Name", format("%s", var.name)), var.igw_tags, var.tags)\}"\
\}\
\
# Route table with a route to the internet\
resource "aws_route_table" "public_rt" \{\
  vpc_id = "$\{aws_vpc.web_vpc.id\}"\
  \
  route \{\
    cidr_block = "0.0.0.0/0"\
    gateway_id = "$\{aws_internet_gateway.web_igw.id\}"\
  \}\
\
  tags \{\
    Name = "Public Subnet Route Table"\
  \}\
\}\
\
# Associate public route table with the public subnets\
resource "aws_route_table_association" "public_subnet_rta" \{\
  count          = 2\
  subnet_id      = "$\{aws_subnet.public_subnet.*.id[count.index]\}"\
  route_table_id = "$\{aws_route_table.public_rt.id\}"\
\}\
\
resource "aws_elb" "web" \{\
  name = "web-elb"\
  subnets = ["$\{aws_subnet.public_subnet.*.id\}"]\
  security_groups = ["$\{aws_security_group.elb_sg.id\}"]\
  instances = ["$\{aws_instance.web.*.id\}"]\
\
  # Listen for HTTP requests and distribute them to the instances\
  listener \{ \
    instance_port     = 80\
    instance_protocol = "http"\
    lb_port           = 80\
    lb_protocol       = "http"\
  \}\
\
  # Check instance health every 10 seconds\
  health_check \{\
    healthy_threshold = 2\
    unhealthy_threshold = 2\
    timeout = 3\
    target = "HTTP:80/"\
    interval = 10\
  \}\
\}\
\
resource "aws_security_group" "elb_sg" \{\
  name        = "ELB Security Group"\
  description = "Allow incoming HTTP traffic from the internet"\
  vpc_id      = "$\{aws_vpc.web_vpc.id\}"\
\
  ingress \{\
    from_port   = 80\
    to_port     = 80\
    protocol    = "tcp"\
    cidr_blocks = ["0.0.0.0/0"]\
  \}\
\
  # Allow all outbound traffic\
  egress \{\
    from_port = 0\
    to_port = 0\
    protocol = "-1"\
    cidr_blocks = ["0.0.0.0/0"]\
  \}\
\}\
\
\pard\pardeftab720\partightenfactor0

\fs34 \cf0 Data sources are a repository of read-only information. And acts as template.
\fs32 \
data "aws_security_group" "sec_group" \{\
  id = "$\{var.security_group_id\}"\
\}\
\
data "aws_availability_zones" "azs" \{ \}\
\
data "aws_vpc" "vpc2" \{\
	filter \{\
		name = "tag:Name"\
		values = ["Satya"]\
	\}\
\}\
\
# default backend is 'local'\
terraform \{\
	backend "s3" \{\
		bucket = "tf-bucket"\
		key = "path/to/key"\
		region = "us-east-1"\
	\}\
\}\
\
data "terraform_remote_state" "network" \{\
	backend = "s3"\
	config \{\
		bucket = "terraform-state-prod"\
		key = "network/terraform.tfstate"\
		region = "us-east-1"\
	\}\
\}\
\

\fs34 Modules are self-contained packages of configurations used to create reusable Terraform elements.
\fs32 \
module "module-name" \{\
	source = "...."\
\}\
\
module "backend" \{\
  source = "./aws_instances"\
  total_instances = 2\
  aws_access_key = "$\{var.aws_access_key\}"\
  aws_secret_key = "$\{var.aws_secret_key\}"\
\}\
\
module "network" \{\
  source = "Azure/network/azurerm"\
  version = "2.0.0"\
  resource_group_name = "$\{data.azurerm_resource_group.dev.name\}"\
  location = "$\{data.azurerm_resource_group.dev.location\}"\
  subnet_names = ["subnet1"]\
  vnet_name = "web"\
\
  tags = \{\
    environment = "dev"\
  \}\
\}\
\
module "vpc" \{\
  source = "terraform-aws-modules/vpc/aws"\
\
  name = "complete-example"\
\
  cidr = "10.10.0.0/16"\
\
  azs             = ["eu-west-1b", "eu-west-1c"]\
  public_subnets  = ["10.10.1.0/24", "10.10.2.0/24"]\
  private_subnets = ["10.10.3.0/24", "10.10.4.0/24"]\
\
  enable_nat_gateway = true\
\
  tags = \{\
    Owner       = "user"\
    Environment = "terraform.workspace"\
    Name        = "teffaform vpc"\
  \}\
\}\
\
module "db" \{\
  source = "terraform-aws-modules/rds/aws"\
\
  identifier = "demodb"\
\
  # All available versions: http://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.VersionMgmt\
  engine            = "mysql"\
  engine_version    = "5.7.19"\
  instance_class    = "db.t3.micro"\
  allocated_storage = 5\
  storage_encrypted = false\
\
  # kms_key_id        = "arm:aws:kms:<region>:<accound id>:key/<kms key id>"\
  name     = "demodb"\
  username = "user"\
  password = "YourPwdShouldBeLongAndSecure!"\
  port     = "3306"\
\
  vpc_security_group_ids = ["$\{data.aws_security_group.default.id\}"]\
\
  maintenance_window = "Mon:00:00-Mon:03:00"\
  backup_window      = "03:00-06:00"\
\
  # disable backups to create DB faster\
  backup_retention_period = 35\
\
  tags = \{\
    Owner       = "user"\
    Environment = "dev"\
  \}\
\
  # DB subnet group\
  subnet_ids = ["$\{data.aws_subnet_ids.all.ids\}"]\
\
  # DB parameter group\
  family = "mysql5.7"\
\
  # Snapshot name upon DB deletion\
  final_snapshot_identifier = "demodb"\
\}\
\
rule "aws_instance_previous_type" \{\
  enabled = false\
\}\
\
server \{\
    listen         80 default_server;\
    listen         [::]:80 default_server;\
    server_name    localhost;\
    root           /var/www/frontend;\
    index          index.php;\
\
  location ~* \\.php$ \{\
    fastcgi_pass unix:/run/php/php7.0-fpm.sock;\
    include         fastcgi_params;\
    fastcgi_param   SCRIPT_FILENAME    $document_root$fastcgi_script_name;\
    fastcgi_param   SCRIPT_NAME        $fastcgi_script_name;\
  \}\
\}\
\
\
Workspaces allows to rename state file. Workspace is a named container for Terraform states.\
\
\pard\pardeftab720\partightenfactor0

\f1\b\fs34 \cf0 Terraform CLI:
\f0\b0 \
\pard\pardeftab720\partightenfactor0

\f12\fs30 \cf23 \cb46 \expnd0\expndtw0\kerning0
$ terraform\
Usage: terraform [--version] [--help] <command> [args]\
\
Common commands:\
    apply              Builds or changes infrastructure\
    console            Interactive console for Terraform interpolations\
    destroy            Destroy Terraform-managed infrastructure\
    fmt                Rewrites config files to canonical format\
    get                Download and install modules for the configuration\
    graph              Create a visual graph of Terraform resources\
    import             Import existing infrastructure into Terraform\
    init               Initialize a new or existing Terraform configuration\
    output             Read an output from a state file\
    plan               Generate and show an execution plan\
    providers          Prints a tree of the providers used in the configuration\
    push               Upload this Terraform module to Terraform Enterprise to run\
    refresh            Update local state file against real resources\
    show               Inspect Terraform state or plan\
    taint              Manually mark a resource for recreation\
    untaint            Manually unmark a resource as tainted\
    validate           Validates the Terraform files\
    version            Prints the Terraform version\
    workspace          Workspace management\
\
All other commands:\
    debug              Debug output management (experimental)\
    force-unlock       Manually unlock the terraform state\
    state              Advanced state management\
\pard\pardeftab720\partightenfactor0

\f0\fs34 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
terraform -version\
terraform taint -help\
terraform init				# will install provider plugins\
terraform init -upgrade commands_exec/\
terraform get \
terraform plan\
terraform plan -out=demo.tfplan\
terraform plan --destroy -out destroy-plan\
terraform apply\
terraform apply demo.tfplan\
terraform apply -var 'zones=["us-east-1b", "us-east-1d"]'\
terraform apply -var-file=sample.tfvars\
terraform destroy\
terraform destroy -target=aws_security_group.elb_sg\
terraform workspace list\
terraform workspace new dev\
terraform workspace select\
$ terraform import aws_instance.example i-abcd1234		# import existing infrastructure\
$ terraform import aws_key_pair.deployer deployer-key\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 terraform import aws_vpc.web_vpc vpc-25b7495d\
\pard\pardeftab720\partightenfactor0
\cf0 terraform refresh\
terraform show				# to show the attributes in the state file\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 terraform state list\
\pard\pardeftab720\partightenfactor0
\cf0 terraform state show aws_vpc.web_vpc\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 terraform graph\
\pard\pardeftab720\partightenfactor0
\cf0 terraform output\
terraform output ips_var\
terraform output -module=network\
site_address=$(terraform output site_address)\
web_server_ip=$(terraform output -module=vm | cut -d " " -f 3)\
terraform console\
echo "2 + 8" | terraform console\
terraform providers\
terraform fmt\
terraform fmt -check\
terraform validate			# performs syntax checks\
\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://github.com/wata727/tflint"}}{\fldrslt \cf0 TFLint}}\'a0is an open-source, static analysis tool for Terraform that focuses on linting\
\pard\pardeftab720\partightenfactor0

\fs36 \cf0 $ tflint -h
\f12\fs28 \cf7 \cb47 \expnd0\expndtw0\kerning0
\
Usage:\
  tflint [OPTIONS] [FILE]\
\
Application Options:\
  -v, --version                             Print TFLint version\
  -f, --format=[default|json|checkstyle]    Output format (default: default)\
  -c, --config=FILE                         Config file name (default: .tflint.hcl)\
      --ignore-module=SOURCE1,SOURCE2...    Ignore module sources\
      --ignore-rule=RULE1,RULE2...          Ignore rule names\
      --var-file=FILE1,FILE2...             Terraform variable file names\
      --deep                                Enable deep check mode\
      --aws-access-key=ACCESS_KEY           AWS access key used in deep check mode\
      --aws-secret-key=SECRET_KEY           AWS secret key used in deep check mode\
      --aws-profile=PROFILE                 AWS shared credential profile name used in deep check mode\
      --aws-region=REGION                   AWS region used in deep check mode\
  -d, --debug                               Enable debug mode\
      --error-with-issues                   Return error code when issues exist\
      --fast                                Ignore slow rules. Currently, ignore only aws_instance_invalid_ami\
  -h, --help                                Show this help message\
\pard\pardeftab720\partightenfactor0

\f0\fs34 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
tflint\
tflint --error-with-issues\
tflint --error-with-issues --ignore-rule=aws_instance_not_specified_iam_profile\
tflint --deep\
tflint --deep --aws-region=us-west-2\
tflint --error-with-issues --ignore-rule=aws_instance_not_specified_iam_profile --debug\
tflint template.tf\
tflint --config other_config.hcl\
$ tflint --aws-access-key AWS_ACCESS_KEY --aws-secret-key AWS_SECRET_KEY --aws-region us-east-1\
tflint --aws-profile AWS_PROFILE --aws-region us-east-1\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardirnatural\partightenfactor0
\cf0 \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://github.com/cesar-rodriguez/terrascan"}}{\fldrslt \cf0 Terrascan}}\'a0is an open-source, security-focused, static analysis tool for Terraform configurations.\
Terrascan\'a0is focused more on security best practices and only supports AWS resources.\
virtualenv scan\
source scan/bin/activate\
pip install terrascan\
\pard\pardeftab720\partightenfactor0

\f12\fs30 \cf7 \cb47 \expnd0\expndtw0\kerning0
\
(scan) student:~ $ terrascan -h\
usage: terrascan [-h] [-l LOCATION] [-t TESTS]\
\
  -h, --help            show this help message and exit\
  -l LOCATION, --location LOCATION\
                        Location of terraform templates to scan\
  -t TESTS, --tests TESTS\
                        Comma separated list of test to run or "all" for all\
                        tests (e.g. encryption,security_group) Valid values\
                        include:encription, logging_and_monitoring,\
                        public_exposure, security_group\
\pard\pardeftab720\partightenfactor0

\f0\fs36 \cf0 \cb1 \kerning1\expnd0\expndtw0 terrascan --location . --tests security_group
\fs34 \
\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://www.terraform.io/"}}{\fldrslt \cf0 https://www.terraform.io/}}\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://github.com/hashicorp/hcl"}}{\fldrslt \cf0 https://github.com/hashicorp/hcl}}\
\
\pard\pardeftab720\partightenfactor0

\fs32 \cf0 \
\pard\pardeftab720\partightenfactor0

\f1\b\fs36 \cf2 Abbreviations
\fs32 \
=============================================================
\f0\b0 \cf0 \
\
AWS - Amazon Web Services\
S3 - Simple Storage Service\
EC2 - Elastic Compute Cloud\
VPC - Virtual Private Cloud\
EMR - Elastic Map Reduce\
IAM - Identity Access Management\
RDS - Relational Database Service\
AMI - Amazon Machine Image\
ELB - Elastic Load Balancer\
ECS - 
\f3 \cf48 \expnd0\expndtw0\kerning0
Elastic
\f0 \cf0 \kerning1\expnd0\expndtw0  Container Services\
EBS - Elastic Block Store\
EFS - Elastic File System\
SNS - Simple Notification Service\
SQS - Simple Queue Service\
ARN - Amazon Resource Name\
ENI - Elastic Network Interfaces\
NAT - Network Address Translation\
\
GCP - Google Cloud Platform\
GCE - Google Compute Engine\
\
\pard\pardeftab720\partightenfactor0

\f5 \cf49 \cb8 \expnd0\expndtw0\kerning0
PEM - Privacy Enhanced Mail
\f0 \cf50 \cb1 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\partightenfactor0
\cf0 \
\pard\pardeftab720\ri-19280\partightenfactor0

\f1\b \cf2 URLs
\f0\b0 \cf51 \
\pard\pardeftab720\partightenfactor0

\f1\b \cf2 =============================================================\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "http://satya-hadoop.blogspot.com"}}{\fldrslt 
\fs36 \cf4 \ul \ulc4 http://satya-hadoop.blogspot.com}}\cf4 \ul \ulc4 \
\pard\pardeftab720\partightenfactor0
\cf3 \ulnone \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "http://hadoop.apache.org"}}{\fldrslt \cf4 \ul http://hadoop.apache.org}}\
\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "http://net.pku.edu.cn/~course/cs402/2011/book/2009-Book-Hadoop%20the%20Definitive%20Guide.pdf"}}{\fldrslt 
\f0\b0 \cf4 \ul http://net.pku.edu.cn/~course/cs402/2011/book/2009-Book-Hadoop%20the%20Definitive%20Guide.pdf}}
\f0\b0 \cf0 \
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "http://developer.yahoo.com/hadoop/tutorial/"}}{\fldrslt \cf4 \ul http://developer.yahoo.com/hadoop/tutorial/}}\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "http://jugnu-life.blogspot.com"}}{\fldrslt \cf4 \ul http://jugnu-life.blogspot.com}}\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "http://www.pappupass.com"}}{\fldrslt \cf4 \ul www.pappupass.com}}\
\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF"}}{\fldrslt \cf0 https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF}}\
\pard\pardeftab720\partightenfactor0
{\field{\*\fldinst{HYPERLINK "https://docs.treasuredata.com/articles/hive-functions"}}{\fldrslt \cf0 https://docs.treasuredata.com/articles/hive-functions}}\
}